{
  "results": {
    "mmlu_generative": {
      "exact_match,get_response": 0.513317191283293,
      "exact_match_stderr,get_response": 0.004031396763377948,
      "alias": "mmlu (generative)"
    },
    "humanities": {
      "exact_match,get_response": 0.4618490967056323,
      "exact_match_stderr,get_response": 0.006966477629221215,
      "alias": " - humanities"
    },
    "mmlu_formal_logic_generative": {
      "alias": "  - formal_logic",
      "exact_match,get_response": 0.4365079365079365,
      "exact_match_stderr,get_response": 0.04435932892851466
    },
    "mmlu_high_school_european_history_generative": {
      "alias": "  - high_school_european_history",
      "exact_match,get_response": 0.6121212121212121,
      "exact_match_stderr,get_response": 0.03804913653971011
    },
    "mmlu_high_school_us_history_generative": {
      "alias": "  - high_school_us_history",
      "exact_match,get_response": 0.5196078431372549,
      "exact_match_stderr,get_response": 0.03506612560524866
    },
    "mmlu_high_school_world_history_generative": {
      "alias": "  - high_school_world_history",
      "exact_match,get_response": 0.6877637130801688,
      "exact_match_stderr,get_response": 0.030165137867847004
    },
    "mmlu_international_law_generative": {
      "alias": "  - international_law",
      "exact_match,get_response": 0.7355371900826446,
      "exact_match_stderr,get_response": 0.04026187527591204
    },
    "mmlu_jurisprudence_generative": {
      "alias": "  - jurisprudence",
      "exact_match,get_response": 0.5648148148148148,
      "exact_match_stderr,get_response": 0.04792898170907061
    },
    "mmlu_logical_fallacies_generative": {
      "alias": "  - logical_fallacies",
      "exact_match,get_response": 0.588957055214724,
      "exact_match_stderr,get_response": 0.038656978537853624
    },
    "mmlu_moral_disputes_generative": {
      "alias": "  - moral_disputes",
      "exact_match,get_response": 0.6213872832369942,
      "exact_match_stderr,get_response": 0.02611374936131034
    },
    "mmlu_moral_scenarios_generative": {
      "alias": "  - moral_scenarios",
      "exact_match,get_response": 0.2581005586592179,
      "exact_match_stderr,get_response": 0.014635185616527829
    },
    "mmlu_philosophy_generative": {
      "alias": "  - philosophy",
      "exact_match,get_response": 0.5434083601286174,
      "exact_match_stderr,get_response": 0.0282908690541976
    },
    "mmlu_prehistory_generative": {
      "alias": "  - prehistory",
      "exact_match,get_response": 0.558641975308642,
      "exact_match_stderr,get_response": 0.027628737155668773
    },
    "mmlu_professional_law_generative": {
      "alias": "  - professional_law",
      "exact_match,get_response": 0.4439374185136897,
      "exact_match_stderr,get_response": 0.012689708167787687
    },
    "mmlu_world_religions_generative": {
      "alias": "  - world_religions",
      "exact_match,get_response": 0.14619883040935672,
      "exact_match_stderr,get_response": 0.027097290118070806
    },
    "other": {
      "exact_match,get_response": 0.5600257483102672,
      "exact_match_stderr,get_response": 0.008648672860471473,
      "alias": " - other"
    },
    "mmlu_business_ethics_generative": {
      "alias": "  - business_ethics",
      "exact_match,get_response": 0.5,
      "exact_match_stderr,get_response": 0.050251890762960605
    },
    "mmlu_clinical_knowledge_generative": {
      "alias": "  - clinical_knowledge",
      "exact_match,get_response": 0.6641509433962264,
      "exact_match_stderr,get_response": 0.029067220146644823
    },
    "mmlu_college_medicine_generative": {
      "alias": "  - college_medicine",
      "exact_match,get_response": 0.5491329479768786,
      "exact_match_stderr,get_response": 0.03794012674697028
    },
    "mmlu_global_facts_generative": {
      "alias": "  - global_facts",
      "exact_match,get_response": 0.06,
      "exact_match_stderr,get_response": 0.023868325657594204
    },
    "mmlu_human_aging_generative": {
      "alias": "  - human_aging",
      "exact_match,get_response": 0.5964125560538116,
      "exact_match_stderr,get_response": 0.032928028193303135
    },
    "mmlu_management_generative": {
      "alias": "  - management",
      "exact_match,get_response": 0.5242718446601942,
      "exact_match_stderr,get_response": 0.049449010929737795
    },
    "mmlu_marketing_generative": {
      "alias": "  - marketing",
      "exact_match,get_response": 0.7307692307692307,
      "exact_match_stderr,get_response": 0.029058588303748842
    },
    "mmlu_medical_genetics_generative": {
      "alias": "  - medical_genetics",
      "exact_match,get_response": 0.62,
      "exact_match_stderr,get_response": 0.048783173121456316
    },
    "mmlu_miscellaneous_generative": {
      "alias": "  - miscellaneous",
      "exact_match,get_response": 0.545338441890166,
      "exact_match_stderr,get_response": 0.0178063045850526
    },
    "mmlu_nutrition_generative": {
      "alias": "  - nutrition",
      "exact_match,get_response": 0.6045751633986928,
      "exact_match_stderr,get_response": 0.027996723180631445
    },
    "mmlu_professional_accounting_generative": {
      "alias": "  - professional_accounting",
      "exact_match,get_response": 0.4929078014184397,
      "exact_match_stderr,get_response": 0.02982449855912901
    },
    "mmlu_professional_medicine_generative": {
      "alias": "  - professional_medicine",
      "exact_match,get_response": 0.6617647058823529,
      "exact_match_stderr,get_response": 0.028739328513983583
    },
    "mmlu_virology_generative": {
      "alias": "  - virology",
      "exact_match,get_response": 0.37349397590361444,
      "exact_match_stderr,get_response": 0.037658451171688624
    },
    "social sciences": {
      "exact_match,get_response": 0.6057848553786155,
      "exact_match_stderr,get_response": 0.008651645033544174,
      "alias": " - social sciences"
    },
    "mmlu_econometrics_generative": {
      "alias": "  - econometrics",
      "exact_match,get_response": 0.43859649122807015,
      "exact_match_stderr,get_response": 0.04668000738510455
    },
    "mmlu_high_school_geography_generative": {
      "alias": "  - high_school_geography",
      "exact_match,get_response": 0.5505050505050505,
      "exact_match_stderr,get_response": 0.035441324919479704
    },
    "mmlu_high_school_government_and_politics_generative": {
      "alias": "  - high_school_government_and_politics",
      "exact_match,get_response": 0.7305699481865285,
      "exact_match_stderr,get_response": 0.03201867122877794
    },
    "mmlu_high_school_macroeconomics_generative": {
      "alias": "  - high_school_macroeconomics",
      "exact_match,get_response": 0.6205128205128205,
      "exact_match_stderr,get_response": 0.024603626924097413
    },
    "mmlu_high_school_microeconomics_generative": {
      "alias": "  - high_school_microeconomics",
      "exact_match,get_response": 0.7016806722689075,
      "exact_match_stderr,get_response": 0.029719142876342863
    },
    "mmlu_high_school_psychology_generative": {
      "alias": "  - high_school_psychology",
      "exact_match,get_response": 0.6220183486238532,
      "exact_match_stderr,get_response": 0.02078918706672811
    },
    "mmlu_human_sexuality_generative": {
      "alias": "  - human_sexuality",
      "exact_match,get_response": 0.5038167938931297,
      "exact_match_stderr,get_response": 0.04385162325601553
    },
    "mmlu_professional_psychology_generative": {
      "alias": "  - professional_psychology",
      "exact_match,get_response": 0.545751633986928,
      "exact_match_stderr,get_response": 0.020142974553795198
    },
    "mmlu_public_relations_generative": {
      "alias": "  - public_relations",
      "exact_match,get_response": 0.3090909090909091,
      "exact_match_stderr,get_response": 0.044262946482000985
    },
    "mmlu_security_studies_generative": {
      "alias": "  - security_studies",
      "exact_match,get_response": 0.6448979591836734,
      "exact_match_stderr,get_response": 0.03063565515038764
    },
    "mmlu_sociology_generative": {
      "alias": "  - sociology",
      "exact_match,get_response": 0.7661691542288557,
      "exact_match_stderr,get_response": 0.02992941540834839
    },
    "mmlu_us_foreign_policy_generative": {
      "alias": "  - us_foreign_policy",
      "exact_match,get_response": 0.7,
      "exact_match_stderr,get_response": 0.046056618647183814
    },
    "stem": {
      "exact_match,get_response": 0.4538534728829686,
      "exact_match_stderr,get_response": 0.008387726905647309,
      "alias": " - stem"
    },
    "mmlu_abstract_algebra_generative": {
      "alias": "  - abstract_algebra",
      "exact_match,get_response": 0.29,
      "exact_match_stderr,get_response": 0.04560480215720684
    },
    "mmlu_anatomy_generative": {
      "alias": "  - anatomy",
      "exact_match,get_response": 0.5407407407407407,
      "exact_match_stderr,get_response": 0.04304979692464242
    },
    "mmlu_astronomy_generative": {
      "alias": "  - astronomy",
      "exact_match,get_response": 0.5723684210526315,
      "exact_match_stderr,get_response": 0.040260970832965634
    },
    "mmlu_college_biology_generative": {
      "alias": "  - college_biology",
      "exact_match,get_response": 0.7569444444444444,
      "exact_match_stderr,get_response": 0.03586879280080343
    },
    "mmlu_college_chemistry_generative": {
      "alias": "  - college_chemistry",
      "exact_match,get_response": 0.36,
      "exact_match_stderr,get_response": 0.04824181513244218
    },
    "mmlu_college_computer_science_generative": {
      "alias": "  - college_computer_science",
      "exact_match,get_response": 0.47,
      "exact_match_stderr,get_response": 0.050161355804659205
    },
    "mmlu_college_mathematics_generative": {
      "alias": "  - college_mathematics",
      "exact_match,get_response": 0.31,
      "exact_match_stderr,get_response": 0.04648231987117316
    },
    "mmlu_college_physics_generative": {
      "alias": "  - college_physics",
      "exact_match,get_response": 0.45098039215686275,
      "exact_match_stderr,get_response": 0.04951218252396262
    },
    "mmlu_computer_security_generative": {
      "alias": "  - computer_security",
      "exact_match,get_response": 0.66,
      "exact_match_stderr,get_response": 0.04760952285695237
    },
    "mmlu_conceptual_physics_generative": {
      "alias": "  - conceptual_physics",
      "exact_match,get_response": 0.5659574468085107,
      "exact_match_stderr,get_response": 0.03240038086792747
    },
    "mmlu_electrical_engineering_generative": {
      "alias": "  - electrical_engineering",
      "exact_match,get_response": 0.41379310344827586,
      "exact_match_stderr,get_response": 0.04104269211806232
    },
    "mmlu_elementary_mathematics_generative": {
      "alias": "  - elementary_mathematics",
      "exact_match,get_response": 0.18253968253968253,
      "exact_match_stderr,get_response": 0.01989487936717555
    },
    "mmlu_high_school_biology_generative": {
      "alias": "  - high_school_biology",
      "exact_match,get_response": 0.7,
      "exact_match_stderr,get_response": 0.026069362295335144
    },
    "mmlu_high_school_chemistry_generative": {
      "alias": "  - high_school_chemistry",
      "exact_match,get_response": 0.49261083743842365,
      "exact_match_stderr,get_response": 0.03517603540361008
    },
    "mmlu_high_school_computer_science_generative": {
      "alias": "  - high_school_computer_science",
      "exact_match,get_response": 0.54,
      "exact_match_stderr,get_response": 0.05009082659620332
    },
    "mmlu_high_school_mathematics_generative": {
      "alias": "  - high_school_mathematics",
      "exact_match,get_response": 0.28888888888888886,
      "exact_match_stderr,get_response": 0.027634907264178544
    },
    "mmlu_high_school_physics_generative": {
      "alias": "  - high_school_physics",
      "exact_match,get_response": 0.40397350993377484,
      "exact_match_stderr,get_response": 0.040064856853653415
    },
    "mmlu_high_school_statistics_generative": {
      "alias": "  - high_school_statistics",
      "exact_match,get_response": 0.44907407407407407,
      "exact_match_stderr,get_response": 0.03392238405321616
    },
    "mmlu_machine_learning_generative": {
      "alias": "  - machine_learning",
      "exact_match,get_response": 0.3392857142857143,
      "exact_match_stderr,get_response": 0.04493949068613539
    }
  },
  "groups": {
    "mmlu_generative": {
      "exact_match,get_response": 0.513317191283293,
      "exact_match_stderr,get_response": 0.004031396763377948,
      "alias": "mmlu (generative)"
    },
    "humanities": {
      "exact_match,get_response": 0.4618490967056323,
      "exact_match_stderr,get_response": 0.006966477629221215,
      "alias": " - humanities"
    },
    "other": {
      "exact_match,get_response": 0.5600257483102672,
      "exact_match_stderr,get_response": 0.008648672860471473,
      "alias": " - other"
    },
    "social sciences": {
      "exact_match,get_response": 0.6057848553786155,
      "exact_match_stderr,get_response": 0.008651645033544174,
      "alias": " - social sciences"
    },
    "stem": {
      "exact_match,get_response": 0.4538534728829686,
      "exact_match_stderr,get_response": 0.008387726905647309,
      "alias": " - stem"
    }
  },
  "group_subtasks": {
    "humanities": [
      "mmlu_moral_scenarios_generative",
      "mmlu_professional_law_generative",
      "mmlu_high_school_european_history_generative",
      "mmlu_world_religions_generative",
      "mmlu_high_school_us_history_generative",
      "mmlu_high_school_world_history_generative",
      "mmlu_logical_fallacies_generative",
      "mmlu_formal_logic_generative",
      "mmlu_philosophy_generative",
      "mmlu_jurisprudence_generative",
      "mmlu_international_law_generative",
      "mmlu_prehistory_generative",
      "mmlu_moral_disputes_generative"
    ],
    "social sciences": [
      "mmlu_human_sexuality_generative",
      "mmlu_sociology_generative",
      "mmlu_public_relations_generative",
      "mmlu_high_school_psychology_generative",
      "mmlu_security_studies_generative",
      "mmlu_us_foreign_policy_generative",
      "mmlu_high_school_government_and_politics_generative",
      "mmlu_econometrics_generative",
      "mmlu_professional_psychology_generative",
      "mmlu_high_school_geography_generative",
      "mmlu_high_school_macroeconomics_generative",
      "mmlu_high_school_microeconomics_generative"
    ],
    "other": [
      "mmlu_clinical_knowledge_generative",
      "mmlu_miscellaneous_generative",
      "mmlu_medical_genetics_generative",
      "mmlu_virology_generative",
      "mmlu_human_aging_generative",
      "mmlu_nutrition_generative",
      "mmlu_management_generative",
      "mmlu_global_facts_generative",
      "mmlu_marketing_generative",
      "mmlu_college_medicine_generative",
      "mmlu_professional_medicine_generative",
      "mmlu_professional_accounting_generative",
      "mmlu_business_ethics_generative"
    ],
    "stem": [
      "mmlu_college_chemistry_generative",
      "mmlu_electrical_engineering_generative",
      "mmlu_high_school_statistics_generative",
      "mmlu_high_school_mathematics_generative",
      "mmlu_high_school_chemistry_generative",
      "mmlu_college_computer_science_generative",
      "mmlu_elementary_mathematics_generative",
      "mmlu_high_school_physics_generative",
      "mmlu_college_physics_generative",
      "mmlu_abstract_algebra_generative",
      "mmlu_computer_security_generative",
      "mmlu_astronomy_generative",
      "mmlu_high_school_computer_science_generative",
      "mmlu_conceptual_physics_generative",
      "mmlu_anatomy_generative",
      "mmlu_college_mathematics_generative",
      "mmlu_high_school_biology_generative",
      "mmlu_college_biology_generative",
      "mmlu_machine_learning_generative"
    ],
    "mmlu_generative": [
      "stem",
      "other",
      "social sciences",
      "humanities"
    ]
  },
  "configs": {
    "mmlu_abstract_algebra_generative": {
      "task": "mmlu_abstract_algebra_generative",
      "task_alias": "abstract_algebra",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_anatomy_generative": {
      "task": "mmlu_anatomy_generative",
      "task_alias": "anatomy",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_astronomy_generative": {
      "task": "mmlu_astronomy_generative",
      "task_alias": "astronomy",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_business_ethics_generative": {
      "task": "mmlu_business_ethics_generative",
      "task_alias": "business_ethics",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_clinical_knowledge_generative": {
      "task": "mmlu_clinical_knowledge_generative",
      "task_alias": "clinical_knowledge",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_biology_generative": {
      "task": "mmlu_college_biology_generative",
      "task_alias": "college_biology",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_chemistry_generative": {
      "task": "mmlu_college_chemistry_generative",
      "task_alias": "college_chemistry",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_computer_science_generative": {
      "task": "mmlu_college_computer_science_generative",
      "task_alias": "college_computer_science",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_mathematics_generative": {
      "task": "mmlu_college_mathematics_generative",
      "task_alias": "college_mathematics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_medicine_generative": {
      "task": "mmlu_college_medicine_generative",
      "task_alias": "college_medicine",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_college_physics_generative": {
      "task": "mmlu_college_physics_generative",
      "task_alias": "college_physics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_computer_security_generative": {
      "task": "mmlu_computer_security_generative",
      "task_alias": "computer_security",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_conceptual_physics_generative": {
      "task": "mmlu_conceptual_physics_generative",
      "task_alias": "conceptual_physics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_econometrics_generative": {
      "task": "mmlu_econometrics_generative",
      "task_alias": "econometrics",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "econometrics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_electrical_engineering_generative": {
      "task": "mmlu_electrical_engineering_generative",
      "task_alias": "electrical_engineering",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_elementary_mathematics_generative": {
      "task": "mmlu_elementary_mathematics_generative",
      "task_alias": "elementary_mathematics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_formal_logic_generative": {
      "task": "mmlu_formal_logic_generative",
      "task_alias": "formal_logic",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "formal_logic",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_global_facts_generative": {
      "task": "mmlu_global_facts_generative",
      "task_alias": "global_facts",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_biology_generative": {
      "task": "mmlu_high_school_biology_generative",
      "task_alias": "high_school_biology",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_chemistry_generative": {
      "task": "mmlu_high_school_chemistry_generative",
      "task_alias": "high_school_chemistry",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_computer_science_generative": {
      "task": "mmlu_high_school_computer_science_generative",
      "task_alias": "high_school_computer_science",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_european_history_generative": {
      "task": "mmlu_high_school_european_history_generative",
      "task_alias": "high_school_european_history",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_european_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_geography_generative": {
      "task": "mmlu_high_school_geography_generative",
      "task_alias": "high_school_geography",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_government_and_politics_generative": {
      "task": "mmlu_high_school_government_and_politics_generative",
      "task_alias": "high_school_government_and_politics",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_government_and_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_macroeconomics_generative": {
      "task": "mmlu_high_school_macroeconomics_generative",
      "task_alias": "high_school_macroeconomics",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_macroeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_mathematics_generative": {
      "task": "mmlu_high_school_mathematics_generative",
      "task_alias": "high_school_mathematics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_microeconomics_generative": {
      "task": "mmlu_high_school_microeconomics_generative",
      "task_alias": "high_school_microeconomics",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_microeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_physics_generative": {
      "task": "mmlu_high_school_physics_generative",
      "task_alias": "high_school_physics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_psychology_generative": {
      "task": "mmlu_high_school_psychology_generative",
      "task_alias": "high_school_psychology",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_statistics_generative": {
      "task": "mmlu_high_school_statistics_generative",
      "task_alias": "high_school_statistics",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_us_history_generative": {
      "task": "mmlu_high_school_us_history_generative",
      "task_alias": "high_school_us_history",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_us_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_high_school_world_history_generative": {
      "task": "mmlu_high_school_world_history_generative",
      "task_alias": "high_school_world_history",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "high_school_world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_human_aging_generative": {
      "task": "mmlu_human_aging_generative",
      "task_alias": "human_aging",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "human_aging",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_human_sexuality_generative": {
      "task": "mmlu_human_sexuality_generative",
      "task_alias": "human_sexuality",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_international_law_generative": {
      "task": "mmlu_international_law_generative",
      "task_alias": "international_law",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about international law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_jurisprudence_generative": {
      "task": "mmlu_jurisprudence_generative",
      "task_alias": "jurisprudence",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_logical_fallacies_generative": {
      "task": "mmlu_logical_fallacies_generative",
      "task_alias": "logical_fallacies",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "logical_fallacies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_machine_learning_generative": {
      "task": "mmlu_machine_learning_generative",
      "task_alias": "machine_learning",
      "tag": "mmlu_stem_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_management_generative": {
      "task": "mmlu_management_generative",
      "task_alias": "management",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about management.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_marketing_generative": {
      "task": "mmlu_marketing_generative",
      "task_alias": "marketing",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_medical_genetics_generative": {
      "task": "mmlu_medical_genetics_generative",
      "task_alias": "medical_genetics",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_miscellaneous_generative": {
      "task": "mmlu_miscellaneous_generative",
      "task_alias": "miscellaneous",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "miscellaneous",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_moral_disputes_generative": {
      "task": "mmlu_moral_disputes_generative",
      "task_alias": "moral_disputes",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "moral_disputes",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_moral_scenarios_generative": {
      "task": "mmlu_moral_scenarios_generative",
      "task_alias": "moral_scenarios",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "moral_scenarios",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_nutrition_generative": {
      "task": "mmlu_nutrition_generative",
      "task_alias": "nutrition",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_philosophy_generative": {
      "task": "mmlu_philosophy_generative",
      "task_alias": "philosophy",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_prehistory_generative": {
      "task": "mmlu_prehistory_generative",
      "task_alias": "prehistory",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "prehistory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_professional_accounting_generative": {
      "task": "mmlu_professional_accounting_generative",
      "task_alias": "professional_accounting",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_professional_law_generative": {
      "task": "mmlu_professional_law_generative",
      "task_alias": "professional_law",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_professional_medicine_generative": {
      "task": "mmlu_professional_medicine_generative",
      "task_alias": "professional_medicine",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_professional_psychology_generative": {
      "task": "mmlu_professional_psychology_generative",
      "task_alias": "professional_psychology",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_public_relations_generative": {
      "task": "mmlu_public_relations_generative",
      "task_alias": "public_relations",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_security_studies_generative": {
      "task": "mmlu_security_studies_generative",
      "task_alias": "security_studies",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "security_studies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_sociology_generative": {
      "task": "mmlu_sociology_generative",
      "task_alias": "sociology",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_us_foreign_policy_generative": {
      "task": "mmlu_us_foreign_policy_generative",
      "task_alias": "us_foreign_policy",
      "tag": "mmlu_social_sciences_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "us_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_virology_generative": {
      "task": "mmlu_virology_generative",
      "task_alias": "virology",
      "tag": "mmlu_other_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about virology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    },
    "mmlu_world_religions_generative": {
      "task": "mmlu_world_religions_generative",
      "task_alias": "world_religions",
      "tag": "mmlu_humanities_generative",
      "dataset_path": "cais/mmlu",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer]}}",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_punctuation": true,
          "ignore_case": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "\n"
        ]
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get_response",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "^(.*?)(?=\\n|$)"
            },
            {
              "function": "remove_whitespace"
            },
            {
              "function": "regex",
              "regex_pattern": "^(.*?)\\s*$"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 3.0,
        "base_url": "http://localhost:8400/v1/completions",
        "model": "FP16-FP8",
        "tokenizer": "/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct"
      }
    }
  },
  "versions": {
    "humanities": null,
    "mmlu_abstract_algebra_generative": 3.0,
    "mmlu_anatomy_generative": 3.0,
    "mmlu_astronomy_generative": 3.0,
    "mmlu_business_ethics_generative": 3.0,
    "mmlu_clinical_knowledge_generative": 3.0,
    "mmlu_college_biology_generative": 3.0,
    "mmlu_college_chemistry_generative": 3.0,
    "mmlu_college_computer_science_generative": 3.0,
    "mmlu_college_mathematics_generative": 3.0,
    "mmlu_college_medicine_generative": 3.0,
    "mmlu_college_physics_generative": 3.0,
    "mmlu_computer_security_generative": 3.0,
    "mmlu_conceptual_physics_generative": 3.0,
    "mmlu_econometrics_generative": 3.0,
    "mmlu_electrical_engineering_generative": 3.0,
    "mmlu_elementary_mathematics_generative": 3.0,
    "mmlu_formal_logic_generative": 3.0,
    "mmlu_generative": 3,
    "mmlu_global_facts_generative": 3.0,
    "mmlu_high_school_biology_generative": 3.0,
    "mmlu_high_school_chemistry_generative": 3.0,
    "mmlu_high_school_computer_science_generative": 3.0,
    "mmlu_high_school_european_history_generative": 3.0,
    "mmlu_high_school_geography_generative": 3.0,
    "mmlu_high_school_government_and_politics_generative": 3.0,
    "mmlu_high_school_macroeconomics_generative": 3.0,
    "mmlu_high_school_mathematics_generative": 3.0,
    "mmlu_high_school_microeconomics_generative": 3.0,
    "mmlu_high_school_physics_generative": 3.0,
    "mmlu_high_school_psychology_generative": 3.0,
    "mmlu_high_school_statistics_generative": 3.0,
    "mmlu_high_school_us_history_generative": 3.0,
    "mmlu_high_school_world_history_generative": 3.0,
    "mmlu_human_aging_generative": 3.0,
    "mmlu_human_sexuality_generative": 3.0,
    "mmlu_international_law_generative": 3.0,
    "mmlu_jurisprudence_generative": 3.0,
    "mmlu_logical_fallacies_generative": 3.0,
    "mmlu_machine_learning_generative": 3.0,
    "mmlu_management_generative": 3.0,
    "mmlu_marketing_generative": 3.0,
    "mmlu_medical_genetics_generative": 3.0,
    "mmlu_miscellaneous_generative": 3.0,
    "mmlu_moral_disputes_generative": 3.0,
    "mmlu_moral_scenarios_generative": 3.0,
    "mmlu_nutrition_generative": 3.0,
    "mmlu_philosophy_generative": 3.0,
    "mmlu_prehistory_generative": 3.0,
    "mmlu_professional_accounting_generative": 3.0,
    "mmlu_professional_law_generative": 3.0,
    "mmlu_professional_medicine_generative": 3.0,
    "mmlu_professional_psychology_generative": 3.0,
    "mmlu_public_relations_generative": 3.0,
    "mmlu_security_studies_generative": 3.0,
    "mmlu_sociology_generative": 3.0,
    "mmlu_us_foreign_policy_generative": 3.0,
    "mmlu_virology_generative": 3.0,
    "mmlu_world_religions_generative": 3.0,
    "other": null,
    "social sciences": null,
    "stem": null
  },
  "n-shot": {
    "mmlu_abstract_algebra_generative": 0,
    "mmlu_anatomy_generative": 0,
    "mmlu_astronomy_generative": 0,
    "mmlu_business_ethics_generative": 0,
    "mmlu_clinical_knowledge_generative": 0,
    "mmlu_college_biology_generative": 0,
    "mmlu_college_chemistry_generative": 0,
    "mmlu_college_computer_science_generative": 0,
    "mmlu_college_mathematics_generative": 0,
    "mmlu_college_medicine_generative": 0,
    "mmlu_college_physics_generative": 0,
    "mmlu_computer_security_generative": 0,
    "mmlu_conceptual_physics_generative": 0,
    "mmlu_econometrics_generative": 0,
    "mmlu_electrical_engineering_generative": 0,
    "mmlu_elementary_mathematics_generative": 0,
    "mmlu_formal_logic_generative": 0,
    "mmlu_global_facts_generative": 0,
    "mmlu_high_school_biology_generative": 0,
    "mmlu_high_school_chemistry_generative": 0,
    "mmlu_high_school_computer_science_generative": 0,
    "mmlu_high_school_european_history_generative": 0,
    "mmlu_high_school_geography_generative": 0,
    "mmlu_high_school_government_and_politics_generative": 0,
    "mmlu_high_school_macroeconomics_generative": 0,
    "mmlu_high_school_mathematics_generative": 0,
    "mmlu_high_school_microeconomics_generative": 0,
    "mmlu_high_school_physics_generative": 0,
    "mmlu_high_school_psychology_generative": 0,
    "mmlu_high_school_statistics_generative": 0,
    "mmlu_high_school_us_history_generative": 0,
    "mmlu_high_school_world_history_generative": 0,
    "mmlu_human_aging_generative": 0,
    "mmlu_human_sexuality_generative": 0,
    "mmlu_international_law_generative": 0,
    "mmlu_jurisprudence_generative": 0,
    "mmlu_logical_fallacies_generative": 0,
    "mmlu_machine_learning_generative": 0,
    "mmlu_management_generative": 0,
    "mmlu_marketing_generative": 0,
    "mmlu_medical_genetics_generative": 0,
    "mmlu_miscellaneous_generative": 0,
    "mmlu_moral_disputes_generative": 0,
    "mmlu_moral_scenarios_generative": 0,
    "mmlu_nutrition_generative": 0,
    "mmlu_philosophy_generative": 0,
    "mmlu_prehistory_generative": 0,
    "mmlu_professional_accounting_generative": 0,
    "mmlu_professional_law_generative": 0,
    "mmlu_professional_medicine_generative": 0,
    "mmlu_professional_psychology_generative": 0,
    "mmlu_public_relations_generative": 0,
    "mmlu_security_studies_generative": 0,
    "mmlu_sociology_generative": 0,
    "mmlu_us_foreign_policy_generative": 0,
    "mmlu_virology_generative": 0,
    "mmlu_world_religions_generative": 0
  },
  "higher_is_better": {
    "humanities": {
      "exact_match": true
    },
    "mmlu_abstract_algebra_generative": {
      "exact_match": true
    },
    "mmlu_anatomy_generative": {
      "exact_match": true
    },
    "mmlu_astronomy_generative": {
      "exact_match": true
    },
    "mmlu_business_ethics_generative": {
      "exact_match": true
    },
    "mmlu_clinical_knowledge_generative": {
      "exact_match": true
    },
    "mmlu_college_biology_generative": {
      "exact_match": true
    },
    "mmlu_college_chemistry_generative": {
      "exact_match": true
    },
    "mmlu_college_computer_science_generative": {
      "exact_match": true
    },
    "mmlu_college_mathematics_generative": {
      "exact_match": true
    },
    "mmlu_college_medicine_generative": {
      "exact_match": true
    },
    "mmlu_college_physics_generative": {
      "exact_match": true
    },
    "mmlu_computer_security_generative": {
      "exact_match": true
    },
    "mmlu_conceptual_physics_generative": {
      "exact_match": true
    },
    "mmlu_econometrics_generative": {
      "exact_match": true
    },
    "mmlu_electrical_engineering_generative": {
      "exact_match": true
    },
    "mmlu_elementary_mathematics_generative": {
      "exact_match": true
    },
    "mmlu_formal_logic_generative": {
      "exact_match": true
    },
    "mmlu_generative": {
      "exact_match": true
    },
    "mmlu_global_facts_generative": {
      "exact_match": true
    },
    "mmlu_high_school_biology_generative": {
      "exact_match": true
    },
    "mmlu_high_school_chemistry_generative": {
      "exact_match": true
    },
    "mmlu_high_school_computer_science_generative": {
      "exact_match": true
    },
    "mmlu_high_school_european_history_generative": {
      "exact_match": true
    },
    "mmlu_high_school_geography_generative": {
      "exact_match": true
    },
    "mmlu_high_school_government_and_politics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_macroeconomics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_mathematics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_microeconomics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_physics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_psychology_generative": {
      "exact_match": true
    },
    "mmlu_high_school_statistics_generative": {
      "exact_match": true
    },
    "mmlu_high_school_us_history_generative": {
      "exact_match": true
    },
    "mmlu_high_school_world_history_generative": {
      "exact_match": true
    },
    "mmlu_human_aging_generative": {
      "exact_match": true
    },
    "mmlu_human_sexuality_generative": {
      "exact_match": true
    },
    "mmlu_international_law_generative": {
      "exact_match": true
    },
    "mmlu_jurisprudence_generative": {
      "exact_match": true
    },
    "mmlu_logical_fallacies_generative": {
      "exact_match": true
    },
    "mmlu_machine_learning_generative": {
      "exact_match": true
    },
    "mmlu_management_generative": {
      "exact_match": true
    },
    "mmlu_marketing_generative": {
      "exact_match": true
    },
    "mmlu_medical_genetics_generative": {
      "exact_match": true
    },
    "mmlu_miscellaneous_generative": {
      "exact_match": true
    },
    "mmlu_moral_disputes_generative": {
      "exact_match": true
    },
    "mmlu_moral_scenarios_generative": {
      "exact_match": true
    },
    "mmlu_nutrition_generative": {
      "exact_match": true
    },
    "mmlu_philosophy_generative": {
      "exact_match": true
    },
    "mmlu_prehistory_generative": {
      "exact_match": true
    },
    "mmlu_professional_accounting_generative": {
      "exact_match": true
    },
    "mmlu_professional_law_generative": {
      "exact_match": true
    },
    "mmlu_professional_medicine_generative": {
      "exact_match": true
    },
    "mmlu_professional_psychology_generative": {
      "exact_match": true
    },
    "mmlu_public_relations_generative": {
      "exact_match": true
    },
    "mmlu_security_studies_generative": {
      "exact_match": true
    },
    "mmlu_sociology_generative": {
      "exact_match": true
    },
    "mmlu_us_foreign_policy_generative": {
      "exact_match": true
    },
    "mmlu_virology_generative": {
      "exact_match": true
    },
    "mmlu_world_religions_generative": {
      "exact_match": true
    },
    "other": {
      "exact_match": true
    },
    "social sciences": {
      "exact_match": true
    },
    "stem": {
      "exact_match": true
    }
  },
  "n-samples": {
    "mmlu_college_chemistry_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_electrical_engineering_generative": {
      "original": 145,
      "effective": 145
    },
    "mmlu_high_school_statistics_generative": {
      "original": 216,
      "effective": 216
    },
    "mmlu_high_school_mathematics_generative": {
      "original": 270,
      "effective": 270
    },
    "mmlu_high_school_chemistry_generative": {
      "original": 203,
      "effective": 203
    },
    "mmlu_college_computer_science_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_elementary_mathematics_generative": {
      "original": 378,
      "effective": 378
    },
    "mmlu_high_school_physics_generative": {
      "original": 151,
      "effective": 151
    },
    "mmlu_college_physics_generative": {
      "original": 102,
      "effective": 102
    },
    "mmlu_abstract_algebra_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_computer_security_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_astronomy_generative": {
      "original": 152,
      "effective": 152
    },
    "mmlu_high_school_computer_science_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_conceptual_physics_generative": {
      "original": 235,
      "effective": 235
    },
    "mmlu_anatomy_generative": {
      "original": 135,
      "effective": 135
    },
    "mmlu_college_mathematics_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_high_school_biology_generative": {
      "original": 310,
      "effective": 310
    },
    "mmlu_college_biology_generative": {
      "original": 144,
      "effective": 144
    },
    "mmlu_machine_learning_generative": {
      "original": 112,
      "effective": 112
    },
    "mmlu_clinical_knowledge_generative": {
      "original": 265,
      "effective": 265
    },
    "mmlu_miscellaneous_generative": {
      "original": 783,
      "effective": 783
    },
    "mmlu_medical_genetics_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_virology_generative": {
      "original": 166,
      "effective": 166
    },
    "mmlu_human_aging_generative": {
      "original": 223,
      "effective": 223
    },
    "mmlu_nutrition_generative": {
      "original": 306,
      "effective": 306
    },
    "mmlu_management_generative": {
      "original": 103,
      "effective": 103
    },
    "mmlu_global_facts_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_marketing_generative": {
      "original": 234,
      "effective": 234
    },
    "mmlu_college_medicine_generative": {
      "original": 173,
      "effective": 173
    },
    "mmlu_professional_medicine_generative": {
      "original": 272,
      "effective": 272
    },
    "mmlu_professional_accounting_generative": {
      "original": 282,
      "effective": 282
    },
    "mmlu_business_ethics_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_human_sexuality_generative": {
      "original": 131,
      "effective": 131
    },
    "mmlu_sociology_generative": {
      "original": 201,
      "effective": 201
    },
    "mmlu_public_relations_generative": {
      "original": 110,
      "effective": 110
    },
    "mmlu_high_school_psychology_generative": {
      "original": 545,
      "effective": 545
    },
    "mmlu_security_studies_generative": {
      "original": 245,
      "effective": 245
    },
    "mmlu_us_foreign_policy_generative": {
      "original": 100,
      "effective": 100
    },
    "mmlu_high_school_government_and_politics_generative": {
      "original": 193,
      "effective": 193
    },
    "mmlu_econometrics_generative": {
      "original": 114,
      "effective": 114
    },
    "mmlu_professional_psychology_generative": {
      "original": 612,
      "effective": 612
    },
    "mmlu_high_school_geography_generative": {
      "original": 198,
      "effective": 198
    },
    "mmlu_high_school_macroeconomics_generative": {
      "original": 390,
      "effective": 390
    },
    "mmlu_high_school_microeconomics_generative": {
      "original": 238,
      "effective": 238
    },
    "mmlu_moral_scenarios_generative": {
      "original": 895,
      "effective": 895
    },
    "mmlu_professional_law_generative": {
      "original": 1534,
      "effective": 1534
    },
    "mmlu_high_school_european_history_generative": {
      "original": 165,
      "effective": 165
    },
    "mmlu_world_religions_generative": {
      "original": 171,
      "effective": 171
    },
    "mmlu_high_school_us_history_generative": {
      "original": 204,
      "effective": 204
    },
    "mmlu_high_school_world_history_generative": {
      "original": 237,
      "effective": 237
    },
    "mmlu_logical_fallacies_generative": {
      "original": 163,
      "effective": 163
    },
    "mmlu_formal_logic_generative": {
      "original": 126,
      "effective": 126
    },
    "mmlu_philosophy_generative": {
      "original": 311,
      "effective": 311
    },
    "mmlu_jurisprudence_generative": {
      "original": 108,
      "effective": 108
    },
    "mmlu_international_law_generative": {
      "original": 121,
      "effective": 121
    },
    "mmlu_prehistory_generative": {
      "original": 324,
      "effective": 324
    },
    "mmlu_moral_disputes_generative": {
      "original": 346,
      "effective": 346
    }
  },
  "config": {
    "model": "local-completions",
    "model_args": "base_url=http://localhost:8400/v1/completions,model=FP16-FP8,tokenizer=/home/scratch.trt_llm_data/llm-models/llama-3.1-model/Llama-3.1-8B-Instruct",
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "v0.20.0rc0-2073-g89889fb52",
  "date": 1759862315.4557247,
  "pretty_env_info": "PyTorch version: 2.8.0a0+5228986c39.nv25.06\nIs debug build: False\nCUDA used to build PyTorch: 12.9\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.2 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: 18.1.3 (1ubuntu1)\nCMake version: version 3.30.2\nLibc version: glibc-2.39\n\nPython version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-70-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: 12.9.86\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 NVL\nGPU 1: NVIDIA H100 NVL\nGPU 2: NVIDIA H100 NVL\nGPU 3: NVIDIA H100 NVL\n\nNvidia driver version: 580.95.05\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.10.2\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.10.2\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          224\nOn-line CPU(s) list:             0-223\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8480+\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              56\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU(s) scaling MHz:              47%\nCPU max MHz:                     2001.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4000.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       5.3 MiB (112 instances)\nL1i cache:                       3.5 MiB (112 instances)\nL2 cache:                        224 MiB (112 instances)\nL3 cache:                        210 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-55,112-167\nNUMA node1 CPU(s):               56-111,168-223\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] mypy_extensions==1.1.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cuda-nvrtc-cu12==12.9.86\n[pip3] nvidia-cudnn-frontend==1.12.0\n[pip3] nvidia-nccl-cu12==2.28.3\n[pip3] nvtx==0.2.11\n[pip3] onnx==1.19.0\n[pip3] onnx_graphsurgeon==0.5.8\n[pip3] optree==0.16.0\n[pip3] pynvjitlink==0.3.0\n[pip3] pytorch-triton==3.3.0+git96316ce52.nvinternal\n[pip3] torch==2.8.0a0+5228986c39.nv25.6\n[pip3] torch_tensorrt==2.8.0a0\n[pip3] torchao==0.11.0+git\n[pip3] torchprofile==0.0.4\n[pip3] torchvision==0.22.0a0+95f10a4e\n[pip3] triton==3.3.1\n[conda] Could not collect",
  "transformers_version": "4.55.0",
  "lm_eval_version": "0.4.9.1",
  "upper_git_hash": "89889fb5260948b3760e9e69bea6ce05587564a0",
  "tokenizer_pad_token": [
    null,
    "None"
  ],
  "tokenizer_eos_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_bos_token": [
    "<|begin_of_text|>",
    "128000"
  ],
  "eot_token_id": 128009,
  "max_length": 2047,
  "task_hashes": {
    "mmlu_college_chemistry_generative": "82f4e2aeb65b8b51326a2229f87009c2b3eae4ef8c2eee3857a49d3543122d83",
    "mmlu_electrical_engineering_generative": "7c8f924db3067a68e5223e6c222e55b3c2cddf00174a43cc18e3d2b63727bdf0",
    "mmlu_high_school_statistics_generative": "e6f46405d41ac6d232aa29aa499e4d7ea0ce5b15a7c39a0e53f60af23ecea9e4",
    "mmlu_high_school_mathematics_generative": "398091eff6b3eac3969ac200916148582792e9a032b1b192d145f516f8b7713b",
    "mmlu_high_school_chemistry_generative": "2b386ce19f00d4591e12f3e6300aa4589a093d27ad38c286c5491bfa6108442e",
    "mmlu_college_computer_science_generative": "929a90f00f9779521e9892fdc733a58d75565a6b4749f02d7d363fcab68b4678",
    "mmlu_elementary_mathematics_generative": "d6e4f387f92ec71aeb8a22224a965c9575445b53dd3566c1fcac4a5451003948",
    "mmlu_high_school_physics_generative": "dd85fc130698900196624b8d4b9e09998e66ec6557dece53014bc3401c2fa859",
    "mmlu_college_physics_generative": "46abe5dfdd8bd30bc41afc97fa26b4b8e75303926c491b130acc6f95bb4c3662",
    "mmlu_abstract_algebra_generative": "f40ed8e978374386781e1773020fb13dc1debd3e5e4ef78b6675a0fcec930fba",
    "mmlu_computer_security_generative": "92b746f2981254dc9233991373b7b5bc9beeb3b4909f1ba244e5cae4536ee733",
    "mmlu_astronomy_generative": "6c7424c1737ee5446289c95a98dfdd1d9ed2b804a93a585ae7a47d4ef0080939",
    "mmlu_high_school_computer_science_generative": "cd15a277131a9234a2b2ead8b61e2a312d4be36e86743b406013e5f74012f5fa",
    "mmlu_conceptual_physics_generative": "7312c57ccd787417a03af2ef3c1791550a95279e2e87f49342b5eae7173ecbe9",
    "mmlu_anatomy_generative": "e78ce6858f45acf5071c370d703537ad0f24edb6b2e9b843f5607e763117a41f",
    "mmlu_college_mathematics_generative": "9472826a0de07b1509d7eb2ffdbc630bedbe2ecff378dcaea061fb7fa66fdaee",
    "mmlu_high_school_biology_generative": "c98f90a329ee303b96f692a97e711a5b19c133ed80573e914385d26ea685cafb",
    "mmlu_college_biology_generative": "1dae1479d766334a4689c8a41cc5290c6a2a99a90f2367f6102bec923b309b01",
    "mmlu_machine_learning_generative": "ae1ae3ad7802f4b0aacdbede4e88429005306f0dfa01f3626bd8daa33780003d",
    "mmlu_clinical_knowledge_generative": "1c11e690a0fe19a5f97d4e6f1bda67da38f36a8a4f6c04c56b85cd95422ae520",
    "mmlu_miscellaneous_generative": "ecd9d90976d550e82ffabe84a412e6930bd125ec30fc47a845840a32b5126fa4",
    "mmlu_medical_genetics_generative": "3097ce46f79cf7804cae0e7f4026d8535f4ffa3a74668eb2edb90afae412fd02",
    "mmlu_virology_generative": "a3ea01b358953f275d0494156cea6ec3217da4859b3b481cf3fe3e427f31ba00",
    "mmlu_human_aging_generative": "9b89d02ed340f2addc93118aaf3404b65f97574f0a5d18f74658a837611be2a7",
    "mmlu_nutrition_generative": "517308b183c65d5add10058add2202902f81710c724935d5f6d1c251e6e9b821",
    "mmlu_management_generative": "622c30ae26e158f93b262b86e9943568b25fa5261b784691463ec40ac15863fd",
    "mmlu_global_facts_generative": "e302f79bbb10b3d81707685b22c98787ee18f6a6614d432fa7a990c5d9a5eb14",
    "mmlu_marketing_generative": "3a95596698272e3825fdbe8747854ea13c40cbb36a4469ae30178b81c512cbf1",
    "mmlu_college_medicine_generative": "56e3152dee8d3ba7d1b087db0480132ef53a7a190da0badf4fe6c1de9911c209",
    "mmlu_professional_medicine_generative": "3d7366dc3dc232a61ba40288db70ca9739107e59a3a24d71dedc0feeecb797b2",
    "mmlu_professional_accounting_generative": "c508842c72b1ded4c192bbcfd159aadc7a102b9de4378b9b7c9f427a7fd61338",
    "mmlu_business_ethics_generative": "baf583501741fa0810d46ad916d401d3c515e90485adc60397284705a46c97b4",
    "mmlu_human_sexuality_generative": "3c5f361c7bc084413b8b0e981c5c5aacd7c8f49c7f4d0f513bd2b11e0f620ca8",
    "mmlu_sociology_generative": "10720cd0dece7470bc738e36195584131a0d8193475cb5a8e8867964b329d8c4",
    "mmlu_public_relations_generative": "72289be9296240f3ceb55092da080b1fe64ba3e2a62c24f257cb3ae5b091f385",
    "mmlu_high_school_psychology_generative": "55f1967083c1117aebb231ee2d0a5e74ce000ecd92431cf439a41fc2f4690f27",
    "mmlu_security_studies_generative": "bf5999de2f3ad6a49542c5a0d567e112f4f07b3805876409d60c8058a90ced1c",
    "mmlu_us_foreign_policy_generative": "4b63209dc31fefcb38f5e6bb4d3f5f913f0904b1f5e3345ec0b5d6b51c2906a3",
    "mmlu_high_school_government_and_politics_generative": "d3a41a20a294514c7dbdb1c192afc85b285c88eebbcfd4aefb3cb878f515176b",
    "mmlu_econometrics_generative": "69b2aaf6cf8326ed7ed5884442314b825b82a13f5635813513265c9727d4ef50",
    "mmlu_professional_psychology_generative": "8883d0c7a3adbb88b15893be4920381446f79e4b1b5e59e64175cd4380b20f09",
    "mmlu_high_school_geography_generative": "7340ecc538b5405e55ac41266b555dd096b64e3b5fa9c72977f8af3c67af718d",
    "mmlu_high_school_macroeconomics_generative": "24ed0aef23d971fa4fba1a73a9adc62d5ec091c2f137b48fb425f33b1e2af578",
    "mmlu_high_school_microeconomics_generative": "49c6185cbfbdd233774831345209f236316e1a4b65fdde7ef162fb719dce7e58",
    "mmlu_moral_scenarios_generative": "b1882ea4abf19ed5ec698d9be36b1b71ddd7fc8988625f07e1bb9666c5e36241",
    "mmlu_professional_law_generative": "ced7834d71beea8a1d852e4ba7fdd3794a3c93feb93e98b4610d54c7f7469c63",
    "mmlu_high_school_european_history_generative": "a4bfb439eb2d1a8dbef3e01b8cefcd8d6f353527c5fd165573465e1ba8d4fc6f",
    "mmlu_world_religions_generative": "9450b332d87747c6f4ec7a4477ba7f1d8462e2d289b9caa7483c6ed59806ee5e",
    "mmlu_high_school_us_history_generative": "df69af40072528074fc2caa13fbc40f7d9528275122f532c6d79e01374c79431",
    "mmlu_high_school_world_history_generative": "5e5d43634e28a82da86991c50e104e71e5ac611ea3813bda8931067d3596f33d",
    "mmlu_logical_fallacies_generative": "91f337d95d8a3e0b3128cae694f681c7bd9db3e8ebee41cd96cd001f162a2337",
    "mmlu_formal_logic_generative": "4f961b8722b612b55a5d9b622a4e4b5f4b23f427963f6cae23a2cf2ff9105a8f",
    "mmlu_philosophy_generative": "59c32eb5b37cfa627c929ef6a43028f1468d200464a5c926af9f5bece9830809",
    "mmlu_jurisprudence_generative": "d66ddb0583d9ad482ae3ea7a5ce282128464ee92098a67ba2048f51e661ac185",
    "mmlu_international_law_generative": "ab5d25b63fc739d7356a1ab1be616b6e1ba3d1c7b93102bbc61911dc64fd4f2c",
    "mmlu_prehistory_generative": "44777500dbb7f1941733f04258e203abb060ef5678b751ea69e6269c30f024ef",
    "mmlu_moral_disputes_generative": "516b93ca7605688ea726e1860943bd7c75707acfce24da279379e6f7f612d568"
  },
  "model_source": "local-completions",
  "model_name": "FP16-FP8",
  "model_name_sanitized": "FP16-FP8",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 467017.108164435,
  "end_time": 473728.822588767,
  "total_evaluation_time_seconds": "6711.7144243319635"
}