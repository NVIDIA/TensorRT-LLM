# Solution 4: Âü∫‰∫é YAML ÈÖçÁΩÆÁöÑÊµãËØïÊñπÊ°à

## ËÆæËÆ°ÁêÜÂøµ

**‰ΩøÁî®ÁõÆÂΩï+YAMLÊñá‰ª∂ÁªÑÁªáÊµãËØïÈÖçÁΩÆÔºåÁÆÄÂçïÁõ¥ËßÇÔºåÊòì‰∫éÁª¥Êä§**

Ê†∏ÂøÉÂéüÂàôÔºö
1. ‚úÖ **ÊåâÊµãËØïÁ±ªÂûãÂíåÁ±ªÂà´ÂàÜÁõÆÂΩï**Ôºötest_type ‚Üí perf/accuracy ‚Üí ÈÖçÁΩÆÊñá‰ª∂
2. ‚úÖ **YAML ÈÖçÁΩÆÊñá‰ª∂**ÔºöÊØè‰∏™ÊµãËØï‰∏Ä‰∏™Áã¨Á´ãÁöÑ YAML Êñá‰ª∂
3. ‚úÖ **Êñá‰ª∂ÂêçÂç≥ÂÖÉÊï∞ÊçÆ**Ôºö‰ªéÊñá‰ª∂ÂêçËß£ÊûêÊ®°ÂûãÂíåbenchmarkÁ±ªÂûãÔºåÊó†ÈúÄYAML metadata
4. ‚úÖ **ÈªòËÆ§ + Ë¶ÜÁõñÊ®°Âºè**ÔºöÊèê‰æõÈªòËÆ§ metrics ÈÖçÁΩÆÔºåÊåâÈúÄË¶ÜÁõñ
5. ‚úÖ **Â§çÁî®Áé∞ÊúâÂ∑•ÂÖ∑**Ôºö‰ΩøÁî® `disagg/slurm/benchmark/submit.py` Êèê‰∫§‰Ωú‰∏ö
6. ‚úÖ **ÊúÄÂ∞èÊîπÂä®**Ôºö‰øùÁïô pytest Ê°ÜÊû∂ÔºåÂè™ÊîπÈÖçÁΩÆËØªÂèñÊñπÂºè

---

## ÁõÆÂΩïÁªìÊûÑ

```
test_configs/
‚îú‚îÄ‚îÄ disagg/                                    # ÊµãËØïÁ±ªÂûãÔºàdisaggregatedÔºâ
‚îÇ   ‚îú‚îÄ‚îÄ perf/                                  # ÊÄßËÉΩÊµãËØï
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek-r1-fp8_1k1k_tep8_bs32_mtp3_nixl.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek-r1-fp8_1k1k_tep8_bs32_nixl.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek-r1-fp8_1k1k_dep16_bs128_nixl.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek-r1-fp8_8k1k_tep8_bs16_nixl.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llama-70b_1k1k_tep8_bs256_nixl.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ special-model_1k1k_custom_metrics.yaml  # Ëá™ÂÆö‰πâ metrics
‚îÇ   ‚îî‚îÄ‚îÄ accuracy/                              # Á≤æÂ∫¶ÊµãËØï
‚îÇ       ‚îú‚îÄ‚îÄ deepseek-r1-fp8_1k1k_gsm8k.yaml
‚îÇ       ‚îî‚îÄ‚îÄ llama-70b_1k1k_mmlu.yaml
‚îú‚îÄ‚îÄ widep/                                     # Âè¶‰∏ÄÁßçÊµãËØïÁ±ªÂûãÔºàÂèØÈÄâÔºâ
‚îÇ   ‚îú‚îÄ‚îÄ perf/
‚îÇ   ‚îî‚îÄ‚îÄ accuracy/
‚îî‚îÄ‚îÄ templates/                                 # Ê®°ÊùøÊñá‰ª∂ÔºàÂèØÈÄâÔºâ
    ‚îú‚îÄ‚îÄ disagg_perf_template.yaml
    ‚îî‚îÄ‚îÄ disagg_accuracy_template.yaml
```

---

## GPU Á°¨‰ª∂ÊîØÊåÅÊú∫Âà∂

### ÊîØÊåÅÁöÑ GPU Á±ªÂûã

Á≥ªÁªüÊîØÊåÅÂ§öÁßç GPU Á°¨‰ª∂Á±ªÂûãÔºåÊØè‰∏™ÈÖçÁΩÆÂèØ‰ª•ÊåáÂÆöÂÖ∂ÊîØÊåÅÁöÑ GPU ÂàóË°®Ôºö

- **GB200**: NVIDIA GB200 GPU
- **GB300**: NVIDIA GB300 GPU  
- **H100**: NVIDIA H100 GPU
- **B200**: NVIDIA B200 GPU
- **B300**: NVIDIA B300 GPU

### ÈÖçÁΩÆÊñπÂºè

Âú® YAML Êñá‰ª∂ÁöÑ `hardware` ËäÇÁÇπ‰∏ãÊåáÂÆö `supported_gpus` Â≠óÊÆµÔºö

```yaml
hardware:
  gpus_per_node: 4
  num_ctx_servers: 1
  num_gen_servers: 4
  supported_gpus: ["GB200", "GB300"]  # Ê≠§ÈÖçÁΩÆÊîØÊåÅ GB200 Âíå GB300
```

### GPU ËøáÊª§Êú∫Âà∂

1. **ÁéØÂ¢ÉÂèòÈáè**: Á≥ªÁªüÈÄöËøá `GPU_TYPE` ÁéØÂ¢ÉÂèòÈáèËé∑ÂèñÂΩìÂâç GPU Á±ªÂûã
2. **Ëá™Âä®ËøáÊª§**: `ConfigLoader` ‰ºöËá™Âä®ËøáÊª§Êéâ‰∏çÊîØÊåÅÂΩìÂâç GPU ÁöÑÈÖçÁΩÆ
3. **pytest ÂèÇÊï∞Âåñ**: Âè™ÊúâÊîØÊåÅÂΩìÂâç GPU ÁöÑÈÖçÁΩÆ‰ºöË¢´Âä†ËΩΩÂà∞ÊµãËØïÁî®‰æã‰∏≠

### ‰ΩøÁî®Âú∫ÊôØ

#### Âú∫ÊôØ 1: Â§ßÊ®°ÂûãÈÖçÁΩÆÔºà‰ªÖÊîØÊåÅÈ´òÁ´Ø GPUÔºâ
```yaml
hardware:
  supported_gpus: ["GB200", "GB300"]  # ‰ªÖÂú® GB200/GB300 ‰∏äËøêË°å
```

#### Âú∫ÊôØ 2: Â∞èÊ®°ÂûãÈÖçÁΩÆÔºàÊîØÊåÅÂ§öÁßç GPUÔºâ
```yaml
hardware:
  supported_gpus: ["H100", "B200", "B300"]  # ÂèØÂú® H100/B200/B300 ‰∏äËøêË°å
```

#### Âú∫ÊôØ 3: ÈÄöÁî®ÈÖçÁΩÆÔºàÊîØÊåÅÊâÄÊúâ GPUÔºâ
```yaml
hardware:
  supported_gpus: ["GB200", "GB300", "H100", "B200", "B300"]  # ÊîØÊåÅÊâÄÊúâ GPU
```

---

## Metrics ÈÖçÁΩÆËØ¥Êòé

### ÈªòËÆ§ÈÖçÁΩÆÊú∫Âà∂

Á≥ªÁªü‰∏∫‰∏çÂêåÊµãËØïÁ±ªÂà´Êèê‰æõ‰∫Ü**ÈªòËÆ§ÁöÑ metrics ÈÖçÁΩÆ**ÔºåÂ§ßÂ§öÊï∞ÊµãËØïÊó†ÈúÄÂú® YAML ‰∏≠ÈÖçÁΩÆ metrics„ÄÇ

#### ÊÄßËÉΩÊµãËØï (perf) ÈªòËÆ§ÈÖçÁΩÆ
- **Êó•ÂøóÊñá‰ª∂**: `benchmark_result.log`
- **ÊèêÂèñÊåáÊ†á**: TTFT (Time To First Token), E2EL (End-to-End Latency)
- **Ê≠£ÂàôË°®ËææÂºè**: È¢ÑÂÆö‰πâÁöÑ TTFT/E2EL ÊèêÂèñÊ®°Âºè

#### Á≤æÂ∫¶ÊµãËØï (accuracy) ÈªòËÆ§ÈÖçÁΩÆ
- **Êó•ÂøóÊñá‰ª∂**: `accuracy_result.json`
- **ÊèêÂèñÊåáÊ†á**: Accuracy
- **Ê≠£ÂàôË°®ËææÂºè**: È¢ÑÂÆö‰πâÁöÑÂáÜÁ°ÆÁéáÊèêÂèñÊ®°Âºè

### ‰ΩøÁî®Âú∫ÊôØ

#### ‚úÖ Âú∫ÊôØ 1Ôºö‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆÔºàÊé®ËçêÔºå90% ÁöÑÊÉÖÂÜµÔºâ
```yaml
# ‰∏çÈúÄË¶ÅÈÖçÁΩÆ metricsÔºåËá™Âä®‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ
benchmark:
  mode: "e2e"
  multi_round: 8
  concurrency_list: "1 2 4 8 16 36"
  # metrics Ëá™Âä®‰ΩøÁî® perf ÈªòËÆ§ÈÖçÁΩÆ
```

#### ‚úÖ Âú∫ÊôØ 2ÔºöÈÉ®ÂàÜË¶ÜÁõñÔºàÂè™‰øÆÊîπ‰∏™Âà´Â≠óÊÆµÔºâ
```yaml
benchmark:
  mode: "e2e"
  metrics:
    # Âè™Ë¶ÜÁõñ log_fileÔºåpattern Âíå metric_names ÁªßÊâøÈªòËÆ§
    log_file: "custom_benchmark.log"
```

#### ‚úÖ Âú∫ÊôØ 3ÔºöÂÆåÂÖ®Ëá™ÂÆö‰πâÔºàÁâπÊÆäÈúÄÊ±ÇÔºâ
```yaml
benchmark:
  mode: "e2e"
  metrics:
    log_file: "custom_result.log"
    extractor_pattern: "Custom Pattern:\s+([0-9.]+)"
    metric_names: ["CUSTOM_METRIC"]
```

---

## YAML ÈÖçÁΩÆÊñá‰ª∂Ê†ºÂºè

### ÊÄßËÉΩÊµãËØïÈÖçÁΩÆÁ§∫‰æã

#### Á§∫‰æã 1ÔºöÊ†áÂáÜÈÖçÁΩÆÔºà‰ΩøÁî®ÈªòËÆ§ metricsÔºâ

`test_configs/disagg/perf/deepseek-r1-fp8_1k1k_tep8_bs32_mtp3_nixl.yaml`

```yaml
# Metadata - ÊµãËØïÂÖÉÊï∞ÊçÆÔºàÁî®‰∫éËØÜÂà´ÂíåËøáÊª§Ôºâ
metadata:
  model_name: "deepseek-r1-fp4"
  precision: "fp4"
  supported_gpus: ["GB200", "GB300"]  # ÊîØÊåÅÁöÑ GPU Á±ªÂûãÂàóË°®

# SLURM Configuration
slurm:
  script_file: "disaggr_torch.slurm"
  partition: "batch"
  account: "coreai_comparch_trtllm"
  job_time: "02:00:00"
  job_name: "deepseek-r1-fp4-1k1k-tep8-mtp3"
  numa_bind: true

# Benchmark Mode
benchmark:
  mode: "e2e"
  use_nv_sa_benchmark: false
  multi_round: 8
  benchmark_ratio: 0.8
  streaming: true
  concurrency_list: "1 2 4 8 16 36"
  # ‚ö†Ô∏è Ê≥®ÊÑèÔºöÊ≤°Êúâ metrics ÈÖçÁΩÆÔºåÂ∞ÜËá™Âä®‰ΩøÁî® perf ÈªòËÆ§ metrics
  #   - log_file: benchmark_result.log
  #   - metric_names: [DISAGG_SERVER_TTFT, DISAGG_SERVER_E2EL]
  #   - extractor_pattern: È¢ÑÂÆö‰πâÁöÑ TTFT/E2EL ÊèêÂèñÊ®°Âºè

# Hardware Configuration
hardware:
  gpus_per_node: 4
  num_ctx_servers: 1
  num_gen_servers: 4
  supported_gpus: ["GB200", "GB300"]  # ÊîØÊåÅÁöÑ GPU Á±ªÂûãÂàóË°®

# Sequence Configuration
sequence:
  input_length: 1024
  output_length: 1024

# Environment Configuration
environment:
  container_mount: "/lustre:/lustre"
  container_image: "/lustre/fsw/portfolios/coreai/users/deemon/trtllm.sqsh"
  model_path: "/lustre/fsw/portfolios/coreai/users/xqiao/DeepSeek-R1-0528-FP4-V2"
  trtllm_repo: "/lustre/fs1/portfolios/coreai/projects/trtllm"
  build_wheel: false
  dataset_file: "/lustre/fs1/portfolios/coreai/datasets/prompts.json"
  work_dir: "/lustre/fs1/portfolios/coreai/perf_test"

# Profiling Configuration
profiling:
  nsys_on: false

# Worker Configuration
worker_config:
  eplb_num_slots: 0
  
  gen:
    tensor_parallel_size: 8
    moe_expert_parallel_size: 8
    enable_attention_dp: false
    enable_lm_head_tp_in_adp: true
    pipeline_parallel_size: 1
    max_batch_size: 32
    max_num_tokens: 128
    max_seq_len: 2251
    cuda_graph_config:
      enable_padding: true
      batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256]
    print_iter_log: true
    kv_cache_config:
      enable_block_reuse: false
      free_gpu_memory_fraction: 0.9
      dtype: fp8
    moe_config:
      backend: CUTLASS
      use_low_precision_moe_combine: true
    cache_transceiver_config:
      max_tokens_in_buffer: 4608
      backend: NIXL
    stream_interval: 20
    num_postprocess_workers: 4
    speculative_config:
      decoding_type: MTP
      num_nextn_predict_layers: 3
  
  ctx:
    max_batch_size: 4
    max_num_tokens: 4608
    max_seq_len: 1227
    tensor_parallel_size: 4
    moe_expert_parallel_size: 4
    enable_attention_dp: true
    pipeline_parallel_size: 1
    print_iter_log: true
    cuda_graph_config: null
    disable_overlap_scheduler: true
    kv_cache_config:
      enable_block_reuse: false
      free_gpu_memory_fraction: 0.85
      dtype: fp8
    cache_transceiver_config:
      max_tokens_in_buffer: 4608
      backend: NIXL
```

#### Á§∫‰æã 2ÔºöËá™ÂÆö‰πâÊó•ÂøóÊñá‰ª∂ÔºàÈÉ®ÂàÜË¶ÜÁõñÔºâ

`test_configs/disagg/perf/special-model_1k1k_custom_log.yaml`

```yaml
# Â§ßÈÉ®ÂàÜÈÖçÁΩÆ‰∏éÁ§∫‰æã1Áõ∏Âêå...

benchmark:
  mode: "e2e"
  multi_round: 8
  concurrency_list: "1 2 4 8 16 36"
  
  # Âè™Ë¶ÜÁõñ log_fileÔºåÂÖ∂‰ªñ‰ΩøÁî®ÈªòËÆ§
  metrics:
    log_file: "custom_benchmark_result.log"
    # extractor_pattern Âíå metric_names ÁªßÊâøÈªòËÆ§ÂÄº
```

#### Á§∫‰æã 3ÔºöÂÆåÂÖ®Ëá™ÂÆö‰πâ metrics

`test_configs/disagg/perf/special-model_1k1k_full_custom.yaml`

```yaml
# Â§ßÈÉ®ÂàÜÈÖçÁΩÆ‰∏éÁ§∫‰æã1Áõ∏Âêå...

benchmark:
  mode: "e2e"
  multi_round: 8
  
  # ÂÆåÂÖ®Ëá™ÂÆö‰πâ metrics ÈÖçÁΩÆ
  metrics:
    log_file: "throughput_log.txt"
    extractor_pattern: |
      Throughput:\s+([0-9.]+)\s+tokens/s
      Latency:\s+([0-9.]+)\s+ms
    metric_names:
      - "THROUGHPUT_TOKENS_PER_SEC"
      - "AVERAGE_LATENCY_MS"
```

### Á≤æÂ∫¶ÊµãËØïÈÖçÁΩÆÁ§∫‰æã

#### Á§∫‰æã 1ÔºöÊ†áÂáÜÁ≤æÂ∫¶ÊµãËØïÔºà‰ΩøÁî®ÈªòËÆ§ metricsÔºâ

`test_configs/disagg/accuracy/deepseek-r1-fp8_1k1k_gsm8k.yaml`

```yaml
# SLURM Configuration
slurm:
  script_file: "disaggr_torch.slurm"
  partition: "batch"
  account: "coreai_comparch_trtllm"
  job_time: "02:00:00"
  job_name: "deepseek-r1-fp8-1k1k-accuracy"
  numa_bind: true

# Benchmark Mode - Accuracy specific
benchmark:
  mode: "accuracy"
  use_nv_sa_benchmark: false
  multi_round: 1
  benchmark_ratio: 1.0
  streaming: false
  concurrency_list: "1"
  
  # Á≤æÂ∫¶È™åËØÅÂèÇÊï∞
  expected_accuracy: 85.5
  relative_error_threshold: 1.0  # Áõ∏ÂØπËØØÂ∑ÆÈòàÂÄº (%)
  absolute_error_threshold: 0.5  # ÁªùÂØπËØØÂ∑ÆÈòàÂÄº (%)
  
  # ‚ö†Ô∏è Ê≥®ÊÑèÔºöÊ≤°Êúâ metrics ÈÖçÁΩÆÔºåÂ∞ÜËá™Âä®‰ΩøÁî® accuracy ÈªòËÆ§ metrics
  #   - log_file: accuracy_result.json
  #   - metric_names: [ACCURACY]
  #   - extractor_pattern: È¢ÑÂÆö‰πâÁöÑÂáÜÁ°ÆÁéáÊèêÂèñÊ®°Âºè

# Hardware Configuration
hardware:
  gpus_per_node: 4
  num_ctx_servers: 1
  num_gen_servers: 4

# Sequence Configuration
sequence:
  input_length: 1024
  output_length: 1024

# Environment Configuration
environment:
  container_mount: "/lustre:/lustre"
  container_image: "/lustre/fsw/portfolios/coreai/users/deemon/trtllm.sqsh"
  model_path: "/lustre/fsw/portfolios/coreai/users/xqiao/DeepSeek-R1-0528-FP4-V2"
  trtllm_repo: "/lustre/fs1/portfolios/coreai/projects/trtllm"
  build_wheel: false
  dataset_file: "/lustre/fs1/portfolios/coreai/datasets/gsm8k.json"
  work_dir: "/lustre/fs1/portfolios/coreai/perf_test"

profiling:
  nsys_on: false

worker_config:
  eplb_num_slots: 0
  gen:
    tensor_parallel_size: 8
    moe_expert_parallel_size: 8
    enable_attention_dp: false
    max_batch_size: 1
    max_num_tokens: 128
    max_seq_len: 2251
    kv_cache_config:
      free_gpu_memory_fraction: 0.9
      dtype: fp8
    cache_transceiver_config:
      max_tokens_in_buffer: 4608
      backend: NIXL
  ctx:
    max_batch_size: 1
    max_num_tokens: 4608
    max_seq_len: 1227
    tensor_parallel_size: 4
    moe_expert_parallel_size: 4
    enable_attention_dp: true
    kv_cache_config:
      free_gpu_memory_fraction: 0.85
      dtype: fp8
    cache_transceiver_config:
      max_tokens_in_buffer: 4608
      backend: NIXL
```

#### Á§∫‰æã 2ÔºöËá™ÂÆö‰πâ accuracy metricsÔºàMMLU Êï∞ÊçÆÈõÜÔºâ

`test_configs/disagg/accuracy/deepseek-r1-fp8_1k1k_mmlu.yaml`

```yaml
# Â§ßÈÉ®ÂàÜÈÖçÁΩÆ‰∏éÁ§∫‰æã1Áõ∏Âêå...

benchmark:
  mode: "accuracy"
  expected_accuracy: 90.0
  
  # Ëá™ÂÆö‰πâ metricsÔºàMMLU Êúâ‰∏çÂêåÁöÑËæìÂá∫Ê†ºÂºèÔºâ
  metrics:
    log_file: "mmlu_results.json"
    extractor_pattern: "MMLU Score:\s+([0-9.]+)"
    metric_names: ["MMLU_SCORE"]
```

---

## Ê†∏ÂøÉÂÆûÁé∞‰ª£Á†Å

### Êñá‰ª∂ 1: `config_loader.py` - ÈÖçÁΩÆÂä†ËΩΩÂô®ÔºàÂê´ÈªòËÆ§ metricsÔºâ

```python
"""
YAML Configuration Loader with Default Metrics Support
"""

import yaml
from pathlib import Path
from typing import List, Optional, Dict
from dataclasses import dataclass


@dataclass
class MetricsConfig:
    """Metrics configuration"""
    log_file: str                          # Êó•ÂøóÊñá‰ª∂Âêç
    extractor_pattern: str                 # Ê≠£ÂàôË°®ËææÂºè
    metric_names: List[str]                # ÊåáÊ†áÂêçÁß∞ÂàóË°®
    
    def merge(self, override: Optional[Dict]) -> 'MetricsConfig':
        """
        Merge with override dict
        
        Args:
            override: Dict with optional keys: log_file, extractor_pattern, metric_names
        
        Returns:
            New MetricsConfig with overridden values
        """
        if not override:
            return self
        
        return MetricsConfig(
            log_file=override.get('log_file', self.log_file),
            extractor_pattern=override.get('extractor_pattern', self.extractor_pattern),
            metric_names=override.get('metric_names', self.metric_names)
        )


# ============================================================================
# ÈªòËÆ§ Metrics ÈÖçÁΩÆ
# ============================================================================

DEFAULT_METRICS_CONFIG = {
    # ÊÄßËÉΩÊµãËØïÈªòËÆ§ÈÖçÁΩÆ
    "perf": MetricsConfig(
        log_file="benchmark_result.log",
        extractor_pattern=r"""
            ^.*?Median\ TTFT\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Median\ E2EL\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Benchmark\ with\ concurrency\ (\d+)\ done
        """,
        metric_names=["DISAGG_SERVER_TTFT", "DISAGG_SERVER_E2EL"]
    ),
    
    # Á≤æÂ∫¶ÊµãËØïÈªòËÆ§ÈÖçÁΩÆ
    "accuracy": MetricsConfig(
        log_file="accuracy_result.json",
        extractor_pattern=r"Accuracy:\s+([0-9.]+)%",
        metric_names=["ACCURACY"]
    )
}


@dataclass
class TestConfig:
    """Test configuration data class"""
    config_path: str        # YAML file path
    test_id: str            # Auto-generated test ID
    test_type: str          # disagg, widep, etc.
    model_name: str         # Model name (‰ªéÊñá‰ª∂ÂêçËß£Êûê)
    test_category: str      # perf or accuracy
    benchmark_type: str     # 1k1k, 8k1k, etc. (‰ªéÊñá‰ª∂ÂêçËß£Êûê)
    config_data: dict       # Full YAML content
    metrics_config: MetricsConfig  # Metrics ÈÖçÁΩÆÔºàÈªòËÆ§ÊàñË¶ÜÁõñÂêéÁöÑÔºâ
    supported_gpus: List[str]  # ÊîØÊåÅÁöÑ GPU Á±ªÂûãÂàóË°®
    
    @property
    def display_name(self) -> str:
        """Display name for pytest"""
        return f"{self.test_type}/{self.test_category}/{Path(self.config_path).stem}"


class ConfigLoader:
    """Configuration loader with default metrics support"""
    
    def __init__(self, base_dir: str = "test_configs"):
        """
        Args:
            base_dir: Base directory for test configs
        """
        self.base_dir = Path(base_dir)
    
    def scan_configs(self, test_type: Optional[str] = None, 
                    test_category: Optional[str] = None, 
                    model_name: Optional[str] = None,
                    gpu_type: Optional[str] = None) -> List[TestConfig]:
        """
        Scan configuration files
        
        Directory structure: test_type/category/model_bench_config.yaml
        
        Args:
            test_type: Filter by test type (disagg, widep, etc.)
            test_category: Filter by category (perf, accuracy)
            model_name: Filter by model name
            gpu_type: Filter by GPU type (GB200, H100, etc.). If None, uses EnvManager.get_gpu_type()
        
        Returns:
            List of TestConfig objects (filtered by GPU support)
        """
        # Get current GPU type from environment if not specified
        if gpu_type is None:
            from disagg_config import EnvManager
            gpu_type = EnvManager.get_gpu_type()
        
        configs = []
        
        if not self.base_dir.exists():
            print(f"Warning: Config directory not found: {self.base_dir}")
            return configs
        
        # Traverse: test_type/category/config.yaml
        for test_type_dir in self.base_dir.iterdir():
            if not test_type_dir.is_dir() or test_type_dir.name == 'templates':
                continue
            
            current_test_type = test_type_dir.name
            
            # Filter by test_type
            if test_type and current_test_type != test_type:
                continue
            
            # Traverse category (perf/accuracy)
            for category_dir in test_type_dir.iterdir():
                if not category_dir.is_dir():
                    continue
                
                current_category = category_dir.name
                
                # Filter by test_category
                if test_category and current_category != test_category:
                    continue
                
                # Load all YAML files in this category
                for yaml_file in category_dir.glob("*.yaml"):
                    try:
                        config = self._load_config_file(
                            yaml_file,
                            current_test_type,
                            current_category
                        )
                        
                        # Filter by model_name
                        if model_name and config.model_name != model_name:
                            continue
                        
                        # Filter by GPU support
                        if gpu_type and gpu_type not in config.supported_gpus:
                            print(f"   ‚è≠Ô∏è  Skipping {yaml_file.name}: not supported on {gpu_type} (supported: {config.supported_gpus})")
                            continue
                        
                        configs.append(config)
                    except Exception as e:
                        print(f"Warning: Failed to load {yaml_file}: {e}")
        
        print(f"\n‚úÖ Loaded {len(configs)} configurations for GPU type: {gpu_type}")
        return configs
    
    def _load_config_file(self, yaml_path: Path, test_type: str,
                         test_category: str) -> TestConfig:
        """Load single YAML config file"""
        with open(yaml_path, 'r') as f:
            config_data = yaml.safe_load(f)
        
        # Extract metadata from YAML file
        metadata = config_data.get('metadata', {})
        model_name = metadata.get('model_name', 'unknown')
        precision = metadata.get('precision', 'unknown')
        supported_gpus = metadata.get('supported_gpus', ["GB200", "GB300", "H100", "B200", "B300"])
        
        # Generate benchmark_type from sequence configuration
        benchmark_type = self._generate_benchmark_type(config_data)
        
        # Get metrics config (default or override)
        metrics_config = self._get_metrics_config(test_category, config_data)
        
        # Generate test ID
        test_id = f"{test_type}_{test_category}_{model_name}_{benchmark_type}"
        
        return TestConfig(
            config_path=str(yaml_path),
            test_id=test_id,
            test_type=test_type,
            model_name=model_name,
            test_category=test_category,
            benchmark_type=benchmark_type,
            config_data=config_data,
            metrics_config=metrics_config,
            supported_gpus=supported_gpus
        )
    
    def _generate_benchmark_type(self, config_data: dict) -> str:
        """
        Generate benchmark type from sequence configuration
        
        Examples:
            input=1024, output=1024 -> "1k1k"
            input=8192, output=1024 -> "8k1k"
            input=16384, output=2048 -> "16k2k"
        
        Args:
            config_data: Full YAML config data
        
        Returns:
            Benchmark type string (e.g., "1k1k", "8k1k")
        """
        sequence = config_data.get('sequence', {})
        input_length = sequence.get('input_length', 0)
        output_length = sequence.get('output_length', 0)
        
        # Convert to k notation
        input_k = input_length // 1024
        output_k = output_length // 1024
        
        return f"{input_k}k{output_k}k"
    
    def _get_metrics_config(self, test_category: str, config_data: dict) -> MetricsConfig:
        """
        Get metrics config: use default or merge with override
        
        Args:
            test_category: 'perf' or 'accuracy'
            config_data: Full YAML config data
        
        Returns:
            MetricsConfig (default or merged with overrides)
        """
        # Ëé∑ÂèñÈªòËÆ§ÈÖçÁΩÆ
        default_config = DEFAULT_METRICS_CONFIG.get(test_category)
        if not default_config:
            # Â¶ÇÊûúÊ≤°ÊúâÈªòËÆ§ÈÖçÁΩÆÔºå‰ΩøÁî®Á©∫ÈÖçÁΩÆ
            print(f"   ‚ö†Ô∏è  No default metrics config for category: {test_category}")
            default_config = MetricsConfig(
                log_file="",
                extractor_pattern="",
                metric_names=[]
            )
        
        # Ê£ÄÊü• YAML ‰∏≠ÊòØÂê¶Êúâ metrics Ë¶ÜÁõñ
        benchmark_config = config_data.get('benchmark', {})
        metrics_override = benchmark_config.get('metrics')
        
        if metrics_override:
            # ÊúâË¶ÜÁõñÈÖçÁΩÆÔºåÂêàÂπ∂
            print(f"   ‚öôÔ∏è  Using custom metrics config (overriding defaults)")
            return default_config.merge(metrics_override)
        else:
            # Ê≤°ÊúâË¶ÜÁõñÈÖçÁΩÆÔºå‰ΩøÁî®ÈªòËÆ§
            print(f"   ‚öôÔ∏è  Using default metrics config for {test_category}")
            return default_config
    
    def load_config_by_path(self, config_path: str) -> TestConfig:
        """Load configuration by file path"""
        yaml_path = Path(config_path)
        
        # Parse path to extract metadata
        # Expected: test_configs/{test_type}/{category}/{config}.yaml
        parts = yaml_path.relative_to(self.base_dir).parts
        
        if len(parts) < 3:
            raise ValueError(f"Invalid config path structure: {config_path}")
        
        test_type = parts[0]
        test_category = parts[1]
        
        return self._load_config_file(yaml_path, test_type, test_category)
    
    def get_all_models(self) -> List[str]:
        """Get list of all unique model names"""
        configs = self.scan_configs()
        return sorted(set(config.model_name for config in configs))
    
    def get_all_test_types(self) -> List[str]:
        """Get list of all test types"""
        if not self.base_dir.exists():
            return []
        return sorted([d.name for d in self.base_dir.iterdir() 
                      if d.is_dir() and d.name != 'templates'])
```

### Êñá‰ª∂ 2: `test_disagg_yaml.py` - pytest ÊµãËØïÊñá‰ª∂

```python
"""
Disaggregated Benchmark Test - YAML Configuration Based
"""

import pytest
import os
import subprocess
import atexit
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

from config_loader import ConfigLoader, TestConfig
from utility import session_tracker, TestCaseTracker
from disagg_config import EnvManager
from disagg_executor import JobManager


# Load all test configurations
config_loader = ConfigLoader(base_dir="test_configs")
ALL_TEST_CONFIGS = config_loader.scan_configs()

# Convert to pytest parameters
ALL_TEST_CASES = [
    pytest.param(config, id=config.test_id)
    for config in ALL_TEST_CONFIGS
]


# Flag to track if session end has been called
_session_ended = False

def _ensure_session_end():
    """Ensure session end is called even on abnormal exit"""
    global _session_ended
    if not _session_ended:
        _session_ended = True
        print("\n‚ö†Ô∏è  Ensuring session cleanup...")
        session_tracker.end_and_collect()

# Register atexit handler
atexit.register(_ensure_session_end)

@pytest.fixture(scope="session", autouse=True)
def session_lifecycle():
    """Session lifecycle management"""
    session_tracker.start()
    try:
        yield
    finally:
        _ensure_session_end()


class TestDisaggBenchmark:
    """Disaggregated benchmark test class - YAML based"""
    
    @pytest.mark.parametrize("test_config", ALL_TEST_CASES)
    def test_benchmark(self, request, test_config: TestConfig):
        """Benchmark test for YAML configurations"""
        full_test_name = request.node.name
        
        # Create test case tracker
        test_tracker = TestCaseTracker()
        test_case_name = f"{test_config.model_name}-{test_config.benchmark_type}"
        
        # Start tracking test case
        test_tracker.start_test_case(test_case_name)
        
        try:
            print(f"\n{'='*60}")
            print(f"Test: {test_config.display_name}")
            print(f"Config file: {test_config.config_path}")
            print(f"Test type: {test_config.test_type}")
            print(f"Category: {test_config.test_category}")
            print(f"Model: {test_config.model_name}")
            print(f"Benchmark: {test_config.benchmark_type}")
            print(f"Metrics log: {test_config.metrics_config.log_file}")
            print(f"{'='*60}")
            
            # Submit job using submit.py
            success, job_id = self._submit_yaml_job(test_config)
            
            # Validate submission result
            assert success, f"Job submission failed: {test_config.test_id}"
            assert job_id, "Unable to get job ID"
            
            # Wait for completion
            completed = JobManager.wait_for_completion(job_id, 7200)
            if not completed:
                JobManager.cancel_job(job_id)
                assert False, f"Job execution timeout: {job_id}"
            
            # End tracking test case
            test_tracker.end_test_case()
            
            # Get timestamps information
            timestamps = test_tracker.get_timestamps()
            
            # Check results using JobManager.check_job_result
            result = self._check_job_result(
                job_id, test_config, timestamps, full_test_name
            )
            assert result["success"], f"Job execution failed: {job_id}"
            
        except Exception as e:
            test_tracker.end_test_case()
            raise e
    
    def _submit_yaml_job(self, test_config: TestConfig) -> tuple[bool, str]:
        """Submit job using submit.py with YAML config"""
        print(f"üöÄ Submitting job using submit.py...")
        
        try:
            # Call submit.py with the config file
            submit_script = os.path.join(
                EnvManager.get_work_dir(),
                "disagg/slurm/benchmark/submit.py"
            )
            
            cmd = ["python3", submit_script, "-c", test_config.config_path]
            
            print(f"   Command: {' '.join(cmd)}")
            
            # Execute submission
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode != 0:
                print(f"   ‚ùå Submission failed: {result.stderr}")
                return False, ""
            
            # Parse job ID from output
            output = result.stdout
            print(f"   Output: {output}")
            
            if "Submitted batch job" in output:
                import re
                match = re.search(r"Submitted batch job (\d+)", output)
                if match:
                    job_id = match.group(1)
                    print(f"   ‚úÖ Job submitted successfully: {job_id}")
                    return True, job_id
            
            print(f"   ‚ùå Unable to extract job ID from output")
            return False, ""
            
        except Exception as e:
            print(f"   ‚ùå Job submission exception: {e}")
            return False, str(e)
    
    def _check_job_result(self, job_id: str, test_config: TestConfig,
                         timestamps: Dict[str, str], 
                         test_name: str) -> Dict[str, Any]:
        """
        Check job result using JobManager.check_job_result
        
        This method calls JobManager.check_job_result which:
        1. Parses log files using metrics_config
        2. Generates performance report
        3. Saves results to CSV
        """
        # Extract parameters from YAML config
        config_data = test_config.config_data
        
        isl = config_data['sequence']['input_length']
        osl = config_data['sequence']['output_length']
        ctx_num = config_data['hardware']['num_ctx_servers']
        gen_num = config_data['hardware']['num_gen_servers']
        gen_tp_size = config_data['worker_config']['gen']['tensor_parallel_size']
        gen_batch_size = config_data['worker_config']['gen']['max_batch_size']
        gen_enable_dp = config_data['worker_config']['gen']['enable_attention_dp']
        eplb_slots = config_data['worker_config'].get('eplb_num_slots', 0)
        
        # Get MTP size if exists
        gen_config = config_data['worker_config']['gen']
        mtp_size = 0
        if 'speculative_config' in gen_config:
            mtp_size = gen_config['speculative_config'].get('num_nextn_predict_layers', 0)
        
        # Generate log directory names (matching submit.py logic)
        dep_flag = "dep" if gen_enable_dp else "tep"
        log_base = f"{isl}-{osl}"
        context_dir = (
            f"ctx{ctx_num}_gen{gen_num}_{dep_flag}{gen_tp_size}_"
            f"batch{gen_batch_size}_eplb{eplb_slots}_mtp{mtp_size}"
        )
        
        log_dir_name = log_base
        
        print(f"   üìÅ Log directory: {log_dir_name}")
        print(f"   üìÅ Context directory: {context_dir}")
        
        # Call JobManager.check_job_result with metrics_config
        result = JobManager.check_job_result(
            job_id=job_id,
            benchmark_type=test_config.benchmark_type,
            config=config_data,              # Pass dict directly
            metrics_config=test_config.metrics_config,  # Pass metrics config
            model_name=test_config.model_name,
            log_dir_name=log_dir_name,
            context_dir=context_dir,
            timestamps=timestamps,
            test_name=test_name
        )
        
        return result


if __name__ == "__main__":
    """Run benchmark tests"""
    pytest.main([__file__, "-v"])
```

### Êñá‰ª∂ 3: ‰øÆÊîπ `disagg_executor.py` ÁöÑ `check_job_result` ÊñπÊ≥ï

ÈúÄË¶Å‰øÆÊîπÁ≠æÂêçÔºåÊé•Âèó `metrics_config` ÂèÇÊï∞Ôºö

```python
# Âú® disagg_executor.py ‰∏≠‰øÆÊîπ

from config_loader import MetricsConfig  # Êñ∞Â¢û import

@staticmethod
def check_job_result(job_id: str, benchmark_type: str, config: dict,
                    metrics_config: MetricsConfig,  # Êñ∞Â¢ûÂèÇÊï∞
                    model_name: str, log_dir_name: str, context_dir: str, 
                    timestamps: Optional[Dict[str, str]] = None, 
                    test_name: Optional[str] = None) -> Dict[str, Any]:
    """
    Check job result with metrics config
    
    Args:
        job_id: SLURM job ID
        benchmark_type: Benchmark type (1k1k, 8k1k, etc.)
        config: Configuration dict (YAML data)
        metrics_config: Metrics configuration (default or custom)
        model_name: Model name
        log_dir_name: Log directory name
        context_dir: Context directory name
        timestamps: Optional timestamps dict
        test_name: Optional test name
    """
    result = {"job_id": job_id, "status": "UNKNOWN", "success": False}

    result_dir = os.path.join(EnvManager.get_work_dir(), log_dir_name, context_dir)
    print(f"   üìÅ Checking result directory: {result_dir}")
    
    # Print the slurm log to console
    slurm_log_writer = LogWritter(EnvManager.get_work_dir())
    slurm_log_writer.print_to_console(f"slurm-{job_id}.out")
    
    # Print the metrics log file specified in metrics_config
    log_writer = LogWritter(result_dir)
    if os.path.exists(os.path.join(result_dir, metrics_config.log_file)):
        log_writer.print_to_console(metrics_config.log_file)
    else:
        print(f"   ‚ö†Ô∏è  Metrics log file not found: {metrics_config.log_file}")
    
    # Parse using metrics config
    log_parser = LogParser(benchmark_type, config, metrics_config, 
                          log_dir_name, context_dir)
    parse_result = log_parser.parse(model_name, timestamps=timestamps, test_name=test_name)
    
    if parse_result["status"] == False:
        return result

    output_path = EnvManager.get_output_path()
    os.makedirs(output_path, exist_ok=True)

    output_csv = os.path.join(output_path, "perf_script_test_results.csv")
    result_saver = ResultSaver(output_csv)
    result_df = parse_result["df"]
    result_saver.append_a_df(result_df)
    result["success"] = True
    result["status"] = "SUCCESS"
    return result
```

### Êñá‰ª∂ 4: ‰øÆÊîπ `disagg_report.py` ÁöÑ `LogParser`

```python
# Âú® disagg_report.py ‰∏≠‰øÆÊîπ

from config_loader import MetricsConfig  # Êñ∞Â¢û import

class LogParser:
    """Log parser with metrics config support"""
    
    def __init__(self, benchmark_type: str, config: dict,
                 metrics_config: MetricsConfig,  # Êñ∞Â¢ûÂèÇÊï∞
                 log_dir_name: str, context_dir: str):
        self.benchmark_type = benchmark_type
        self.config = config
        self.metrics_config = metrics_config  # ‰øùÂ≠ò metrics ÈÖçÁΩÆ
        self.log_dir_name = log_dir_name
        self.context_dir = context_dir
    
    def parse(self, model_name: str, timestamps: Optional[Dict] = None, 
             test_name: Optional[str] = None) -> Dict[str, Any]:
        """Parse logs using configured metrics"""
        
        # Build log file path
        log_file_path = os.path.join(
            EnvManager.get_work_dir(),
            self.log_dir_name,
            self.context_dir,
            self.metrics_config.log_file
        )
        
        if not os.path.exists(log_file_path):
            print(f"   ‚ùå Log file not found: {log_file_path}")
            return {"status": False, "df": None}
        
        # Read log file
        with open(log_file_path, 'r') as f:
            log_content = f.read()
        
        # Extract metrics using configured pattern
        import re
        results = {}
        
        matches = re.finditer(
            self.metrics_config.extractor_pattern, 
            log_content, 
            re.MULTILINE | re.VERBOSE
        )
        
        for match in matches:
            groups = match.groups()
            for i, metric_name in enumerate(self.metrics_config.metric_names):
                if i < len(groups):
                    results[metric_name] = groups[i]
        
        # Build DataFrame
        df = self._build_dataframe(results, model_name, timestamps, test_name)
        
        return {"status": True, "df": df}
    
    def _build_dataframe(self, results: Dict, model_name: str, 
                        timestamps: Optional[Dict], test_name: Optional[str]):
        """Build DataFrame from parsed results"""
        # ... existing DataFrame building logic ...
        pass
```

### Êñá‰ª∂ 5: `list_configs.py` - ÈÖçÁΩÆÊü•ÁúãÂ∑•ÂÖ∑

```python
"""
List and inspect test configurations
"""

import argparse
from config_loader import ConfigLoader


def main():
    parser = argparse.ArgumentParser(description="List test configurations")
    parser.add_argument("--base-dir", default="test_configs", help="Base config directory")
    parser.add_argument("--test-type", help="Filter by test type (disagg, widep, etc.)")
    parser.add_argument("--category", help="Filter by category (perf, accuracy)")
    parser.add_argument("--model", help="Filter by model name")
    parser.add_argument("--gpu-type", help="Filter by GPU type (GB200, H100, etc.). Default: from GPU_TYPE env var")
    parser.add_argument("--verbose", "-v", action="store_true", help="Show detailed info")
    parser.add_argument("--show-metrics", action="store_true", help="Show metrics config")
    parser.add_argument("--show-all-gpus", action="store_true", help="Show all configs regardless of GPU support")
    
    args = parser.parse_args()
    
    loader = ConfigLoader(base_dir=args.base_dir)
    
    # If --show-all-gpus is specified, pass empty string to disable GPU filtering
    gpu_filter = "" if args.show_all_gpus else args.gpu_type
    
    configs = loader.scan_configs(
        test_type=args.test_type,
        test_category=args.category,
        model_name=args.model,
        gpu_type=gpu_filter
    )
    
    print(f"\nFound {len(configs)} test configurations\n")
    print("=" * 80)
    
    # Group by test_type and category
    grouped = {}
    for config in configs:
        key = (config.test_type, config.test_category)
        if key not in grouped:
            grouped[key] = []
        grouped[key].append(config)
    
    for (test_type, category), group_configs in sorted(grouped.items()):
        print(f"\n{test_type} / {category}")
        print("-" * 40)
        print(f"  Total: {len(group_configs)} configurations")
        
        # Group by model
        by_model = {}
        for config in group_configs:
            if config.model_name not in by_model:
                by_model[config.model_name] = []
            by_model[config.model_name].append(config)
        
        for model, model_configs in sorted(by_model.items()):
            print(f"\n  {model}: {len(model_configs)} configs")
            for config in model_configs:
                filename = config.config_path.split('/')[-1]
                print(f"    - {filename}")
                
                if args.verbose:
                    gen_config = config.config_data['worker_config']['gen']
                    print(f"      TP: {gen_config['tensor_parallel_size']}, "
                          f"Batch: {gen_config['max_batch_size']}, "
                          f"DP: {gen_config['enable_attention_dp']}")
                
                if args.show_metrics:
                    metrics = config.metrics_config
                    print(f"      Metrics log: {metrics.log_file}")
                    print(f"      Metric names: {', '.join(metrics.metric_names)}")
                
                if args.verbose or args.show_all_gpus:
                    print(f"      Supported GPUs: {', '.join(config.supported_gpus)}")
    
    print("\n" + "=" * 80)
    print(f"\nTotal: {len(configs)} configurations")
    
    # Show GPU type information
    if not args.show_all_gpus:
        from disagg_config import EnvManager
        current_gpu = args.gpu_type or EnvManager.get_gpu_type()
        print(f"Filtered for GPU type: {current_gpu}")
    
    # Show summary
    print("\nSummary:")
    print(f"  Models: {len(loader.get_all_models())}")
    print(f"  Test types: {', '.join(loader.get_all_test_types())}")


if __name__ == "__main__":
    main()
```

---

## ÈÖçÁΩÆÊñá‰ª∂ÂëΩÂêçËßÑËåÉ

### Êñá‰ª∂ÂêçÊ†ºÂºè

**Êé®ËçêÊ†ºÂºèÔºö`{model}_{benchmark_type}_{config_details}.yaml`**

- ‰ΩøÁî®‰∏ãÂàíÁ∫ø `_` ÂàÜÈöîÂêÑÈÉ®ÂàÜÔºå‰æø‰∫é‰∫∫Á±ªÈòÖËØª
- Á¨¨1ÈÉ®ÂàÜÔºöÊ®°ÂûãÂêçÔºàÂÜÖÈÉ®ÂèØÁî®ËøûÂ≠óÁ¨¶ `-`Ôºâ
- Á¨¨2ÈÉ®ÂàÜÔºöbenchmarkÁ±ªÂûãÔºàÂ¶Ç 1k1k, 8k1kÔºâ
- ‰πãÂêéÔºöÈÖçÁΩÆÁªÜËäÇÔºàÂ¶Ç tep8_bs32_mtp3_nixlÔºâ

**‚ö†Ô∏è Ê≥®ÊÑè**ÔºöÊñá‰ª∂Âêç‰ªÖÁî®‰∫é‰∫∫Á±ªÂèØËØªÊÄßÔºåÂÆûÈôÖÁöÑ `model_name`„ÄÅ`benchmark_type`„ÄÅ`precision`„ÄÅ`supported_gpus` Á≠â‰ø°ÊÅØÂùá‰ªé YAML Êñá‰ª∂ÂÜÖÁöÑ `metadata` Âíå `sequence` Â≠óÊÆµËØªÂèñ„ÄÇ

### benchmark_type Ëá™Âä®ÁîüÊàê

`benchmark_type` ‰ºöÊ†πÊçÆ YAML Êñá‰ª∂‰∏≠ÁöÑ `sequence` ÈÖçÁΩÆËá™Âä®ÁîüÊàêÔºö
- `input_length: 1024, output_length: 1024` ‚Üí `1k1k`
- `input_length: 8192, output_length: 1024` ‚Üí `8k1k`
- `input_length: 16384, output_length: 2048` ‚Üí `16k2k`

### ÊÄßËÉΩÊµãËØïÂëΩÂêçÁ§∫‰æã

- `deepseek-r1-fp4_1k1k_tep8_bs32_mtp3_nixl.yaml`
  - Êñá‰ª∂ÂêçËæÖÂä©ËØÜÂà´Ôºödeepseek-r1-fp4, 1k1kÈÖçÁΩÆ, TEP8Êû∂ÊûÑ
  - ÂÆûÈôÖÊï∞ÊçÆ‰ªé YAML ÁöÑ `metadata` Âíå `sequence` ËØªÂèñ

- `llama-70b_1k1k_dep16_bs128_nixl.yaml`
  - Êñá‰ª∂ÂêçËæÖÂä©ËØÜÂà´Ôºöllama-70b, 1k1kÈÖçÁΩÆ, DEP16Êû∂ÊûÑ

### Á≤æÂ∫¶ÊµãËØïÂëΩÂêçÁ§∫‰æã

- `deepseek-r1-fp4_1k1k_gsm8k.yaml`
  - Êñá‰ª∂ÂêçËæÖÂä©ËØÜÂà´Ôºödeepseek-r1-fp4, 1k1kÈÖçÁΩÆ, GSM8KÊï∞ÊçÆÈõÜ

---

## ‰ΩøÁî®ÊñπÂºè

### 1. ÂàõÂª∫ÊµãËØïÈÖçÁΩÆ

```bash
# ÂàõÂª∫ÁõÆÂΩïÁªìÊûÑ
mkdir -p test_configs/disagg/perf
mkdir -p test_configs/disagg/accuracy

# ÂàõÂª∫ÊÄßËÉΩÊµãËØïÈÖçÁΩÆÔºà‰ΩøÁî®ÈªòËÆ§ metricsÔºâ
vim test_configs/disagg/perf/deepseek-r1-fp8_1k1k_tep8_bs32.yaml
# ‰∏çÈúÄË¶ÅÈÖçÁΩÆ metricsÔºåËá™Âä®‰ΩøÁî®ÈªòËÆ§

# ÂàõÂª∫Á≤æÂ∫¶ÊµãËØïÈÖçÁΩÆÔºà‰ΩøÁî®ÈªòËÆ§ metricsÔºâ
vim test_configs/disagg/accuracy/deepseek-r1-fp8_1k1k_gsm8k.yaml
# ‰∏çÈúÄË¶ÅÈÖçÁΩÆ metricsÔºåËá™Âä®‰ΩøÁî®ÈªòËÆ§
```

### 2. Êü•ÁúãÊâÄÊúâÈÖçÁΩÆ

```bash
# ÂàóÂá∫ÊâÄÊúâÈÖçÁΩÆÔºàËá™Âä®ËøáÊª§ÂΩìÂâç GPU Á±ªÂûãÔºâ
python list_configs.py

# Êü•ÁúãÊâÄÊúâÈÖçÁΩÆÔºåÂåÖÊã¨‰∏çÊîØÊåÅÂΩìÂâç GPU ÁöÑ
python list_configs.py --show-all-gpus -v

# Êü•ÁúãÁâπÂÆö GPU Á±ªÂûãÁöÑÈÖçÁΩÆ
python list_configs.py --gpu-type GB200

# Êü•ÁúãÈÖçÁΩÆÂπ∂ÊòæÁ§∫ metrics ‰ø°ÊÅØ
python list_configs.py --show-metrics

# Êü•ÁúãÁâπÂÆöÁ±ªÂà´
python list_configs.py --category perf -v

# Êü•ÁúãÁâπÂÆöÊ®°Âûã
python list_configs.py --model deepseek-r1-fp4 --show-metrics

# Êü•ÁúãÁâπÂÆöÊ®°ÂûãÂú® H100 ‰∏äÁöÑÈÖçÁΩÆ
python list_configs.py --model deepseek-v3-lite-fp8 --gpu-type H100 -v
```

### 3. ËøêË°åÊµãËØï

```bash
# ËøêË°åÊâÄÊúâÊµãËØï
pytest test_disagg_yaml.py -v

# Âè™ËøêË°åÊÄßËÉΩÊµãËØï
pytest test_disagg_yaml.py -k "perf" -v

# Âè™ËøêË°åÁ≤æÂ∫¶ÊµãËØï
pytest test_disagg_yaml.py -k "accuracy" -v

# ËøêË°åÁâπÂÆöÊ®°Âûã
pytest test_disagg_yaml.py -k "deepseek-r1-fp8" -v

# Êü•ÁúãËØ¶ÁªÜËæìÂá∫
pytest test_disagg_yaml.py -s -vv
```

---

## ÂÖ≥ÈîÆÊîπËøõËØ¥Êòé

### 1. ÈªòËÆ§ Metrics ÈÖçÁΩÆ

**ÊîπËøõÂâç**ÔºöÊØè‰∏™ YAML Êñá‰ª∂ÈÉΩË¶ÅÈÖçÁΩÆ metricsÔºåÂ§ßÈáèÈáçÂ§ç

**ÊîπËøõÂêé**ÔºöÂÆö‰πâÈªòËÆ§ÈÖçÁΩÆÔºå90% ÁöÑÊñá‰ª∂‰∏çÈúÄË¶ÅÈÖçÁΩÆ

```python
DEFAULT_METRICS_CONFIG = {
    "perf": MetricsConfig(
        log_file="benchmark_result.log",
        extractor_pattern=r"...",  # È¢ÑÂÆö‰πâÁöÑ TTFT/E2EL Ê®°Âºè
        metric_names=["DISAGG_SERVER_TTFT", "DISAGG_SERVER_E2EL"]
    ),
    "accuracy": MetricsConfig(...)
}
```

### 2. Êô∫ËÉΩÂêàÂπ∂Êú∫Âà∂

```python
def _get_metrics_config(self, test_category: str, config_data: dict):
    default_config = DEFAULT_METRICS_CONFIG.get(test_category)
    metrics_override = config_data.get('benchmark', {}).get('metrics')
    
    if metrics_override:
        # ÈÉ®ÂàÜË¶ÜÁõñÔºöÂè™Ë¶ÜÁõñÊåáÂÆöÁöÑÂ≠óÊÆµ
        return default_config.merge(metrics_override)
    else:
        # ‰ΩøÁî®ÈªòËÆ§
        return default_config
```

### 3. ÁÅµÊ¥ªÁöÑË¶ÜÁõñÊñπÂºè

```yaml
# ÂÆåÂÖ®‰ΩøÁî®ÈªòËÆ§
benchmark:
  mode: "e2e"
  # ‰∏çÈÖçÁΩÆ metrics

# ÈÉ®ÂàÜË¶ÜÁõñ
benchmark:
  metrics:
    log_file: "custom.log"  # Âè™ÊîπËøô‰∏™

# ÂÆåÂÖ®Ëá™ÂÆö‰πâ
benchmark:
  metrics:
    log_file: "custom.log"
    extractor_pattern: "..."
    metric_names: [...]
```

---

## ÈúÄË¶Å‰øÆÊîπÁöÑÁé∞Êúâ‰ª£Á†ÅÊÄªÁªì

### 1. `disagg_executor.py`

```python
# ‰øÆÊîπÊñπÊ≥ïÁ≠æÂêç
def check_job_result(..., metrics_config: MetricsConfig, ...):
    # ‰ΩøÁî® metrics_config.log_file
    # ‰º†ÈÄí metrics_config Áªô LogParser
```

### 2. `disagg_report.py`

```python
# ‰øÆÊîπ LogParser ÊûÑÈÄ†ÂáΩÊï∞
class LogParser:
    def __init__(..., metrics_config: MetricsConfig, ...):
        self.metrics_config = metrics_config
    
    def parse(...):
        # ‰ΩøÁî® self.metrics_config.log_file
        # ‰ΩøÁî® self.metrics_config.extractor_pattern
        # ‰ΩøÁî® self.metrics_config.metric_names
```

---

## ÊÄªÁªì

### Ê†∏ÂøÉÊîπËøõ

1. ‚úÖ **ÈªòËÆ§ÈÖçÁΩÆ + ÂèØÈÄâË¶ÜÁõñ**ÔºöÂáèÂ∞ë 90% ÁöÑÈáçÂ§çÈÖçÁΩÆ
2. ‚úÖ **ÁÆÄÂåñÈÖçÁΩÆÊñá‰ª∂**ÔºöÂ§ßÂ§öÊï∞ YAML ‰∏çÈúÄË¶Å metrics ËäÇÁÇπ
3. ‚úÖ **ÁÅµÊ¥ªË¶ÜÁõñ**ÔºöÊîØÊåÅÈÉ®ÂàÜË¶ÜÁõñÂíåÂÆåÂÖ®Ëá™ÂÆö‰πâ
4. ‚úÖ **‰ª£Á†ÅÈõÜ‰∏≠ÁÆ°ÁêÜ**ÔºöÂú® ConfigLoader ‰∏≠Áªü‰∏ÄÁÆ°ÁêÜÈªòËÆ§ÈÖçÁΩÆ
5. ‚úÖ **Êòì‰∫éÊâ©Â±ï**ÔºöÊ∑ªÂä†Êñ∞ÁöÑÊµãËØïÁ±ªÂà´Âè™ÈúÄÂú® DEFAULT_METRICS_CONFIG ‰∏≠ÂÆö‰πâ
6. ‚úÖ **Metadata Â≠óÊÆµ**ÔºöÈõÜ‰∏≠ÁÆ°ÁêÜ `model_name`„ÄÅ`precision`„ÄÅ`supported_gpus` Á≠âÂÖÉÊï∞ÊçÆ
7. ‚úÖ **Âä®ÊÄÅ benchmark_type**Ôºö‰ªé `sequence` ÈÖçÁΩÆËá™Âä®ÁîüÊàêÔºåÈÅøÂÖçÊñá‰ª∂Âêç‰∏éÂÜÖÂÆπ‰∏ç‰∏ÄËá¥
8. ‚úÖ **GPU Á±ªÂûãËøáÊª§**ÔºöËá™Âä®Ê†πÊçÆÂΩìÂâç GPU Á±ªÂûãËøáÊª§ÈÖçÁΩÆÔºåÊîØÊåÅÂ§ö GPU ÁéØÂ¢É

### Metrics ÈÖçÁΩÆÂÜ≥Á≠ñÊ†ë

```
ÊòØÂê¶ÈúÄË¶ÅËá™ÂÆö‰πâ metricsÔºü
‚îú‚îÄ Âê¶Ôºà90% ÊÉÖÂÜµÔºâ
‚îÇ  ‚îî‚îÄ ‰∏çÈÖçÁΩÆ metrics ËäÇÁÇπÔºå‰ΩøÁî®ÈªòËÆ§
‚îÇ
‚îú‚îÄ ÊòØÔºàÂ∞ëÊï∞ÊÉÖÂÜµÔºâ
‚îÇ  ‚îú‚îÄ Âè™ÈúÄ‰øÆÊîπÊó•ÂøóÊñá‰ª∂Ôºü
‚îÇ  ‚îÇ  ‚îî‚îÄ Âè™ÈÖçÁΩÆ log_file
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ ÈúÄË¶ÅÂÆåÂÖ®Ëá™ÂÆö‰πâÔºü
‚îÇ     ‚îî‚îÄ ÈÖçÁΩÆÂÆåÊï¥ÁöÑ metrics ËäÇÁÇπ
```

### ‰ºòÂäøÊÄªÁªì

- **ÁÆÄÊ¥Å**ÔºöÂ§ßÂ§öÊï∞ÈÖçÁΩÆÊñá‰ª∂Êõ¥ÁÆÄÂçï
- **ÁÅµÊ¥ª**ÔºöÊîØÊåÅÊåâÈúÄË¶ÜÁõñ
- **ÂèØÁª¥Êä§**ÔºöÈªòËÆ§ÈÖçÁΩÆÈõÜ‰∏≠ÁÆ°ÁêÜ
- **ÂèØÊâ©Â±ï**ÔºöÊòì‰∫éÊ∑ªÂä†Êñ∞ÁöÑ metrics Á±ªÂûã
- **ÂèØÈù†**ÔºöÈÖçÁΩÆÊñá‰ª∂ÊòØÂîØ‰∏ÄÁúüÂÆûÊù•Ê∫êÔºàSingle Source of TruthÔºâ
- **Êô∫ËÉΩ**ÔºöËá™Âä®Ê†πÊçÆ GPU Á±ªÂûãËøáÊª§ÈÖçÁΩÆ

### ËÆæËÆ°ÁêÜÂøµ

**ÈÖçÁΩÆÂç≥Êï∞ÊçÆÔºàConfiguration as DataÔºâ**

ÊâÄÊúâÂÖ≥ÈîÆ‰ø°ÊÅØÔºà`model_name`„ÄÅ`precision`„ÄÅ`benchmark_type`„ÄÅ`supported_gpus`ÔºâÈÉΩ‰ªé YAML Êñá‰ª∂ÂÜÖÂÆπËØªÂèñÔºåËÄå‰∏çÊòØ‰ªéÊñá‰ª∂ÂêçËß£Êûê„ÄÇËøôÁ°Æ‰øù‰∫ÜÔºö

1. **ÂîØ‰∏ÄÁúüÂÆûÊù•Ê∫ê**ÔºöYAML Êñá‰ª∂ÂÜÖÂÆπÊòØÊùÉÂ®ÅÊï∞ÊçÆÊ∫ê
2. **ÁÅµÊ¥ªÈáçÊûÑ**ÔºöÂèØ‰ª•‰øÆÊîπÈÖçÁΩÆÂÜÖÂÆπËÄåÊó†ÈúÄÈáçÂëΩÂêçÊñá‰ª∂
3. **Á®ãÂ∫èÂèãÂ•Ω**Ôºö‰æø‰∫éÁ®ãÂ∫èÂåñÁîüÊàêÂíå‰øÆÊîπÈÖçÁΩÆ
4. **‰∫∫Á±ªÂèØËØª**ÔºöÊñá‰ª∂Âêç‰ªçÁÑ∂‰øùÁïôÂèØËØªÊÄßÔºå‰æø‰∫éÊµèËßàÂíåËØÜÂà´

**ÂÖÉÊï∞ÊçÆÊâ©Â±ïÊÄßÔºàMetadata ExtensibilityÔºâ**

ÈÄöËøá `metadata` Â≠óÊÆµÔºåÂèØ‰ª•ËΩªÊùæÊ∑ªÂä†Êñ∞ÁöÑÂÖÉÊï∞ÊçÆÔºö

```yaml
metadata:
  model_name: "deepseek-r1-fp4"
  precision: "fp4"
  supported_gpus: ["GB200", "GB300"]
  # Êú™Êù•ÂèØÊâ©Â±ï
  author: "team-name"
  created_date: "2025-01-15"
  tags: ["production", "high-priority"]
```

Â∞±ÊòØËøô‰πàÁÆÄÂçïÔºÅüéâ
