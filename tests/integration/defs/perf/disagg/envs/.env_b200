export CONTAINER_IMAGE="urm.nvidia.com/sw-tensorrt-docker/tensorrt-llm:pytorch-25.06-py3-x86_64-ubuntu24.04-trt10.11.0.33-skip-tritondevel-202509091430-7383"
USER_NAME="${USER:-$(whoami)}"
export WORK_DIR="/home/trt_llm_qa/${USER_NAME}/tensorrt-llm-bench"
export SCRIPTS_DIR="/home/trt_llm_qa/${USER_NAME}/tensorrt-llm-bench/disaggregated"
export REPO_DIR="/home/trt_llm_qa/${USER_NAME}/trtllm_cluster_test/tensorrt_llm"
export GPU_TYPE="B200"
export SLURM_PARTITION="b200@cr+mp-1000W/umbriel-b200@ts4/8gpu-224cpu-2048gb"
export SLURM_ACCOUNT="tensorrt"
export MODEL_DIR="/home/scratch.trt_llm_data/llm-models"
export OUTPUT_PATH="/home/trt_llm_qa/${USER_NAME}/output"
# To avoid home dir pip cache issue when install TensorRT-LLM from wheel
export XDG_CACHE_HOME="/home/trt_llm_qa/${USER_NAME}"
export PIP_CACHE_DIR="${XDG_CACHE_HOME}/pip"
export INSTALL_MODE="wheel"
