export CONTAINER_IMAGE="urm.nvidia.com/sw-tensorrt-docker/tensorrt-llm:pytorch-25.06-py3-x86_64-ubuntu24.04-trt10.11.0.33-skip-tritondevel-202509091430-7383"
USER_NAME="${USER:-$(whoami)}"
export WORK_DIR="/lustre/fsw/coreai_comparch_trtllm/${USER_NAME}/tensorrt-llm-bench"
export SCRIPT_DIR="/lustre/fsw/coreai_comparch_trtllm/${USER_NAME}/tensorrt-llm-bench/disaggregated"
export REPO_DIR="/lustre/fsw/coreai_comparch_trtllm/${USER_NAME}/trtllm_cluster_test/tensorrt_llm/"
export GPU_TYPE="H100"
export SLURM_PARTITION="batch"
export SLURM_ACCOUNT="coreai_comparch_trtllm"
export MODEL_DIR="/lustre/fsw/coreai_comparch_trtllm/llm_data/llm-models"
export OUTPUT_PATH="/lustre/fsw/coreai_comparch_trtllm/${USER_NAME}/output"
# To avoid home dir pip cache issue when install TensorRT-LLM from wheel
export XDG_CACHE_HOME="/lustre/fsw/coreai_comparch_trtllm/${USER_NAME}"
export PIP_CACHE_DIR="${XDG_CACHE_HOME}/pip"
export INSTALL_MODE="none"