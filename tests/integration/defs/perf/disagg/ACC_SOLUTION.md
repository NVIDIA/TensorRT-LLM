# ç²¾åº¦æµ‹è¯•æ”¯æŒè®¾è®¡æ–¹æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰

## æ ¸å¿ƒè®¾è®¡åŸåˆ™

- ç²¾åº¦æµ‹è¯•ä¸å­˜CSVï¼Œåªåˆ¤æ–­pass/fail
- æ€§èƒ½æµ‹è¯•ç»§ç»­å­˜CSVï¼ˆç°æœ‰é€»è¾‘ä¸å˜ï¼‰
- ä½¿ç”¨test_categoryåŒºåˆ†ä¸¤ç§æµ‹è¯•ç±»å‹
- ç²¾åº¦é…ç½®æ”¾åœ¨YAMLçš„metadataéƒ¨åˆ†
- **æ‰€æœ‰è¾“å‡ºç»Ÿä¸€åœ¨ benchmark.log ä¸­**
- **é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å’Œå…³é”®å­—åŒ¹é…æå–ä¸åŒæ•°æ®é›†çš„accuracyå€¼**
- æ”¯æŒå¤šæ•°æ®é›†ï¼ˆgsm8kã€mmluã€humanevalç­‰ï¼‰

## 1. æ•°æ®ç»“æ„æ‰©å±• (config_loader.py)

### 1.1 æ·»åŠ DatasetThresholdç±»

```python
@dataclass
class DatasetThreshold:
    """å•ä¸ªæ•°æ®é›†çš„ç²¾åº¦é˜ˆå€¼é…ç½®"""
    dataset_name: str              # æ•°æ®é›†åç§°ï¼šgsm8k, mmlu, humanevalç­‰
    expected_value: float          # æœŸæœ›å€¼
    threshold: float               # é˜ˆå€¼
    threshold_type: str            # "relative" æˆ– "absolute"
    
    def validate(self, actual_value: float) -> tuple[bool, str]:
        """éªŒè¯ç²¾åº¦æ˜¯å¦é€šè¿‡"""
        if self.threshold_type == "relative":
            error = abs(actual_value - self.expected_value) / self.expected_value
            passed = error < self.threshold
            msg = f"Relative error: {error:.6f} (threshold: {self.threshold})"
        else:  # absolute
            error = abs(actual_value - self.expected_value)
            passed = error < self.threshold
            msg = f"Absolute error: {error:.6f} (threshold: {self.threshold})"
        
        return passed, msg
```

### 1.2 æ·»åŠ AccuracyConfigç±»

å­˜å‚¨å¤šä¸ªæ•°æ®é›†çš„é˜ˆå€¼é…ç½®ï¼š

```python
@dataclass
class AccuracyConfig:
    """ç²¾åº¦æµ‹è¯•é…ç½®ï¼ˆæ”¯æŒå¤šæ•°æ®é›†ï¼‰"""
    datasets: List[DatasetThreshold]  # æ•°æ®é›†é˜ˆå€¼åˆ—è¡¨
    
    def get_dataset_config(self, dataset_name: str) -> Optional[DatasetThreshold]:
        """æ ¹æ®æ•°æ®é›†åç§°è·å–é…ç½®"""
        for ds in self.datasets:
            if ds.dataset_name == dataset_name:
                return ds
        return None
```

### 1.3 ä¿®æ”¹_load_config_fileæ–¹æ³•

ä»YAMLçš„metadataè¯»å–ç²¾åº¦é…ç½®ï¼š

```python
# åœ¨metadataä¸­è¯»å–accuracyé…ç½®
accuracy_config = None
if test_category == "accuracy":
    acc_meta = metadata.get('accuracy', {})
    if acc_meta:
        datasets = []
        # æ”¯æŒdatasetsåˆ—è¡¨é…ç½®
        for ds_config in acc_meta.get('datasets', []):
            datasets.append(DatasetThreshold(
                dataset_name=ds_config.get('name', 'gsm8k'),
                expected_value=ds_config.get('expected_value', 0.0),
                threshold=ds_config.get('threshold', 0.01),
                threshold_type=ds_config.get('threshold_type', 'relative')
            ))
        
        accuracy_config = AccuracyConfig(datasets=datasets)
```

### 1.4 æ›´æ–°DEFAULT_METRICS_CONFIG

ä½¿ç”¨å…ƒç»„keyæ–¹æ¡ˆï¼Œaccuracyä¹Ÿä½¿ç”¨benchmark.logï¼š

```python
_COMMON_ACCURACY_CONFIG = MetricsConfig(
    log_file="benchmark.log",  # ç»Ÿä¸€ä½¿ç”¨benchmark.log
    # æ­£åˆ™æå–æ ¼å¼: "gsm8k: acc=0.85" æˆ– "|gsm8k|acc|0.85|"
    extractor_pattern=r'(\w+)[\s|:]+acc[\s|=:]+([0-9.]+)',
    metric_names=["ACCURACY"]
)

DEFAULT_METRICS_CONFIG = {
    # Disagg æ€§èƒ½æµ‹è¯•
    ("disagg", "perf"): MetricsConfig(
        log_file="benchmark.log",
        extractor_pattern=r"""
            ^.*?Median\ TTFT\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Median\ E2EL\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Benchmark\ with\ concurrency\ (\d+)\ done
        """,
        metric_names=["DISAGG_SERVER_TTFT", "DISAGG_SERVER_E2EL"]
    ),
    
    # Widep æ€§èƒ½æµ‹è¯•
    ("widep", "perf"): MetricsConfig(
        log_file="benchmark.log",
        extractor_pattern=r"""
            ^.*?Median\ TTFT\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Median\ E2EL\ \(ms\):\s+([0-9.]+).*?$\n
            ^.*?(?:\n|.)*?$\n
            ^.*?Benchmark\ with\ concurrency\ (\d+)\ done
        """,
        metric_names=["WIDEP_SERVER_TTFT", "WIDEP_SERVER_E2EL"]
    ),
    
    # ç²¾åº¦æµ‹è¯•ï¼šå¤ç”¨é€šç”¨é…ç½®
    ("disagg", "accuracy"): _COMMON_ACCURACY_CONFIG,
    ("widep", "accuracy"): _COMMON_ACCURACY_CONFIG,
}
```

## 2. æ—¥å¿—è§£ææ‰©å±• (report.py)

### 2.1 æ·»åŠ AccuracyParserç±»

ä»benchmark.logè§£æå¤šä¸ªæ•°æ®é›†çš„accuracyï¼š

```python
class AccuracyParser:
    """ç²¾åº¦æµ‹è¯•è§£æå™¨ï¼ˆä»benchmark.logæå–ï¼‰"""
    
    def __init__(self, metrics_config: MetricsConfig, accuracy_config: AccuracyConfig, result_dir: str):
        self.metrics_config = metrics_config
        self.accuracy_config = accuracy_config
        self.result_dir = result_dir
    
    def parse_and_validate(self) -> Dict[str, Any]:
        """è§£æbenchmark.logå¹¶éªŒè¯æ‰€æœ‰æ•°æ®é›†çš„ç²¾åº¦"""
        log_file = os.path.join(self.result_dir, self.metrics_config.log_file)
        
        if not os.path.exists(log_file):
            return {"success": False, "error": f"Log file not found: {log_file}"}
        
        # è¯»å–æ—¥å¿—
        with open(log_file, 'r', encoding='utf-8', errors='replace') as f:
            log_content = f.read()
        
        # ä½¿ç”¨æ­£åˆ™æå–æ‰€æœ‰æ•°æ®é›†çš„accuracy
        # æ ¼å¼ç¤ºä¾‹ï¼š
        #   gsm8k: acc=0.85
        #   mmlu: acc=0.75
        #   æˆ–ï¼š|gsm8k|acc|0.85|
        pattern = re.compile(self.metrics_config.extractor_pattern, re.IGNORECASE)
        matches = pattern.findall(log_content)
        
        if not matches:
            return {"success": False, "error": "No accuracy values found in log"}
        
        # è§£æä¸ºå­—å…¸ï¼š{dataset_name: accuracy_value}
        parsed_results = {}
        for match in matches:
            dataset_name = match[0].lower()  # æ•°æ®é›†åç§°ï¼ˆå°å†™ï¼‰
            acc_value = float(match[1])      # accuracyå€¼
            parsed_results[dataset_name] = acc_value
        
        print(f"   ğŸ“Š Parsed accuracy results: {parsed_results}")
        
        # éªŒè¯æ¯ä¸ªé…ç½®çš„æ•°æ®é›†
        validation_results = []
        all_passed = True
        
        for dataset_config in self.accuracy_config.datasets:
            dataset_name = dataset_config.dataset_name.lower()
            
            if dataset_name not in parsed_results:
                validation_results.append({
                    "dataset": dataset_config.dataset_name,
                    "passed": False,
                    "error": f"Dataset {dataset_config.dataset_name} not found in log"
                })
                all_passed = False
                continue
            
            actual_value = parsed_results[dataset_name]
            passed, msg = dataset_config.validate(actual_value)
            
            validation_results.append({
                "dataset": dataset_config.dataset_name,
                "passed": passed,
                "actual": actual_value,
                "expected": dataset_config.expected_value,
                "threshold": dataset_config.threshold,
                "threshold_type": dataset_config.threshold_type,
                "message": msg
            })
            
            if not passed:
                all_passed = False
        
        return {
            "success": True,
            "all_passed": all_passed,
            "results": validation_results
        }
```

## 3. æ‰§è¡Œå™¨ä¿®æ”¹ (executor.py)

### 3.1 ä¿®æ”¹JobManager.check_resultæ–¹æ³•

æ·»åŠ test_categoryå’Œaccuracy_configå‚æ•°ï¼š

```python
@staticmethod
def check_result(job_id: str, test_config, timestamps, test_name) -> Dict[str, Any]:
    # ... ç°æœ‰ä»£ç  ...
    
    return JobManager._check_job_result(
        job_id=job_id,
        test_category=test_config.test_category,  # æ–°å¢
        benchmark_type=test_config.benchmark_type,
        config=config_data,
        metrics_config=test_config.metrics_config,
        accuracy_config=test_config.accuracy_config,  # æ–°å¢
        model_name=test_config.model_name,
        result_dir=result_dir,
        timestamps=timestamps,
        test_name=test_name
    )
```

### 3.2 ä¿®æ”¹_check_job_resultæ–¹æ³•ç­¾å

æ·»åŠ test_categoryå’Œaccuracy_configå‚æ•°ï¼š

```python
@staticmethod
def _check_job_result(job_id: str, test_category: str, benchmark_type: str, 
                     config: dict, metrics_config, accuracy_config, 
                     model_name: str, result_dir: str, 
                     timestamps: Optional[Dict[str, str]] = None, 
                     test_name: Optional[str] = None) -> Dict[str, Any]:
```

### 3.3 åœ¨_check_job_resultä¸­æ·»åŠ åˆ†æµé€»è¾‘

```python
# ... æ‰“å°æ—¥å¿—çš„å…±é€šé€»è¾‘ ...

# æ ¹æ®test_categoryåˆ†æµ
if test_category == "accuracy":
    # ç²¾åº¦æµ‹è¯•ï¼šä¸å­˜CSVï¼ŒåªéªŒè¯pass/fail
    if not accuracy_config:
        return {"success": False, "error": "Accuracy config not found"}
    
    # è§£æå¹¶éªŒè¯
    accuracy_parser = AccuracyParser(metrics_config, accuracy_config, result_dir)
    validation_result = accuracy_parser.parse_and_validate()
    
    if not validation_result["success"]:
        result["error"] = validation_result.get("error", "Validation failed")
        return result
    
    # æ‰“å°éªŒè¯ç»“æœ
    print(f"   ğŸ“Š Accuracy Validation Results:")
    all_passed = validation_result["all_passed"]
    
    for ds_result in validation_result["results"]:
        status_icon = "âœ…" if ds_result["passed"] else "âŒ"
        print(f"      {status_icon} {ds_result['dataset']}:")
        if "error" in ds_result:
            print(f"         Error: {ds_result['error']}")
        else:
            print(f"         Expected: {ds_result['expected']}")
            print(f"         Actual: {ds_result['actual']}")
            print(f"         Threshold: {ds_result['threshold']} ({ds_result['threshold_type']})")
            print(f"         {ds_result['message']}")
    
    if all_passed:
        print(f"   âœ… All accuracy tests PASSED")
        result["success"] = True
        result["status"] = "PASSED"
    else:
        print(f"   âŒ Some accuracy tests FAILED")
        result["success"] = False
        result["status"] = "FAILED"
    
    result.update(validation_result)
    return result

else:  # perf
    # æ€§èƒ½æµ‹è¯•ï¼šè§£æå¹¶å­˜CSVï¼ˆç°æœ‰é€»è¾‘ä¸å˜ï¼‰
    # ... ç°æœ‰çš„perfå¤„ç†é€»è¾‘ ...
```

## 4. TestConfigæ‰©å±• (config_loader.py)

åœ¨TestConfigæ·»åŠ accuracy_configå­—æ®µï¼š

```python
@dataclass
class TestConfig:
    config_path: str
    test_id: str
    test_type: str
    model_name: str
    test_category: str
    benchmark_type: str
    config_data: dict
    metrics_config: MetricsConfig
    accuracy_config: Optional[AccuracyConfig] = None  # æ–°å¢
    supported_gpus: List[str]
```

## 5. YAMLé…ç½®ç¤ºä¾‹

### 5.1 å•æ•°æ®é›†ç²¾åº¦æµ‹è¯•

```yaml
metadata:
  model_name: "deepseek-r1-fp4"
  precision: "fp4"
  supported_gpus: ["GB200", "GB300"]
  
  # ç²¾åº¦æµ‹è¯•é…ç½®
  accuracy:
    datasets:
      - name: "gsm8k"
        expected_value: 0.85
        threshold: 0.02
        threshold_type: "relative"
```

### 5.2 å¤šæ•°æ®é›†ç²¾åº¦æµ‹è¯•

```yaml
metadata:
  model_name: "deepseek-r1-fp4"
  precision: "fp4"
  supported_gpus: ["GB200", "GB300"]
  
  # ç²¾åº¦æµ‹è¯•é…ç½®ï¼ˆå¤šæ•°æ®é›†ï¼‰
  accuracy:
    datasets:
      - name: "gsm8k"
        expected_value: 0.85
        threshold: 0.02
        threshold_type: "relative"
      
      - name: "mmlu"
        expected_value: 0.75
        threshold: 0.03
        threshold_type: "relative"
      
      - name: "humaneval"
        expected_value: 0.70
        threshold: 0.05
        threshold_type: "absolute"
```

### 5.3 è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¯é€‰ï¼‰

å¦‚æœlm_evalè¾“å‡ºæ ¼å¼ç‰¹æ®Šï¼Œå¯ä»¥è¦†ç›–é»˜è®¤çš„æ­£åˆ™ï¼š

```yaml
benchmark:
  mode: "accuracy"
  # å¯é€‰ï¼šè¦†ç›–é»˜è®¤çš„æ­£åˆ™è¡¨è¾¾å¼
  metrics:
    extractor_pattern: r'\|(\w+)\|acc\|([0-9.]+)\|'  # åŒ¹é…è¡¨æ ¼æ ¼å¼
```

## ä¿®æ”¹æ–‡ä»¶æ¸…å•

1. `config_loader.py` - æ·»åŠ DatasetThresholdå’ŒAccuracyConfigç±»ï¼Œæ‰©å±•TestConfigï¼Œä¿®æ”¹_load_config_fileå’Œ_get_metrics_configæ–¹æ³•
2. `report.py` - æ·»åŠ AccuracyParserç±»ï¼ˆç®€åŒ–ç‰ˆï¼Œåªè§£æbenchmark.logï¼‰
3. `executor.py` - ä¿®æ”¹check_resultå’Œ_check_job_resultæ–¹æ³•ï¼Œæ·»åŠ åˆ†æµé€»è¾‘

## è®¾è®¡è¦ç‚¹

1. **ç»Ÿä¸€æ—¥å¿—æ–‡ä»¶**ï¼šæ‰€æœ‰è¾“å‡ºéƒ½åœ¨benchmark.logä¸­ï¼Œç®€åŒ–æ–‡ä»¶ç®¡ç†
2. **æ­£åˆ™è¡¨è¾¾å¼çµæ´»**ï¼šæ”¯æŒå¤šç§æ ¼å¼ï¼ˆ`gsm8k: acc=0.85` æˆ– `|gsm8k|acc|0.85|`ï¼‰
3. **å¤šæ•°æ®é›†æ”¯æŒ**ï¼šä¸€æ¬¡è§£ææå–æ‰€æœ‰æ•°æ®é›†çš„accuracyï¼ŒæŒ‰é…ç½®éªŒè¯
4. **å…³é”®å­—åŒ¹é…**ï¼šé€šè¿‡æ•°æ®é›†åç§°ï¼ˆgsm8kã€mmluç­‰ï¼‰åŒ¹é…å¯¹åº”çš„é˜ˆå€¼é…ç½®
5. **æ¸…æ™°çš„è¾“å‡º**ï¼šæ¯ä¸ªæ•°æ®é›†çš„éªŒè¯ç»“æœç‹¬ç«‹æ˜¾ç¤ºï¼Œä¾¿äºè°ƒè¯•

## å®æ–½å¾…åŠ

- [ ] åœ¨config_loader.pyæ·»åŠ DatasetThresholdæ•°æ®ç±»
- [ ] åœ¨config_loader.pyæ·»åŠ AccuracyConfigæ•°æ®ç±»ï¼ˆæ”¯æŒå¤šæ•°æ®é›†ï¼‰
- [ ] æ›´æ–°DEFAULT_METRICS_CONFIGä½¿ç”¨å…ƒç»„keyæ–¹æ¡ˆï¼Œaccuracyä½¿ç”¨benchmark.log
- [ ] æ‰©å±•TestConfigæ·»åŠ accuracy_configå­—æ®µ
- [ ] ä¿®æ”¹_load_config_fileå’Œ_get_metrics_configæ–¹æ³•ï¼Œæ”¯æŒä»YAMLè¯»å–å¤šæ•°æ®é›†accuracyé…ç½®
- [ ] åœ¨report.pyæ·»åŠ AccuracyParserç±»ï¼ˆä»benchmark.logè§£æå’ŒéªŒè¯ï¼‰
- [ ] ä¿®æ”¹executor.pyçš„check_resultæ–¹æ³•ä¼ é€’test_categoryå’Œaccuracy_config
- [ ] ä¿®æ”¹_check_job_resultæ–¹æ³•æ·»åŠ test_categoryåˆ†æµé€»è¾‘ï¼Œé›†æˆAccuracyParser

