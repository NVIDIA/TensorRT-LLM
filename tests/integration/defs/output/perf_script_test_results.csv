network_name,network_hash,sm_clk,mem_clk,gpu_idx,perf_case_name,test_name,original_test_name,raw_result,perf_metric,total_time__sec,start_timestamp,end_timestamp,state,command,threshold,absolute_threshold,metric_type
"llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8","test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]",1755,1593,0,"H100/perf/test_perf.py::test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]","test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]","H100/perf/test_perf.py::test_perf[llama_v3.1_8b-bench-bfloat16-input_output_len:128,128-quant:fp8]","\n[05/22/2025-07:20:22] [TRT-LLM] [I] Starting TensorRT-LLM init.\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-07:20:22] [TRT-LLM] [I] TensorRT-LLM inited.\n[TensorRT-LLM] TensorRT-LLM version: 0.21.0rc0\n[05/22/2025-07:20:25] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: verbose\n[05/22/2025-07:20:25] [TRT-LLM] [I] Found dataset.\n[05/22/2025-07:20:25] [TRT-LLM] [I] \n===========================================================\n= DATASET DETAILS\n===========================================================\nDataset Path:         None\nNumber of Sequences:  512\n\n-- Percentiles statistics ---------------------------------\n\n        Input              Output           Seq. Length\n-----------------------------------------------------------\nMIN:   128.0000           128.0000           256.0000\nMAX:   128.0000           128.0000           256.0000\nAVG:   128.0000           128.0000           256.0000\nP50:   128.0000           128.0000           256.0000\nP90:   128.0000           128.0000           256.0000\nP95:   128.0000           128.0000           256.0000\nP99:   128.0000           128.0000           256.0000\n===========================================================\n\n[05/22/2025-07:20:25] [TRT-LLM] [I] Max batch size and max num tokens are not provided, use tuning heuristics or pre-defined setting from trtllm-bench.\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated engine size: 7.48 GB\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated total available memory for KV cache: 72.17 GB\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated total KV cache memory: 68.56 GB\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated max number of requests in KV cache memory: 4387.86\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated max batch size (after fine-tune): 4096\n[05/22/2025-07:20:25] [TRT-LLM] [I] Estimated max num tokens (after fine-tune): 8192\n[05/22/2025-07:20:25] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-07:20:25] [TRT-LLM] [I] Set multiple_profiles to True.\n[05/22/2025-07:20:25] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n[05/22/2025-07:20:25] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n[05/22/2025-07:20:25] [TRT-LLM] [W] Overriding LlmArgsBase.max_input_len (annotation=int required=False default=1024 description='The maximum input length.') with build_config.max_input_len (1024).\n[05/22/2025-07:20:25] [TRT-LLM] [W] Overriding LlmArgsBase.max_seq_len (annotation=Union[int, NoneType] required=False default=None description='The maximum sequence length.') with build_config.max_seq_len (256).\n[05/22/2025-07:20:25] [TRT-LLM] [W] Overriding LlmArgsBase.max_beam_width (annotation=int required=False default=1 description='The maximum beam width.') with build_config.max_beam_width (1).\n[05/22/2025-07:20:25] [TRT-LLM] [W] Using default gpus_per_node: 1\n[05/22/2025-07:20:25] [TRT-LLM] [I] Specified dtype 'auto'; inferred dtype 'bfloat16'.\n[05/22/2025-07:20:25] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:20:25] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:20:25] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-07:20:25] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:20:25] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:20:25] [TRT-LLM] [I] Initializing model from /scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\n[05/22/2025-07:20:28] [TRT-LLM] [I] Initializing tokenizer from /scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\n[05/22/2025-07:20:28] [TRT-LLM] [I] Loading calibration dataset\n[05/22/2025-07:20:34] [TRT-LLM] [I] Starting quantization...\nRegistered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> for KV Cache quantization\nInserted 771 quantizers\n[05/22/2025-07:21:56] [TRT-LLM] [I] Quantization done. Total time used: 82.46 s.\ncurrent rank: 0, tp rank: 0, pp rank: 0\n[05/22/2025-07:21:58] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:21:58] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:22:16] [TRT-LLM] [I] Quantized model exported to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/tmp251svz2n-llm-workspace/quantized-checkpoint \nTotal time used 20.49 s.\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.producer = {'name': 'modelopt', 'version': '0.29.0'}\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.share_embedding_table = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.bias = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rotary_pct = 1.0\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rank = 0\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.decoder = llama\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rmsnorm = True\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.lm_head_bias = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.fc_after_embed = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_input_layernorm_in_first_layer = True\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_last_layernorm = True\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.layer_idx_offset = 0\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:22:17] [TRT-LLM] [W] Implicitly setting LLaMAConfig.model_type = llama\n[05/22/2025-07:22:17] [TRT-LLM] [I] Set paged_kv_cache to True.\n[05/22/2025-07:22:17] [TRT-LLM] [W] Overriding paged_state to False\n[05/22/2025-07:22:17] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-07:22:17] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-07:22:17] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-07:22:17] [TRT-LLM] [W] max_input_len is 1024 is larger than max_seq_len 256, clipping it to max_seq_len\n[05/22/2025-07:22:17] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n[05/22/2025-07:22:24] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 6364, GPU 636 (MiB)\n[05/22/2025-07:22:25] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1005, GPU +6, now: CPU 7168, GPU 642 (MiB)\n[05/22/2025-07:22:25] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-07:22:26] [TRT-LLM] [I] Total time of constructing network from module object 8.95593547821045 seconds\n[05/22/2025-07:22:26] [TRT-LLM] [I] Total optimization profiles added: 6\n[05/22/2025-07:22:26] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n[05/22/2025-07:22:26] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n[05/22/2025-07:22:26] [TRT] [W] Unused Input: position_ids\n[05/22/2025-07:22:30] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n[05/22/2025-07:22:30] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n[05/22/2025-07:22:30] [TRT] [I] Compiler backend is used during engine build.\n[05/22/2025-07:22:40] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:22:40] [TRT] [I] Detected 17 inputs and 1 output network tensors.\nXQA Macros: USE_INPUT_KV=1 INPUT_FP16=0 CACHE_ELEM_ENUM=2 HEAD_GRP_SIZE=4 __FORCE_INCLUDE_CUDA_FP16_HPP_FROM_FP16_H__=1 ROPE_STYLE=1 LOW_PREC_OUTPUT=1 DTYPE=__nv_bfloat16 GENERATE_CUBIN=1 HEAD_ELEMS=128 NDEBUG=1 BEAM_WIDTH=1 TOKENS_PER_PAGE=32 M_TILESIZE=4 __FORCE_INCLUDE_CUDA_BF16_HPP_FROM_BF16_H__=1 USE_CUSTOM_BARRIER=1 SLIDING_WINDOW=1 \n[05/22/2025-07:22:43] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:22:43] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:22:43] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:22:43] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:22:43] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.11304ms to assign 21 blocks to 175 nodes requiring 140780032 bytes.\n[05/22/2025-07:22:43] [TRT] [I] Total Activation Memory: 140777984 bytes\n[05/22/2025-07:22:46] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:22:46] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:22:47] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:22:47] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:22:47] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:22:47] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:22:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.4582ms to assign 21 blocks to 175 nodes requiring 143139328 bytes.\n[05/22/2025-07:22:47] [TRT] [I] Total Activation Memory: 143139328 bytes\n[05/22/2025-07:22:51] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:22:51] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:22:51] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:22:51] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:22:51] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:22:51] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:22:51] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.58895ms to assign 21 blocks to 175 nodes requiring 147857920 bytes.\n[05/22/2025-07:22:51] [TRT] [I] Total Activation Memory: 147857920 bytes\n[05/22/2025-07:22:55] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:22:55] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:22:56] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:22:56] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:22:56] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:22:56] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:22:56] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.70618ms to assign 21 blocks to 175 nodes requiring 157295104 bytes.\n[05/22/2025-07:22:56] [TRT] [I] Total Activation Memory: 157295104 bytes\n[05/22/2025-07:22:59] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:22:59] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:23:00] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:23:00] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:23:00] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:23:00] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:23:00] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.7238ms to assign 21 blocks to 175 nodes requiring 176169472 bytes.\n[05/22/2025-07:23:00] [TRT] [I] Total Activation Memory: 176169472 bytes\n[05/22/2025-07:23:05] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:23:05] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:23:06] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:23:06] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:23:06] [TRT] [I] Max Scratch Memory: 754974720 bytes\n[05/22/2025-07:23:06] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:23:06] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 7.04059ms to assign 21 blocks to 175 nodes requiring 1056973312 bytes.\n[05/22/2025-07:23:06] [TRT] [I] Total Activation Memory: 1056973312 bytes\n[05/22/2025-07:23:06] [TRT] [I] Total Weights Memory: 9148379652 bytes\n[05/22/2025-07:23:06] [TRT] [I] Compiler backend is used during engine execution.\n[05/22/2025-07:23:06] [TRT] [I] Engine generation completed in 36.5777 seconds.\n[05/22/2025-07:23:06] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8725 MiB\n[05/22/2025-07:23:09] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:42\n[05/22/2025-07:23:09] [TRT] [I] Serialized 4959 bytes of code generator cache.\n[05/22/2025-07:23:09] [TRT] [I] Serialized 1912332 bytes of compilation cache.\n[05/22/2025-07:23:09] [TRT] [I] Serialized 9 timing cache entries\n[05/22/2025-07:23:09] [TRT-LLM] [I] Timing cache serialized to model.cache\n[05/22/2025-07:23:09] [TRT-LLM] [I] Build phase peak memory: 32810.35 MB, children: 11886.42 MB\n[05/22/2025-07:23:10] [TRT-LLM] [I] Serializing engine to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/tmp251svz2n-llm-workspace/tmp.engine/rank0.engine...\n[05/22/2025-07:23:29] [TRT-LLM] [I] Engine serialized. Total time: 00:00:18\n[05/22/2025-07:23:31] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue\n[05/22/2025-07:23:31] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_error_queue\n[05/22/2025-07:23:31] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue\n[05/22/2025-07:23:31] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue\n[05/22/2025-07:23:31] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue\n[05/22/2025-07:23:48] [TRT-LLM] [I] Starting TensorRT-LLM init.\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-07:23:48] [TRT-LLM] [I] TensorRT-LLM inited.\n[TensorRT-LLM] TensorRT-LLM version: 0.21.0rc0\n[05/22/2025-07:23:50] [TRT-LLM] [W] Found worker process 94149 was bound to {2, 18}, this may harmperformance.\n[05/22/2025-07:23:50] [TRT-LLM] [W] Will clear the cpu affinity\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[05/22/2025-07:23:50] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info\n[TensorRT-LLM][INFO] Engine version 0.21.0rc0 found in the config file, assuming engine(s) built by new builder API.\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 4096\n[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 4096\n[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256\n[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (256) * 32\n[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 255 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n[TensorRT-LLM][INFO] Loaded engine size: 8791 MiB\n[TensorRT-LLM][INFO] Engine load time 3243 ms\n[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1008.01 MiB for execution context memory.\n[TensorRT-LLM][INFO] gatherContextLogits: 0\n[TensorRT-LLM][INFO] gatherGenerationLogits: 0\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 2. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 3. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 4. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 5. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1022.30 MB GPU memory for runtime buffers.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 5.10 GB GPU memory for decoder.\n[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.18 GiB, available: 61.46 GiB, extraCostMemory: 0.00 GiB\n[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 28322\n[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n[TensorRT-LLM][INFO] before Create KVCacheManager cacheTransPreAllocaSize:0\n[TensorRT-LLM][INFO] Max KV cache pages per sequence: 8 [window size=256]\n[TensorRT-LLM][INFO] Number of tokens per block: 32.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 55.32 GiB for max tokens in paged KV cache (906304).\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.fc_after_embed = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_input_layernorm_in_first_layer = True\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_last_layernorm = True\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.layer_idx_offset = 0\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.producer = {'name': 'modelopt', 'version': '0.29.0'}\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.share_embedding_table = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.bias = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rotary_pct = 1.0\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rank = 0\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.decoder = llama\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rmsnorm = True\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.lm_head_bias = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:23:56] [TRT-LLM] [W] Implicitly setting LLaMAConfig.model_type = llama\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set gemm_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set explicitly_disable_gemm_plugin to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set qserve_gemm_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set identity_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set lora_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set dora_plugin to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set smooth_quant_plugins to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set moe_plugin to auto.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set gemm_allreduce_plugin to None.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set context_fmha to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set paged_kv_cache to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set remove_input_padding to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set norm_quant_fusion to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set reduce_fusion to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set user_buffer to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set tokens_per_block to 32.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set fuse_fp4_quant to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set multiple_profiles to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set streamingllm to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set manage_weights to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set use_fused_mlp to True.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n[05/22/2025-07:23:56] [TRT-LLM] [I] Save model to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[05/22/2025-07:24:09] [TRT-LLM] [I] \n===========================================================\n= ENGINE BUILD INFO\n===========================================================\nModel Name:		meta-llama/Llama-3.1-8B\nModel Path:		/scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\nWorkspace Directory:	/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8\nEngine Directory:	/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n\n===========================================================\n= ENGINE CONFIGURATION DETAILS\n===========================================================\nMax Sequence Length:		256\nMax Batch Size:			4096\nMax Num Tokens:			8192\nQuantization:			FP8\nKV Cache Dtype:			FP8\n===========================================================\n\n[05/22/2025-07:24:09] [TRT-LLM] [I] \n\n===========================================================\nENGINE SAVED: /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n===========================================================\n\n",36.5777,254.122006,2025-05-22 07:19:59,2025-05-22 07:24:13,valid,  trtllm-bench --log_level=verbose --workspace=/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8 --model=meta-llama/Llama-3.1-8B --model_path=/scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B build --dataset=/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-19-47/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/synthetic_data.json --tp_size=1 --pp_size=1 --max_seq_len=256 --quantization=FP8,0.1,30,BUILD_TIME
"llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8","test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]",375,1593,0,"H100/perf/test_perf.py::test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]","test_perf_metric_build_time[llama_v3.1_8b-bench-bfloat16-maxbs:512-maxnt:2048-input_output_len:128,128-quant:fp8]","H100/perf/test_perf.py::test_perf[llama_v3.1_8b-bench-bfloat16-input_output_len:128,128-quant:fp8]","\n[05/22/2025-07:56:21] [TRT-LLM] [I] Starting TensorRT-LLM init.\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-07:56:21] [TRT-LLM] [I] TensorRT-LLM inited.\n[TensorRT-LLM] TensorRT-LLM version: 0.21.0rc0\n[05/22/2025-07:56:23] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: verbose\n[05/22/2025-07:56:23] [TRT-LLM] [I] Found dataset.\n[05/22/2025-07:56:23] [TRT-LLM] [I] \n===========================================================\n= DATASET DETAILS\n===========================================================\nDataset Path:         None\nNumber of Sequences:  512\n\n-- Percentiles statistics ---------------------------------\n\n        Input              Output           Seq. Length\n-----------------------------------------------------------\nMIN:   128.0000           128.0000           256.0000\nMAX:   128.0000           128.0000           256.0000\nAVG:   128.0000           128.0000           256.0000\nP50:   128.0000           128.0000           256.0000\nP90:   128.0000           128.0000           256.0000\nP95:   128.0000           128.0000           256.0000\nP99:   128.0000           128.0000           256.0000\n===========================================================\n\n[05/22/2025-07:56:23] [TRT-LLM] [I] Max batch size and max num tokens are not provided, use tuning heuristics or pre-defined setting from trtllm-bench.\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated engine size: 7.48 GB\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated total available memory for KV cache: 72.17 GB\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated total KV cache memory: 68.56 GB\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated max number of requests in KV cache memory: 4387.86\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated max batch size (after fine-tune): 4096\n[05/22/2025-07:56:23] [TRT-LLM] [I] Estimated max num tokens (after fine-tune): 8192\n[05/22/2025-07:56:23] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-07:56:23] [TRT-LLM] [I] Set multiple_profiles to True.\n[05/22/2025-07:56:23] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n[05/22/2025-07:56:23] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n[05/22/2025-07:56:23] [TRT-LLM] [W] Overriding LlmArgsBase.max_input_len (annotation=int required=False default=1024 description='The maximum input length.') with build_config.max_input_len (1024).\n[05/22/2025-07:56:23] [TRT-LLM] [W] Overriding LlmArgsBase.max_seq_len (annotation=Union[int, NoneType] required=False default=None description='The maximum sequence length.') with build_config.max_seq_len (256).\n[05/22/2025-07:56:23] [TRT-LLM] [W] Overriding LlmArgsBase.max_beam_width (annotation=int required=False default=1 description='The maximum beam width.') with build_config.max_beam_width (1).\n[05/22/2025-07:56:23] [TRT-LLM] [W] Using default gpus_per_node: 1\n[05/22/2025-07:56:23] [TRT-LLM] [I] Specified dtype 'auto'; inferred dtype 'bfloat16'.\n[05/22/2025-07:56:23] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:56:23] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:56:23] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-07:56:23] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:56:23] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:56:23] [TRT-LLM] [I] Initializing model from /scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\n[05/22/2025-07:56:25] [TRT-LLM] [I] Initializing tokenizer from /scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\n[05/22/2025-07:56:25] [TRT-LLM] [I] Loading calibration dataset\n[05/22/2025-07:56:31] [TRT-LLM] [I] Starting quantization...\nRegistered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> for KV Cache quantization\nInserted 771 quantizers\n[05/22/2025-07:57:53] [TRT-LLM] [I] Quantization done. Total time used: 82.30 s.\ncurrent rank: 0, tp rank: 0, pp rank: 0\n[05/22/2025-07:57:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:57:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:58:14] [TRT-LLM] [I] Quantized model exported to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/tmplbgae_lk-llm-workspace/quantized-checkpoint \nTotal time used 20.10 s.\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.producer = {'name': 'modelopt', 'version': '0.29.0'}\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.share_embedding_table = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.bias = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rotary_pct = 1.0\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rank = 0\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.decoder = llama\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rmsnorm = True\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.lm_head_bias = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.fc_after_embed = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_input_layernorm_in_first_layer = True\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_last_layernorm = True\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.layer_idx_offset = 0\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-07:58:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.model_type = llama\n[05/22/2025-07:58:15] [TRT-LLM] [I] Set paged_kv_cache to True.\n[05/22/2025-07:58:15] [TRT-LLM] [W] Overriding paged_state to False\n[05/22/2025-07:58:15] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-07:58:15] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-07:58:15] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-07:58:15] [TRT-LLM] [W] max_input_len is 1024 is larger than max_seq_len 256, clipping it to max_seq_len\n[05/22/2025-07:58:15] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n[05/22/2025-07:58:28] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 6151, GPU 636 (MiB)\n[05/22/2025-07:58:29] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1220, GPU +6, now: CPU 7170, GPU 642 (MiB)\n[05/22/2025-07:58:29] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-07:58:29] [TRT-LLM] [I] Total time of constructing network from module object 14.438017129898071 seconds\n[05/22/2025-07:58:29] [TRT-LLM] [I] Total optimization profiles added: 6\n[05/22/2025-07:58:29] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n[05/22/2025-07:58:29] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n[05/22/2025-07:58:29] [TRT] [W] Unused Input: position_ids\n[05/22/2025-07:58:33] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n[05/22/2025-07:58:33] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n[05/22/2025-07:58:33] [TRT] [I] Compiler backend is used during engine build.\n[05/22/2025-07:58:43] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:58:43] [TRT] [I] Detected 17 inputs and 1 output network tensors.\nXQA Macros: USE_INPUT_KV=1 INPUT_FP16=0 CACHE_ELEM_ENUM=2 HEAD_GRP_SIZE=4 __FORCE_INCLUDE_CUDA_FP16_HPP_FROM_FP16_H__=1 ROPE_STYLE=1 LOW_PREC_OUTPUT=1 DTYPE=__nv_bfloat16 GENERATE_CUBIN=1 HEAD_ELEMS=128 NDEBUG=1 BEAM_WIDTH=1 TOKENS_PER_PAGE=32 M_TILESIZE=4 __FORCE_INCLUDE_CUDA_BF16_HPP_FROM_BF16_H__=1 USE_CUSTOM_BARRIER=1 SLIDING_WINDOW=1 \n[05/22/2025-07:58:45] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:58:45] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:58:45] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:58:45] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:58:45] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 5.97675ms to assign 21 blocks to 175 nodes requiring 140780032 bytes.\n[05/22/2025-07:58:45] [TRT] [I] Total Activation Memory: 140777984 bytes\n[05/22/2025-07:59:00] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:59:00] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:59:01] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:59:01] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:59:01] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:59:01] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:59:01] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.52666ms to assign 21 blocks to 175 nodes requiring 143139328 bytes.\n[05/22/2025-07:59:01] [TRT] [I] Total Activation Memory: 143139328 bytes\n[05/22/2025-07:59:04] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:59:04] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:59:05] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:59:05] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:59:05] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:59:05] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:59:05] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.61347ms to assign 21 blocks to 175 nodes requiring 147857920 bytes.\n[05/22/2025-07:59:05] [TRT] [I] Total Activation Memory: 147857920 bytes\n[05/22/2025-07:59:08] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:59:08] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:59:09] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:59:09] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:59:09] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:59:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:59:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.88478ms to assign 21 blocks to 175 nodes requiring 157295104 bytes.\n[05/22/2025-07:59:09] [TRT] [I] Total Activation Memory: 157295104 bytes\n[05/22/2025-07:59:12] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:59:12] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:59:13] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:59:13] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:59:13] [TRT] [I] Max Scratch Memory: 138412032 bytes\n[05/22/2025-07:59:13] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:59:13] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.79197ms to assign 21 blocks to 175 nodes requiring 176169472 bytes.\n[05/22/2025-07:59:13] [TRT] [I] Total Activation Memory: 176169472 bytes\n[05/22/2025-07:59:18] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[05/22/2025-07:59:18] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n[05/22/2025-07:59:18] [TRT] [I] Total Host Persistent Memory: 67152 bytes\n[05/22/2025-07:59:18] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[05/22/2025-07:59:18] [TRT] [I] Max Scratch Memory: 754974720 bytes\n[05/22/2025-07:59:18] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 175 steps to complete.\n[05/22/2025-07:59:18] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 7.09646ms to assign 21 blocks to 175 nodes requiring 1056973312 bytes.\n[05/22/2025-07:59:18] [TRT] [I] Total Activation Memory: 1056973312 bytes\n[05/22/2025-07:59:19] [TRT] [I] Total Weights Memory: 9148379652 bytes\n[05/22/2025-07:59:19] [TRT] [I] Compiler backend is used during engine execution.\n[05/22/2025-07:59:19] [TRT] [I] Engine generation completed in 46.117 seconds.\n[05/22/2025-07:59:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8725 MiB\n[05/22/2025-07:59:21] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:51\n[05/22/2025-07:59:21] [TRT] [I] Serialized 5010 bytes of code generator cache.\n[05/22/2025-07:59:21] [TRT] [I] Serialized 1654870 bytes of compilation cache.\n[05/22/2025-07:59:21] [TRT] [I] Serialized 9 timing cache entries\n[05/22/2025-07:59:21] [TRT-LLM] [I] Timing cache serialized to model.cache\n[05/22/2025-07:59:21] [TRT-LLM] [I] Build phase peak memory: 32983.74 MB, children: 11888.58 MB\n[05/22/2025-07:59:23] [TRT-LLM] [I] Serializing engine to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/tmplbgae_lk-llm-workspace/tmp.engine/rank0.engine...\n[05/22/2025-07:59:39] [TRT-LLM] [I] Engine serialized. Total time: 00:00:16\n[05/22/2025-07:59:40] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_queue\n[05/22/2025-07:59:40] [TRT-LLM] [I] Generating a new HMAC key for server proxy_request_error_queue\n[05/22/2025-07:59:40] [TRT-LLM] [I] Generating a new HMAC key for server proxy_result_queue\n[05/22/2025-07:59:40] [TRT-LLM] [I] Generating a new HMAC key for server proxy_stats_queue\n[05/22/2025-07:59:40] [TRT-LLM] [I] Generating a new HMAC key for server proxy_kv_cache_events_queue\n[05/22/2025-08:00:05] [TRT-LLM] [I] Starting TensorRT-LLM init.\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-08:00:05] [TRT-LLM] [I] TensorRT-LLM inited.\n[TensorRT-LLM] TensorRT-LLM version: 0.21.0rc0\n[05/22/2025-08:00:07] [TRT-LLM] [W] Found worker process 98701 was bound to {2, 18}, this may harmperformance.\n[05/22/2025-08:00:07] [TRT-LLM] [W] Will clear the cpu affinity\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[05/22/2025-08:00:07] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: info\n[TensorRT-LLM][INFO] Engine version 0.21.0rc0 found in the config file, assuming engine(s) built by new builder API.\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 4096\n[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 4096\n[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 256\n[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (256) * 32\n[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 255 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n[TensorRT-LLM][INFO] Loaded engine size: 8786 MiB\n[TensorRT-LLM][INFO] Engine load time 3261 ms\n[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1008.01 MiB for execution context memory.\n[TensorRT-LLM][INFO] gatherContextLogits: 0\n[TensorRT-LLM][INFO] gatherGenerationLogits: 0\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 2. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 3. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 4. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8724 (MiB)\n[TensorRT-LLM][INFO] Switching optimization profile from: 0 to 5. Please ensure there are no enqueued operations pending in this context prior to switching profiles\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1022.30 MB GPU memory for runtime buffers.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 5.10 GB GPU memory for decoder.\n[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 79.18 GiB, available: 61.47 GiB, extraCostMemory: 0.00 GiB\n[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 28327\n[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n[TensorRT-LLM][INFO] before Create KVCacheManager cacheTransPreAllocaSize:0\n[TensorRT-LLM][INFO] Max KV cache pages per sequence: 8 [window size=256]\n[TensorRT-LLM][INFO] Number of tokens per block: 32.\n[TensorRT-LLM][INFO] [MemUsageChange] Allocated 55.33 GiB for max tokens in paged KV cache (906464).\n[TensorRT-LLM][INFO] Set logger level to INFO\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.fc_after_embed = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_input_layernorm_in_first_layer = True\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_last_layernorm = True\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.layer_idx_offset = 0\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.has_partial_lora_mask = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.producer = {'name': 'modelopt', 'version': '0.29.0'}\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.share_embedding_table = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.bias = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rotary_pct = 1.0\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rank = 0\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.decoder = llama\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rmsnorm = True\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.lm_head_bias = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n[05/22/2025-08:00:12] [TRT-LLM] [W] Implicitly setting LLaMAConfig.model_type = llama\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set dtype to bfloat16.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set gemm_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set explicitly_disable_gemm_plugin to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set qserve_gemm_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set identity_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set nccl_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set lora_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set dora_plugin to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set smooth_quant_plugins to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set moe_plugin to auto.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set gemm_allreduce_plugin to None.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set context_fmha to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set paged_kv_cache to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set remove_input_padding to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set norm_quant_fusion to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set reduce_fusion to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set user_buffer to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set tokens_per_block to 32.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set fuse_fp4_quant to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set multiple_profiles to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set paged_state to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set streamingllm to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set manage_weights to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set use_fused_mlp to True.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n[05/22/2025-08:00:12] [TRT-LLM] [I] Save model to /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[05/22/2025-08:00:31] [TRT-LLM] [I] \n===========================================================\n= ENGINE BUILD INFO\n===========================================================\nModel Name:		meta-llama/Llama-3.1-8B\nModel Path:		/scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B\nWorkspace Directory:	/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8\nEngine Directory:	/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n\n===========================================================\n= ENGINE CONFIGURATION DETAILS\n===========================================================\nMax Sequence Length:		256\nMax Batch Size:			4096\nMax Num Tokens:			8192\nQuantization:			FP8\nKV Cache Dtype:			FP8\n===========================================================\n\n[05/22/2025-08:00:31] [TRT-LLM] [I] \n\n===========================================================\nENGINE SAVED: /home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/meta-llama/Llama-3.1-8B/tp_1_pp_1\n===========================================================\n\n",46.117,268.112764,2025-05-22 07:56:08,2025-05-22 08:00:36,valid,  trtllm-bench --log_level=verbose --workspace=/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8 --model=meta-llama/Llama-3.1-8B --model_path=/scratch.trt_llm_data/llm-models/llama-3.1-model/Meta-Llama-3.1-8B build --dataset=/home/scratch.huig_gpu/TensorRT-LLM/tests/integration/defs/llm-test-workspace/ws-2025-05-22-07-56-01/perf_engines/llama_v3.1_8b-bench-bfloat16-input_output_len_128_128-quant_fp8/synthetic_data.json --tp_size=1 --pp_size=1 --max_seq_len=256 --quantization=FP8,0.1,30,BUILD_TIME
