version: 0.0.1
l0_a10:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: pytorch
  tests:
  # ------------- PyTorch tests ---------------
  - unittest/_torch/sampler/test_torch_sampler.py
  - unittest/_torch/modeling/test_modeling_mistral.py
  - unittest/_torch/modeling/test_modeling_pixtral.py
  # NOTE: this is a CPU-only test, but we do not have a dedicated job for this (and therefore no
  # test list either).
  - unittest/_torch/models/checkpoints/hf/test_weight_loader.py
  - unittest/others/test_time_breakdown.py
  - unittest/disaggregated/test_disagg_utils.py
  - unittest/disaggregated/test_router.py
  - unittest/disaggregated/test_remoteDictionary.py
  - unittest/disaggregated/test_disagg_cluster_manager_worker.py
  - unittest/disaggregated/test_cluster_storage.py
  - disaggregated/test_disaggregated.py::test_disaggregated_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_single_gpu_with_mpirun_trt_backend[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_cuda_graph[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_mixed[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_overlap[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_diff_max_tokens[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_kv_cache_time_output[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_perf_metrics[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_cache_aware_balance[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_conditional[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_ngram[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_workers.py::test_workers_conditional_disaggregation[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_workers.py::test_workers_kv_cache_events[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_workers.py::test_workers_kv_cache_aware_router[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_workers.py::test_workers_kv_cache_aware_router_eviction[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated_single_gpu.py::test_disaggregated_simple_llama[False-False-TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated_single_gpu.py::test_disaggregated_simple_llama[False-True-TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated_single_gpu.py::test_disaggregated_simple_llama[True-False-TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated_single_gpu.py::test_disaggregated_simple_llama[True-True-TinyLlama-1.1B-Chat-v1.0]
  - test_e2e.py::test_openai_chat_guided_decoding
  - test_e2e.py::test_openai_chat_multimodal_example
  - test_e2e.py::test_openai_perf_metrics
  - test_e2e.py::test_openai_prometheus
  - test_e2e.py::test_openai_lora
  - test_e2e.py::test_trtllm_serve_multimodal_example
  - test_e2e.py::test_trtllm_serve_lora_example
  - test_e2e.py::test_trtllm_serve_top_logprobs[pytorch]
  - test_e2e.py::test_openai_misc_example[pytorch]
  - test_e2e.py::test_openai_reasoning[pytorch]
  - test_e2e.py::test_openai_completions_example[pytorch]
  - test_e2e.py::test_openai_chat_example[pytorch] TIMEOUT (90)
  - test_e2e.py::test_trtllm_bench_request_rate_and_concurrency[enable_concurrency-]
  - test_e2e.py::test_trtllm_bench_invalid_token_pytorch[TinyLlama-1.1B-Chat-v1.0-TinyLlama-1.1B-Chat-v1.0]
  # llmapi
  - unittest/llmapi/test_llm_utils.py
  - unittest/llmapi/test_gc_utils.py
  - unittest/llmapi/test_reasoning_parser.py
  - unittest/llmapi/test_serialization.py
  - unittest/llmapi/test_utils.py
  - unittest/llmapi/test_llm_args.py
  - unittest/llmapi/test_additional_model_outputs.py
  # executor
  - unittest/executor/test_rpc.py
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: cpp
  tests:
  # ------------- CPP tests ---------------
  - cpp/test_e2e.py::test_model[-medusa-86]
  - cpp/test_e2e.py::test_model[-redrafter-86]
  - cpp/test_e2e.py::test_model[-mamba-86]
  - cpp/test_e2e.py::test_model[-recurrentgemma-86]
  - cpp/test_e2e.py::test_model[-eagle-86] TIMEOUT (90)
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: tensorrt
  tests:
  # ------------- TRT tests ---------------
  - unittest/trt/attention/test_gpt_attention.py -k "partition0"
  - unittest/trt/attention/test_gpt_attention.py -k "partition1"
  - unittest/trt/attention/test_gpt_attention.py -k "partition2"
  - unittest/trt/attention/test_gpt_attention.py -k "partition3"
  - unittest/trt/attention/test_gpt_attention.py -k "xqa_generic"
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-use_attention_plugin-enable_context_fmha-tp:1-pp:1-float16-BertModel-bert/bert-base-uncased]
  - unittest/trt/model/test_mistral.py
  - unittest/trt/model/test_llama.py
  - llmapi/test_llm_api_connector.py::test_connector_simple[True]
  - llmapi/test_llm_api_connector.py::test_connector_simple[False]
  - llmapi/test_llm_api_connector.py::test_connector_async_onboard[True]
  - llmapi/test_llm_api_connector.py::test_connector_async_onboard[False]
  - llmapi/test_llm_api_connector.py::test_connector_async_save[True]
  - llmapi/test_llm_api_connector.py::test_connector_async_save[False]
  - llmapi/test_llm_api_connector.py::test_connector_scheduler_output[True]
  - llmapi/test_llm_api_connector.py::test_connector_scheduler_output[False]
  - llmapi/test_llm_api_connector.py::test_connector_scheduler_output_chunked_context[True]
  - llmapi/test_llm_api_connector.py::test_connector_scheduler_output_chunked_context[False]
  - llmapi/test_llm_api_connector.py::test_connector_disagg_prefill[False]
  - llmapi/test_llm_api_connector.py::test_connector_disagg_prefill[True]
  - llmapi/test_llm_e2e.py::test_llmapi_load_engine_from_build_command[llama-llama-models/llama-7b-hf] # 5min
  - llmapi/test_llm_e2e.py::test_llmapi_build_command_parameters_align[llama-llama-models-v2/TinyLlama-1.1B-Chat-v1.0]
  - llmapi/test_llm_e2e.py::test_llmapi_load_engine_from_build_command_with_lora[llama-llama-models-v2/llama-v2-7b-hf]
  - llmapi/test_llm_examples.py::test_llmapi_chat_example
  - llmapi/test_llm_e2e.py::test_llmapi_exit
  - llmapi/test_llm_examples.py::test_llmapi_server_example
  - llmapi/test_llm_examples.py::test_llmapi_kv_cache_connector[Qwen2-0.5B]
  - test_e2e.py::test_trtllm_serve_example
  - test_e2e.py::test_trtllm_serve_top_logprobs[trt]
  - test_e2e.py::test_openai_misc_example[trt]
  - test_e2e.py::test_openai_completions_example[trt]
  - test_e2e.py::test_openai_chat_example[trt]
  - test_e2e.py::test_openai_reasoning[trt]
  - test_e2e.py::test_trtllm_bench_sanity[--non-streaming-FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_latency_sanity[FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - unittest/trt/quantization
  - unittest/trt/functional # 37 mins
  - llmapi/test_llm_examples.py::test_llmapi_quickstart_atexit
  - unittest/api_stability
  - unittest/bindings
  - unittest/test_model_runner_cpp.py
  - unittest/llmapi/test_build_cache.py
  - accuracy/test_cli_flow.py::TestGpt2::test_auto_dtype # 1 min
  - accuracy/test_cli_flow.py::TestGpt2::test_beam_search # 1 min
  - accuracy/test_cli_flow.py::TestGpt2::test_beam_search_large # 6 mins
  - accuracy/test_cli_flow.py::TestGpt2::test_variable_beam_width_search # 1 mins
  - accuracy/test_cli_flow.py::TestGpt2::test_weight_streaming_ootb # 1.5 mins
  - accuracy/test_cli_flow.py::TestGpt2::test_cuda_graph # 1 min
  - accuracy/test_cli_flow.py::TestSantacoder::test_auto_dtype # 1.5 mins
  - accuracy/test_cli_flow.py::TestMamba130M::test_auto_dtype # 1 min
  - accuracy/test_cli_flow.py::TestLongAlpaca7B::test_multiblock_aggressive # 6 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_lookahead # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle_2[cuda_graph=False-chunked_context=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle_2[cuda_graph=True-chunked_context=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle_2[cuda_graph=True-chunked_context=True] # 5 mins
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_auto_dtype
  - unittest/trt/attention/test_gpt_attention_IFB.py
  - unittest/trt/attention/test_gpt_attention_no_cache.py
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-enable_attention_plugin-disable_weight_only-float16-nb:1-use_cpp_runtime]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: tensorrt
  tests:
  - test_e2e.py::test_mistral_e2e[use_py_session---]
  - test_e2e.py::test_mistral_e2e[use_cpp_session-remove_input_padding--]
  - test_e2e.py::test_mistral_e2e[use_py_session-remove_input_padding--]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-disable_remove_input_padding-disable_attention_plugin-disable_context_fmha-tp:1-pp:1-float32-BertModel-bert/bert-base-uncased]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-use_attention_plugin-enable_context_fmha-tp:1-pp:1-float16-RobertaModel-bert/roberta-base]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-disable_attention_plugin-disable_context_fmha-tp:1-pp:1-float16-RobertaForSequenceClassification-bert/twitter-roberta-base-emotion]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-disable_remove_input_padding-disable_attention_plugin-disable_context_fmha-tp:1-pp:1-float32-RobertaModel-bert/roberta-base]
  - examples/test_nemotron.py::test_llm_nemotron_3_8b_1gpu[bfloat16-fp8] # 18mins
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-disable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime] # 8 mins
  - unittest/trt/attention/test_bert_attention.py # 12 mins
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_py_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_cpp_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba-130m-float16-disable_gemm_plugin] # 3 mins
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba2-130m-float16-disable_gemm_plugin]
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba-codestral-7B-v0.1-float16-disable_gemm_plugin] # 4 mins
  - accuracy/test_cli_flow.py::TestGpt2::test_context_fmha_disabled # 1 min
  - accuracy/test_cli_flow.py::TestLlama7B::test_auto_dtype # 2 mins
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_cyclic_kv_cache
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_cyclic_kv_cache_beam_search
  - accuracy/test_cli_flow.py::TestGpt2::test_attention_ootb
  - accuracy/test_cli_flow.py::TestStarcoder2_3B::test_auto_dtype
  - accuracy/test_cli_flow.py::TestPhi2::test_auto_dtype # 2 mins
  - accuracy/test_cli_flow.py::TestGemma2_9BIt::test_auto_dtype
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba-130m-float16-enable_gemm_plugin] # 2 mins
  - llmapi/test_llm_e2e.py::test_llmapi_load_engine_from_build_command[llama-codellama/CodeLlama-7b-Instruct-hf] # 5min
  - llmapi/test_llm_e2e.py::test_llmapi_load_ckpt_from_convert_command # 5min
  - examples/test_openai.py::test_llm_openai_triton_1gpu
  - examples/test_openai.py::test_llm_openai_triton_plugingen_1gpu
  - test_e2e.py::test_build_time_benchmark_sanity
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-enable_gemm_plugin-enable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime]
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-enable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime] # 4 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_medusa[cuda_graph=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_medusa[cuda_graph=True] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle[cuda_graph=False-chunked_context=False-typical_acceptance=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle[cuda_graph=True-chunked_context=False-typical_acceptance=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle[cuda_graph=True-chunked_context=True-typical_acceptance=False] # 5 mins
  - accuracy/test_cli_flow.py::TestVicuna7B::test_eagle[cuda_graph=True-chunked_context=False-typical_acceptance=True] # 5 mins
  - accuracy/test_llm_api.py::TestEagleVicuna_7B_v1_3::test_auto_dtype
  - accuracy/test_llm_api.py::TestEagle2Vicuna_7B_v1_3::test_auto_dtype
  - stress_test/stress_test.py::test_run_stress_test[llama-v3-8b-instruct-hf_tp1-stress_time_300s_timeout_450s-MAX_UTILIZATION-trt-stress-test]
  - stress_test/stress_test.py::test_run_stress_test[llama-v3-8b-instruct-hf_tp1-stress_time_300s_timeout_450s-GUARANTEED_NO_EVICT-trt-stress-test]
  - test_e2e.py::test_gpt3_175b_1layers_build_only # 6 mins
  - examples/test_chatglm.py::test_llm_glm_4_9b_single_gpu_summary[glm-4-9b-disable_weight_only]
  - unittest/trt/model/test_mamba.py # 3 mins
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba2-130m-float16-enable_gemm_plugin]
  - examples/test_mamba.py::test_llm_mamba_1gpu[mamba-codestral-7B-v0.1-float16-enable_gemm_plugin] # 3 mins
  - accuracy/test_cli_flow.py::TestLlama7B::test_streamingllm # 2 mins
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: pytorch
  tests:
  - stress_test/stress_test.py::test_run_stress_test[llama-v3-8b-instruct-hf_tp1-stress_time_300s_timeout_450s-MAX_UTILIZATION-pytorch-stress-test]
  - stress_test/stress_test.py::test_run_stress_test[llama-v3-8b-instruct-hf_tp1-stress_time_300s_timeout_450s-GUARANTEED_NO_EVICT-pytorch-stress-test]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: fmha
  tests:
  - test_fmha.py::test_fmha TIMEOUT (90)
l0_a10_pybind:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a10*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
  tests:
  - unittest/bindings
  - test_e2e.py::test_openai_chat_example[trt]
  - test_e2e.py::test_openai_chat_example[pytorch] TIMEOUT (90)
