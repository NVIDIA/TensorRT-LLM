version: 0.0.1
l0_h100:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*h100*'
      linux_distribution_name: ubuntu*
  tests:
  # ------------- PyTorch tests ---------------
  # Only key models in H100: llama/mixtral/nemotron/deepseek
  - unittest/_torch -k "not (modeling or multi_gpu or auto_deploy)"
  - unittest/_torch -k "modeling_llama"
  - unittest/_torch/multi_gpu_modeling -k "llama and tp1 and pp1"
  - unittest/_torch/modeling -k "modeling_mixtral"
  - unittest/_torch/modeling -k "modeling_nemotron"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and tp1 and nextn0"
  - test_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-False-False]
  - test_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-instruct-hf-fp8-True-True]
  - examples/test_pytorch.py::test_llm_llama_1gpu[llama-3.1-8b-disable_fp4]
  - examples/test_pytorch.py::test_llm_deepseek_1gpu[deepseek-v3-lite-enable_fp8-disable_fp4]
  # ------------- CPP tests ---------------
  - test_cpp.py::test_unit_tests[90]
  - test_cpp.py::test_model[fp8-llama-90]
  - test_cpp.py::test_model[t5-90]
  - test_cpp.py::test_benchmarks[t5-90]
  - test_cpp.py::test_model[encoder-90]
  - test_cpp.py::test_model[enc_dec_language_adapter-90]
  # ------------- TRT tests ---------------
  - unittest/attention/test_gpt_attention.py -k "xqa_generic"
  - unittest/functional/test_moe.py
  - unittest/quantization/test_weight_only_quant_matmul.py
  - unittest/quantization/test_weight_only_groupwise_quant_matmul.py
  - test_e2e.py::test_trtllm_bench_sanity[extra_config-streaming-FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_sanity[extra_config-non-streaming-FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_sanity[streaming-FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_sanity[non-streaming-FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_latency_sanity[FP16-meta-llama/Llama-3.1-8B-llama-3.1-model/Meta-Llama-3.1-8B]
  - test_e2e.py::test_trtllm_bench_request_rate_and_concurrency[enable_concurrency]
  - test_e2e.py::test_trtllm_bench_request_rate_and_concurrency[enable_concurrency-enable_request_rate] # negative test
  - test_e2e.py::test_trtllm_bench_help_sanity[meta-llama/Llama-3.1-8B]
  - accuracy/test_accuracy.py::TestLongAlpaca7B::test_multiblock_aggressive # 6 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_medusa[] # 5 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_medusa[cuda_graph] # 5 mins
  - examples/test_llama.py::test_llama_3_x_fp8_with_bf16_lora[llama-3.1-8b]
  - examples/test_qwen.py::test_llm_hf_qwen_multi_lora_1gpu[qwen2.5_1.5b_instruct]
  - examples/test_llama.py::test_llama_3_x_fp8_with_bf16_lora[llama-3.2-1b]
  - unittest/model_api/test_model_level_api.py # 9 mins on H100
  - unittest/model_api/test_model_api_multi_gpu.py # 0.5 mins on H100
  - unittest/model/test_gpt_e2e.py # 3 mins / 6 mins on H100
  - unittest/model/eagle # 1 mins on H100
  - unittest/test_model_runner_cpp.py
  - test_cache.py::test_cache_sanity # 1 sec
  - unittest/llmapi/test_llm_quant.py # 5.5 mins on H100
  - test_e2e.py::test_llmapi_quickstart_atexit
  - examples/test_medusa.py::test_llm_medusa_with_qaunt_base_model_1gpu[fp8-use_py_session-medusa-vicuna-7b-v1.3-4-heads-float16-bs1]
  - examples/test_medusa.py::test_llm_medusa_with_qaunt_base_model_1gpu[fp8-use_cpp_session-medusa-vicuna-7b-v1.3-4-heads-float16-bs1]
  - unittest/attention/test_gpt_attention_IFB.py
  - unittest/attention/test_gpt_attention_no_cache.py
  - unittest/model/test_mamba.py # 3 mins
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_cpp_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - accuracy/test_accuracy.py::TestLlama3_1_8BInstruct::test_fp8_pre_quantized
  - accuracy/test_accuracy.py::TestLlama2_7B::test_fp8_gemm_plugin
  - accuracy/test_accuracy.py::TestLlama2_7B::test_fp8_gemm_swiglu_plugin
  - accuracy/test_accuracy.py::TestLlama2_7B::test_fp8_low_latency_gemm_plugin
  - examples/test_multimodal.py::test_llm_multimodal_general[Llama-3.2-11B-Vision-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[Llama-3.2-11B-Vision-pp:1-tp:1-bfloat16-bs:8-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_fp8_multimodal_general[fp8-fp8-cnn_dailymail-Qwen2-VL-7B-Instruct-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False]
  - examples/test_multimodal.py::test_llm_fp8_multimodal_general[fp8-fp8-scienceqa-Llama-3.2-11B-Vision-Instruct-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False]
  - examples/test_enc_dec.py::test_llm_enc_dec_mmlu[flan-t5-small-float32-tp:1-pp:1-nb:1-disable_fp8] # 4 mins
  - examples/test_enc_dec.py::test_llm_enc_dec_general[compare_hf-t5-small-float32-enable_gemm_plugin-enable_attention_plugin-enable_paged_kv_cache-tp:1-pp:1-nb:1-enable_fp8] # 3 mins
  - examples/test_enc_dec.py::test_llm_enc_dec_general[compare_hf-flan-t5-small-float32-enable_gemm_plugin-enable_attention_plugin-enable_paged_kv_cache-tp:1-pp:1-nb:1-enable_fp8] # 3 mins
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*h100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
  tests:
  # ------------- CPP tests ---------------
  - test_cpp.py::test_model[fp8-chatglm-90]
  - test_cpp.py::test_model[bart-90]
  - test_cpp.py::test_benchmarks[bart-90]
  # ------------- TRT tests ---------------
  - examples/test_eagle.py::test_llm_eagle_1gpu[llama3.1-eagle-8b-hf_v0.5-float16-bs8] # 9 mins
  - examples/test_mistral.py::test_llm_mistral_nemo_minitron_fp8_quantization[Mistral-NeMo-Minitron-8B-Instruct]
  - examples/test_mistral.py::test_llm_mistral_nemo_fp8_quantization_1gpu[Mistral-Nemo-12b-Base-summarization]
  - examples/test_multimodal.py::test_llm_multimodal_general[llava-1.5-7b-hf-pp:1-tp:1-float16-bs:1-cpp_e2e:False-nb:1] # 7 mins
  - examples/test_multimodal.py::test_llm_multimodal_general[fuyu-8b-pp:1-tp:1-float16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[cogvlm-chat-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[neva-22b-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[video-neva-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[cogvlm-chat-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[kosmos-2-pp:1-tp:1-float16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[Phi-3-vision-128k-instruct-pp:1-tp:1-float16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[Phi-3.5-vision-instruct-pp:1-tp:1-float16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_llama.py::test_llm_llama_v2_1gpu_fp8_summary_and_mmlu[llama-v2-7b-hf-enable_fp8_paged_fmha-disable_mmlu_test]
  - examples/test_multimodal.py::test_llm_multimodal_general[VILA1.5-3b-pp:1-tp:1-float16-bs:1-cpp_e2e:False-nb:1] # 10 mins
  - examples/test_llama.py::test_llm_llama_v3_1_autoq_1gpu_mmlu[llama-3.1-8b] # 12 mins
  - examples/test_enc_dec.py::test_llm_enc_dec_mmlu[flan-t5-small-float32-tp:1-pp:1-nb:1-enable_fp8] # 7 mins
  - examples/test_enc_dec.py::test_llm_enc_dec_general[compare_hf-bart-large-cnn-float16-enable_gemm_plugin-enable_attention_plugin-enable_paged_kv_cache-tp:1-pp:1-nb:1-enable_fp8] # 13 mins
  - examples/test_nemotron.py::test_llm_nemotron_3_8b_1gpu[bfloat16-fp8] # 18mins
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-disable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime] # 8 mins
  - unittest/attention/test_bert_attention.py # 12 mins
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_py_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-2b-instruct]
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-1b-a400m-instruct]
  - examples/test_llama.py::test_llama_3_x_fp8_with_bf16_lora[llama-v3-8b-instruct-hf]
  - examples/test_llama.py::test_llm_llama_v2_1gpu_fp8_summary_and_mmlu[llama-v2-7b-hf-enable_fp8_fmha-enable_mmlu_test] # 24mins
  - examples/test_llama.py::test_llm_llama_v3_8b_1048k_long_context_ppl[passkey-Llama-3-8B-Instruct-Gradient-1048k]
  - examples/test_llama.py::test_llm_llama_v3_8b_1048k_long_context_ppl[SlimPajama-6B-Llama-3-8B-Instruct-Gradient-1048k]
  - accuracy/test_accuracy.py::TestGpt2::test_auto_dtype # 1 min
  - accuracy/test_accuracy.py::TestGpt2::test_beam_search # 1 min
  - accuracy/test_accuracy.py::TestGpt2::test_weight_streaming_ootb # 1.5 mins
  - accuracy/test_accuracy.py::TestGpt2::test_cuda_graph # 1 min
  - accuracy/test_accuracy.py::TestGpt2::test_context_fmha_disabled # 1 min
  - accuracy/test_accuracy.py::TestGptNext::test_auto_dtype # 1.5 mins
  - accuracy/test_accuracy.py::TestGptJ6B::test_float32 # 4 mins
  - accuracy/test_accuracy.py::TestGptJ6B::test_cyclic_kv_cache
  - accuracy/test_accuracy.py::TestGptJ6B::test_cyclic_kv_cache_beam_search
  - accuracy/test_accuracy.py::TestSantacoder::test_auto_dtype # 1.5 mins
  - accuracy/test_accuracy.py::TestMamba130M::test_auto_dtype # 1 min
  - accuracy/test_accuracy.py::TestVicuna7B::test_lookahead # 5 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_eagle[] # 5 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_eagle[cuda_graph] # 5 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_eagle[cuda_graph-chunked_context] # 5 mins
  - accuracy/test_accuracy.py::TestVicuna7B::test_eagle[cuda_graph-typical_acceptance] # 5 mins
  - accuracy/test_accuracy.py::TestPhi2::test_auto_dtype # 2 mins
  - accuracy/test_accuracy.py::TestGpt2Medium::test_fp8
  - accuracy/test_accuracy.py::TestGpt2Medium::test_fp8_lm_head
  - accuracy/test_accuracy.py::TestGpt2::test_attention_ootb
  - accuracy/test_accuracy.py::TestStarcoder2_3B::test_auto_dtype
  - accuracy/test_accuracy.py::TestMinitron4BBase::test_fp8
  - accuracy/test_accuracy.py::TestLlama2_7B::test_fp8
  - accuracy/test_accuracy.py::TestLlama3_1_8B::test_fp8
  - accuracy/test_accuracy.py::TestLlama3_2_1B::test_auto_dtype
  - accuracy/test_accuracy.py::TestGemma2_9BIt::test_auto_dtype
  - examples/test_gpt.py::test_llm_minitron_fp8_with_pseudo_loras[4b]
  - examples/test_gptj.py::test_llm_gptj_fp8_manage_weights_summary
  - examples/test_chatglm.py::test_llm_glm_4_9b_single_gpu_summary[glm-4-9b-disable_weight_only]
  - unittest/model_api/test_model_quantization.py # 20 mins on H100
  - test_e2e.py::test_benchmark_sanity_enable_fp8[llama_7b] # 55.77s H100 only
  - test_e2e.py::test_benchmark_sanity_enable_fp8[gpt_350m] # 34.07s H100 only
  - unittest/bindings # 8 mins on H100
  - test_e2e.py::test_model_api_examples
  - test_e2e.py::test_build_time_benchmark_sanity
