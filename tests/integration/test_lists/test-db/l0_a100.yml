version: 0.0.1
l0_a100:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: "pytorch"
  tests:
    - unittest/llmapi/test_llm_pytorch.py
    - unittest/llmapi/test_mpi_session.py ISOLATION
    - unittest/llmapi/test_memory_profiling.py # profile kvcache for vision encoder
    - unittest/trt/model_api/test_model_quantization.py
    # executor
    - unittest/executor/test_base_worker.py
    - unittest/executor/test_rpc_proxy.py
    - unittest/executor/test_rpc_worker.py
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: tensorrt
  tests:
  - unittest/trt/attention/test_sage_attention.py unittest/llmapi/test_llm_download.py unittest/llmapi/test_llm_kv_cache_events.py unittest/trt/model/redrafter unittest/trt/model/test_phi.py unittest/trt/model/test_unet.py unittest/trt/python_plugin unittest/tools unittest/utils unittest/others
  - unittest/llmapi/test_llm_models.py -m "part1"
  - unittest/llmapi/test_llm_models.py -m "not (part0 or part1)"
  - unittest/llmapi/test_llm.py -m "part0"
  - unittest/llmapi/test_llm.py -m "not part0" TIMEOUT (90)
  - unittest/llmapi/test_executor.py
  - accuracy/test_cli_flow.py::TestGpt2::test_int8_kv_cache # 1 min
  - accuracy/test_cli_flow.py::TestGpt2::test_weight_only[int4] # 1 min
  - accuracy/test_cli_flow.py::TestStarcoder2_15B::test_smooth_quant_ootb
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_int4_awq_int8_kv_cache
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_smooth_quant_ootb
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_smooth_quant
  - accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_weight_only[int8]
  - accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_weight_only[int4]
  - accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_weight_only_int8_kv_cache[int8]
  - accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_weight_only_manage_weights[int4]
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_smooth_quant_ootb_manage_weights
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_int8_gptq
  - accuracy/test_cli_flow.py::TestQwen2_7BInstruct::test_int4_awq_prequantized
  - accuracy/test_llm_api.py::TestLlama3_1_8BInstruct::test_guided_decoding[xgrammar]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: tensorrt
  tests:
  - accuracy/test_cli_flow.py::TestGptNext::test_auto_dtype # 1.5 mins
  - accuracy/test_cli_flow.py::TestSantacoder::test_auto_dtype # 1.5 mins
  - accuracy/test_cli_flow.py::TestGpt2::test_weight_only[int8] # 1 min
  - unittest/trt/model_api/test_model_level_api.py
  - unittest/trt/model_api/test_model_api_multi_gpu.py
  - unittest/trt/model/test_gpt_e2e.py
  - unittest/trt/model/eagle
  - unittest/llmapi/test_llm_models.py -m "part0"
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-enable_attention_plugin-disable_weight_only-float16-nb:1-use_cpp_runtime]
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-enable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: triton
  tests:
  - triton_server/test_triton.py::test_gpt_disaggregated_serving_bls[gpt-disaggregated-serving-bls]
  - triton_server/test_triton.py::test_llava[llava]
  - triton_server/test_triton.py::test_gpt_ib[gpt-ib]
  - triton_server/test_triton.py::test_gpt_ib_ptuning[gpt-ib-ptuning]
  - triton_server/test_triton.py::test_gpt_2b_ib_lora[gpt-2b-ib-lora]
  - triton_server/test_triton.py::test_gpt_ib_lad[gpt-ib-lad]
  - triton_server/test_triton.py::test_gpt_speculative_decoding[gpt-speculative-decoding]
  - triton_server/test_triton.py::test_gpt_ib_speculative_decoding_bls[gpt-ib-speculative-decoding-bls]
  - triton_server/test_triton.py::test_gpt_gather_logits[gpt-gather-logits]
  - triton_server/test_triton.py::test_medusa[medusa]
  - triton_server/test_triton.py::test_mllama[mllama]
  - triton_server/test_triton.py::test_eagle[eagle]
  - triton_server/test_triton.py::test_llava_onevision[llava_onevision]
  - triton_server/test_triton.py::test_qwen2_vl[qwen2_vl]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-enableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-max_utilization---1-1-1-False-ensemble]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-enableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-max_utilization---1-1-1-False-tensorrt_llm_bls]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-enableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-guaranteed_no_evict---1-1-1-False-ensemble]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-enableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-guaranteed_no_evict---1-1-1-False-tensorrt_llm_bls]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-disableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-max_utilization---1-1-1-False-ensemble]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-disableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-max_utilization---1-1-1-False-tensorrt_llm_bls]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-disableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-guaranteed_no_evict---1-1-1-False-ensemble]
  - triton_server/test_triton_llm.py::test_mistral_small_3_1_24b_pixtral[TYPE_FP16-TYPE_BF16-False-1---False-True-False-0-1-disableDecoupleMode-inflight_fused_batching-disableTrtOverlap--0.7-guaranteed_no_evict---1-1-1-False-tensorrt_llm_bls]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*a100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: fmha
  tests:
  - test_fmha.py::test_fmha
