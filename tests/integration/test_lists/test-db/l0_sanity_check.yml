version: 0.0.1
l0_sanity_check:
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      wildcards:
        gpu:
          - '*b100*'
          - '*h100*'
          - '*h200*'
          - '*l40s*'
          - '*a10*'
          - '*gb202*'
          - '*gb203*'
        linux_distribution_name: ubuntu*
    tests:
      - llmapi/test_llm_examples.py::test_llmapi_quickstart
      - llmapi/test_llm_examples.py::test_llmapi_example_inference
      - llmapi/test_llm_examples.py::test_llmapi_example_inference_async
      - llmapi/test_llm_examples.py::test_llmapi_example_inference_async_streaming
      - llmapi/test_llm_examples.py::test_llmapi_example_logits_processor
      - llmapi/test_llm_examples.py::test_llmapi_example_multilora
      - llmapi/test_llm_examples.py::test_llmapi_example_guided_decoding
      - llmapi/test_llm_examples.py::test_llmapi_example_customize
      - llmapi/test_llm_examples.py::test_llmapi_example_quantization
      - examples/test_llm_api_with_mpi.py::test_llm_api_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
      # pytorch test
      - test_e2e.py::test_openai_chat_structural_tag_example
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      # "No available XQA kernels are found for speculative decoding mode" on B100
      wildcards:
        gpu:
          - '*h100*'
          - '*h200*'
          - '*l40s*'
          - '*a10*'
        linux_distribution_name: ubuntu*
    tests:
      - llmapi/test_llm_examples.py::test_llmapi_example_lookahead_decoding
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      # Need FP8 support
      # "No available XQA kernels are found for speculative decoding mode" on B100
      wildcards:
        gpu:
          - '*h100*'
          - '*h200*'
          - '*l40s*'
        linux_distribution_name: ubuntu*
    tests:
      - llmapi/test_llm_examples.py::test_llmapi_example_medusa_decoding_use_modelopt
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      wildcards:
        gpu:
          - '*b100*'
          - '*h200*'
        linux_distribution_name: ubuntu*
    tests:
      # pytorch test
      - test_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-False-False]
      - test_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-FP8-llama-3.1-model/Llama-3.1-8B-Instruct-FP8]
