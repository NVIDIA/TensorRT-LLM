version: 0.0.1
l0_sanity_check:
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      wildcards:
        gpu:
          - '*b100*'
          - '*h100*'
          - '*h200*'
          - '*l40s*'
          - '*a10*'
          - '*gb202*'
          - '*gb203*'
          - '*5080*'
          - '*5090*'
        linux_distribution_name: ubuntu*
    tests:
      - llmapi/test_llm_examples.py::test_llmapi_quickstart
      - llmapi/test_llm_examples.py::test_llmapi_example_inference
      - llmapi/test_llm_examples.py::test_llmapi_example_inference_async
      - llmapi/test_llm_examples.py::test_llmapi_example_inference_async_streaming
      - llmapi/test_llm_examples.py::test_llmapi_example_multilora
      - llmapi/test_llm_examples.py::test_llmapi_example_guided_decoding
      - llmapi/test_llm_examples.py::test_llmapi_example_logits_processor
      - llmapi/test_llm_examples.py::test_llmapi_speculative_decoding_mtp
      - llmapi/test_llm_examples.py::test_llmapi_speculative_decoding_eagle3
      - llmapi/test_llm_examples.py::test_llmapi_speculative_decoding_ngram
      - llmapi/test_llm_examples.py::test_llmapi_sampling
      - llmapi/test_llm_examples.py::test_llmapi_runtime
      - examples/test_llm_api_with_mpi.py::test_llm_api_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
