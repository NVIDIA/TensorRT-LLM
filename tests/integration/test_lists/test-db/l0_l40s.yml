version: 0.0.1
l0_l40s:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: pytorch
  tests:
  # ------------- PyTorch tests ---------------
  - unittest/_torch/modeling -k "modeling_mllama"
  - unittest/_torch/modeling -k "modeling_vila"
  - unittest/_torch/modeling -k "modeling_siglip"
  - unittest/_torch/modeling -k "modeling_qwen2_5vl"
  - test_e2e.py::test_ptp_scaffolding[DeepSeek-R1-Distill-Qwen-7B-DeepSeek-R1/DeepSeek-R1-Distill-Qwen-7B]
  - test_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-image-False]
  - test_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-video-False]
  - test_e2e.py::test_ptp_quickstart_multimodal[llava-v1.6-mistral-7b-llava-v1.6-mistral-7b-hf-image-False]
  - test_e2e.py::test_ptp_quickstart_multimodal_phi4mm[phi4-multimodal-instruct-multimodals/Phi-4-multimodal-instruct-audio]
  - test_e2e.py::test_ptp_quickstart_multimodal_phi4mm[phi4-multimodal-instruct-multimodals/Phi-4-multimodal-instruct-image]
  - test_e2e.py::test_ptp_quickstart_multimodal_phi4mm[phi4-multimodal-instruct-multimodals/Phi-4-multimodal-instruct-image_audio]
  - accuracy/test_llm_api_pytorch.py::TestQwen2_VL_7B::test_auto_dtype
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: tensorrt
  tests:
  # ------------- TRT tests ---------------
  - unittest/trt/attention/test_gpt_attention.py -k "partition0"
  - unittest/trt/attention/test_gpt_attention.py -k "partition1"
  - unittest/trt/attention/test_gpt_attention.py -k "partition2"
  - unittest/trt/attention/test_gpt_attention.py -k "partition3"
  - unittest/trt/attention/test_gpt_attention.py -k "xqa_generic"
  - unittest/trt/quantization
  - unittest/trt/functional # 37 mins
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_fp8
  - accuracy/test_cli_flow.py::TestLlama3_1_8B::test_fp8_rowwise_meta_recipe
  - accuracy/test_cli_flow.py::TestGemma2_9BIt::test_auto_dtype
  - accuracy/test_cli_flow.py::TestPhi3Small128kInstruct::test_auto_dtype
  - accuracy/test_llm_api.py::TestLlama3_1_8B::test_fp8_rowwise
  - examples/test_llama.py::test_llm_llama_v2_lora_1gpu[chinese-llama-2-lora-13b-llama-v2-13b-hf-lora_fp16-base_fp16]
  - examples/test_llama.py::test_llm_llama_v3_dora_1gpu[commonsense-llama-v3-8b-dora-r32-llama-v3-8b-hf-base_fp16]
  - examples/test_llama.py::test_llm_llama_v1_1gpu_kv_cache_reuse_with_prompt_table[llama-7b]
  - examples/test_phi.py::test_llm_phi_lora_1gpu[Phi-3-mini-4k-instruct-ru-lora-Phi-3-mini-4k-instruct-lora_fp16-base_fp16]
  - examples/test_nemotron_nas.py::test_nemotron_nas_summary_1gpu[DeciLM-7B]
  - examples/test_gpt.py::test_llm_gpt_starcoder_lora_1gpu[peft-lora-starcoder2-15b-unity-copilot-starcoder2-lora_fp16-base_fp16]
  - llmapi/test_llm_examples.py::test_llmapi_quickstart
  - llmapi/test_llm_examples.py::test_llmapi_example_inference
  - llmapi/test_llm_examples.py::test_llmapi_example_inference_async
  - llmapi/test_llm_examples.py::test_llmapi_example_inference_async_streaming
  - llmapi/test_llm_examples.py::test_llmapi_example_multilora
  - llmapi/test_llm_examples.py::test_llmapi_example_guided_decoding
  - llmapi/test_llm_examples.py::test_llmapi_example_logits_processor
  - examples/test_llm_api_with_mpi.py::test_llm_api_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: tensorrt
  tests:
  - accuracy/test_cli_flow.py::TestGpt2::test_attention_ootb
  - accuracy/test_cli_flow.py::TestStarcoder2_3B::test_auto_dtype
  - accuracy/test_cli_flow.py::TestPhi2::test_auto_dtype
  - accuracy/test_cli_flow.py::TestPhi3Small8kInstruct::test_auto_dtype
  - accuracy/test_cli_flow.py::TestQwen1_5MoeA2_7BChat::test_weight_only
  - examples/test_gpt.py::test_llm_gpt2_next_prompt_tuning[use_cpp_session-tp1] # 10 mins
  - examples/test_gpt.py::test_llm_gpt2_next_prompt_tuning[use_py_session-tp1]
  - examples/test_llama.py::test_llm_llama_1gpu_fp8_kv_cache[llama-v2-7b-hf-bfloat16] #4 mins
  - examples/test_multimodal.py::test_llm_multimodal_general[video-neva-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[kosmos-2-pp:1-tp:1-float16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_nemotron.py::test_llm_nemotron_3_8b_1gpu[bfloat16-fp8] # 18mins
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-disable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime] # 8 mins
  - unittest/trt/attention/test_bert_attention.py # 12 mins
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_py_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_cpp_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-2b-instruct]
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-1b-a400m-instruct] TIMEOUT (90)
  - examples/test_llama.py::test_llama_3_x_fp8_with_bf16_lora[llama-v3-8b-instruct-hf]
  - examples/test_qwen.py::test_llm_qwen1_5_7b_single_gpu_lora[qwen1.5_7b_chat-Qwen1.5-7B-Chat-750Mb-lora]
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
      backend: fmha
  tests:
  - test_fmha.py::test_fmha
