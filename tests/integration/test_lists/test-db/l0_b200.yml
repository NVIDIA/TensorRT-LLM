version: 0.0.1
l0_b200:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*b100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: pytorch
  tests:
  # ------------- PyTorch tests ---------------
  - accuracy/test_llm_api_pytorch.py::TestLlama3_1_8B::test_nvfp4
  - accuracy/test_llm_api_pytorch.py::TestDeepSeekV3Lite::test_auto_dtype
  - accuracy/test_llm_api_pytorch.py::TestDeepSeekV3Lite::test_nvfp4
  - test_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-NVFP4-nvfp4-quantized/Meta-Llama-3.1-8B]
  - test_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-FP8-llama-3.1-model/Llama-3.1-8B-Instruct-FP8]
  - test_e2e.py::test_ptq_quickstart_advanced_mtp[DeepSeek-V3-Lite-BF16-DeepSeek-V3-Lite/bf16]
  - test_e2e.py::test_ptp_quickstart_advanced_eagle3[Llama-3.1-8b-Instruct-llama-3.1-model/Llama-3.1-8B-Instruct-EAGLE3-LLaMA3.1-Instruct-8B]
  - test_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-False-False]
  - unittest/_torch -k "not (modeling or multi_gpu or auto_deploy)"
  - unittest/_torch -k "modeling_llama"
  - unittest/_torch/modeling -k "modeling_mixtral"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and tp1 and nextn0"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and tp1 and not nextn0"
  - unittest/_torch/auto_deploy/unit/singlegpu
  - unittest/_torch/speculative/test_eagle3.py
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*b100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: pre_merge
      backend: tensorrt
  tests:
  # ------------- TRT tests ---------------
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_nvfp4
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_nvfp4_gemm_plugin[disable_norm_quant_fusion-disable_fused_quant]
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_nvfp4_gemm_plugin[disable_norm_quant_fusion-enable_fused_quant]
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_nvfp4_gemm_plugin[enable_norm_quant_fusion-disable_fused_quant]
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_nvfp4_gemm_plugin[enable_norm_quant_fusion-enable_fused_quant]
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_auto_dtype
  - accuracy/test_cli_flow.py::TestLlama3_8BInstruct::test_fp8
  - accuracy/test_cli_flow.py::TestMixtral8x7B::test_nvfp4_prequantized
  - unittest/trt/attention/test_gpt_attention.py -k "trtllm_gen"
  - unittest/llmapi/test_llm_quant.py # 3.5 mins on B200
  - unittest/trt/functional/test_fp4_gemm.py # 3 mins on B200
