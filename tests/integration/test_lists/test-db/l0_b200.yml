version: 0.0.1
l0_b200:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*b100*'
      linux_distribution_name: ubuntu*
  tests:
  # ------------- PyTorch tests ---------------
  - test_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-NVFP4-nvfp4-quantized/Meta-Llama-3.1-8B]
  - test_e2e.py::test_ptp_quickstart_advanced[Llama3.1-8B-FP8-llama-3.1-model/Llama-3.1-8B-Instruct-FP8]
  - test_e2e.py::test_ptq_quickstart_advanced_mtp[DeepSeek-V3-Lite-BF16-DeepSeek-V3-Lite/bf16]
  - test_e2e.py::test_ptp_quickstart_advanced_mixed_precision
  - examples/test_pytorch.py::test_llm_llama_1gpu[llama-3.1-8b-enable_fp4]
  - test_e2e.py::test_trtllm_bench_pytorch_backend_sanity[meta-llama/Llama-3.1-8B-llama-3.1-8b-False-False]
  - unittest/_torch -k "not (modeling or multi_gpu or auto_deploy)"
  - unittest/_torch -k "modeling_llama"
  - unittest/_torch/modeling -k "modeling_mixtral"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and tp1 and nextn0"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and tp1 and not nextn0"
  - unittest/_torch/auto_deploy/unit/singlegpu
  - examples/test_pytorch.py::test_llm_deepseek_1gpu[deepseek-v3-lite-disable_fp8-enable_fp4]
  - examples/test_pytorch.py::test_llm_deepseek_1gpu[deepseek-v3-lite-disable_fp8-disable_fp4]
  # ------------- TRT tests ---------------
  - examples/test_llama.py::test_llm_llama_1gpu_fp4_model_config[llama-v3-8b-instruct-hf-fp4_plugin] # 4 mins
  - examples/test_llama.py::test_llm_llama_1gpu_fp4[llama-v3-8b-instruct-hf-enable_norm_quant_fusion-disable_fused_quant-fp4_plugin-float16]
  - examples/test_llama.py::test_llm_llama_1gpu_fp4[llama-v3-8b-instruct-hf-enable_norm_quant_fusion-enable_fused_quant-fp4_plugin-float16]
  - examples/test_llama.py::test_llm_llama_1gpu_fp4[llama-v3-8b-instruct-hf-disable_norm_quant_fusion-disable_fused_quant-fp4_plugin-float16]
  - examples/test_llama.py::test_llm_llama_1gpu_fp4[llama-v3-8b-instruct-hf-disable_norm_quant_fusion-enable_fused_quant-fp4_plugin-float16]
  - examples/test_llama.py::test_llm_llama_1gpu_fp4[llama-v3-8b-instruct-hf-disable_norm_quant_fusion-disable_fused_quant-fp4_ootb-float16]
  - accuracy/test_accuracy.py::TestLlama3_8BInstruct::test_auto_dtype
  - accuracy/test_accuracy.py::TestLlama3_8BInstruct::test_fp8
  - unittest/attention/test_gpt_attention.py -k "trtllm_gen"
  - unittest/llmapi/test_llm_quant.py # 3.5 mins on B200
  - examples/test_mixtral.py::test_llm_mixtral_1gpu_fp4_llmapi[Mixtral-8x7B-Instruct-v0.1] # 9 mins on B200
  - unittest/functional/test_fp4_gemm.py # 3 mins on B200
