version: 0.0.1
l0_dgx_h100:
- condition:
    ranges:
      system_gpu_count:
        gte: 4
        lte: 4
    wildcards:
      gpu:
      - '*h100*'
      linux_distribution_name: ubuntu*
  tests:
  # ------------- PyTorch tests ---------------
  - unittest/_torch/multi_gpu
  - unittest/_torch/multi_gpu_modeling -k "deepseek and not (tp1 and pp1) and nextn0"
  - unittest/_torch/multi_gpu_modeling -k "deepseek and not (tp1 and pp1) and not nextn0"
  - unittest/_torch/multi_gpu_modeling -k "llama and not (tp1 and pp1)"
  - unittest/_torch/auto_deploy/unit/multigpu
  - disaggregated/test_disaggregated.py::test_disaggregated_multi_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_cuda_graph[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_mixed[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_overlap[TinyLlama-1.1B-Chat-v1.0]
  - disaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8[DeepSeek-V3-Lite-fp8]
  - disaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp[DeepSeek-V3-Lite-fp8]
  - disaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp_one[DeepSeek-V3-Lite-fp8]
  - disaggregated/test_disaggregated.py::test_disaggregated_deepseek_v3_lite_fp8_attention_dp_one_mtp[DeepSeek-V3-Lite-fp8]
  - disaggregated/test_disaggregated.py::test_disaggregated_overlap_dp[DeepSeek-V3-Lite-fp8]
  # ------------- CPP tests ---------------
  - test_cpp.py::test_multi_gpu_simple[90]
  - test_cpp.py::test_multi_gpu_t5[90]
  - test_cpp.py::test_multi_gpu_llama_executor[90]
  - test_cpp.py::test_multi_gpu_trt_gpt_real_decoder[90]
  - test_cpp.py::test_multi_gpu_disagg[90]
  # ------------- TRT tests ---------------
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_fp8_tp2[disable_reduce_fusion-disable_fp8_context_fmha]
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_fp8_tp2[enable_reduce_fusion-enable_fp8_context_fmha]
  - accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_pp4
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_fp8_2gpus[cp2]
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_tp2cp2
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_fp8_2gpus[pp2] # 2 mins
  - accuracy/test_cli_flow.py::TestLlama3_1_8B::test_fp8_rowwise_tp4[enable_gemm_allreduce_plugin]
  - accuracy/test_cli_flow.py::TestMixtral8x7B::test_fp8_tp2pp2
  - accuracy/test_cli_flow.py::TestMixtral8x7B::test_fp8_tp2pp2_manage_weights
  - accuracy/test_cli_flow.py::TestQwen2_57B_A14B::test_tp4
  - examples/test_llama.py::test_llm_llama_long_alpaca_8gpu_summary[pg64317-tp4pp2-nb:4]
  - examples/test_llama.py::test_llm_llama_v2_lora_benchmark_2gpu[chinese_lora-llama-v2-13b-hf]
  - examples/test_mixtral.py::test_llm_mixtral_moe_plugin_lora_4gpus[Mixtral-8x7B-v0.1-chinese-mixtral-lora]
  - unittest/llmapi/test_llm_multi_gpu.py -m "gpu2 and part0"
  - unittest/llmapi/test_llm_multi_gpu.py -m "gpu2 and part1"
  - unittest/llmapi/test_llm_multi_gpu.py -m "gpu2 and part2"
  - unittest/llmapi/test_llm_multi_gpu.py -m "gpu2 and part3"
  - unittest/llmapi/test_llm_multi_gpu.py -m "gpu4 and part0"
  - unittest/llmapi/test_llm_multi_gpu.py -m "not (gpu2 or gpu4)"
  - unittest/llmapi/test_llm_models_multi_gpu.py
  - examples/test_enc_dec.py::test_llm_enc_dec_general[compare_hf-t5-small-float32-enable_gemm_plugin-enable_attention_plugin-enable_paged_kv_cache-tp:2-pp:2-nb:1-enable_fp8]
  - test_e2e.py::test_llmapi_exit_multi_gpu
  - examples/test_nemotron_nas.py::test_nemotron_nas_summary_2gpu[DeciLM-7B]
  - test_e2e.py::test_llmapi_example_distributed_tp2
  - unittest/trt/functional/test_allreduce_norm.py
  - examples/test_multimodal.py::test_llm_multimodal_general[Llama-3.2-11B-Vision-pp:1-tp:2-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - deterministic/test_mixtral_deterministic.py::test_llm_mixtral_4gpus_deterministic[Mixtral-8x7B-Instruct-v0.1-float16]
- condition:
    ranges:
      system_gpu_count:
        gte: 4
        lte: 4
    wildcards:
      gpu:
      - '*h100*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
  tests:
  # ------------- PyTorch tests ---------------
  - unittest/_torch/auto_deploy/integration/test_ad_build.py
  - unittest/_torch/auto_deploy/integration/test_lm_eval.py
  # ------------- TRT tests ---------------
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-use_attention_plugin-enable_context_fmha-tp:2-pp:1-float16-BertForSequenceClassification-bert/bert-base-uncased-yelp-polarity]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-use_attention_plugin-enable_context_fmha-tp:2-pp:1-float16-RobertaForQuestionAnswering-bert/roberta-base-squad2]
  - examples/test_bert.py::test_llm_bert_general[compare_hf-enable_remove_input_padding-disable_attention_plugin-disable_context_fmha-tp:2-pp:1-float16-RobertaForSequenceClassification-bert/twitter-roberta-base-emotion]
  - examples/test_recurrentgemma.py::test_llm_recurrentgemma_2gpu[recurrentgemma-2b]
  - examples/test_mamba.py::test_llm_mamba2_2gpu[mamba-codestral-7B-v0.1]
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_smooth_quant_ootb_tp2
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_int4_awq_tp2
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_int4_awq_prequantized_tp2
  - accuracy/test_cli_flow.py::TestLlama2_7B::test_int4_gptq_prequantized_tp2
  - accuracy/test_cli_flow.py::TestLlama3_1_8B::test_tp4[disable_gemm_allreduce_plugin]
  - accuracy/test_cli_flow.py::TestLlama3_1_8B::test_tp4[enable_gemm_allreduce_plugin]
  - accuracy/test_cli_flow.py::TestLlama3_1_8B::test_fp8_rowwise_tp4[disable_gemm_allreduce_plugin]
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_fp8_tp2[disable_reduce_fusion-enable_fp8_context_fmha]
  - accuracy/test_cli_flow.py::TestLlama3_2_1B::test_fp8_tp2[enable_reduce_fusion-disable_fp8_context_fmha]
  - accuracy/test_cli_flow.py::TestPhi2::test_tp2
  - test_e2e.py::test_llmapi_quant_llama_70b
  - test_e2e.py::test_llmapi_example_distributed_autopp_tp2
  - examples/test_enc_dec.py::test_llm_enc_dec_general[compare_hf-t5-small-float16-enable_gemm_plugin-enable_attention_plugin-disable_paged_kv_cache-tp:2-pp:2-nb:1-disable_fp8]
  - examples/test_gpt.py::test_llm_gpt2_next_prompt_tuning[use_py_session-tp2]
  - unittest/llmapi/apps/_test_openai_multi_gpu.py -m "part0"
