server_configs:
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_1024_conc4_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 2048
  max_seq_len: 2068
  client_configs:
  - name: con4_isl1024_osl1024
    concurrency: 4
    iterations: 10
    isl: 1024
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_1024_conc32_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 2048
  max_seq_len: 2068
  client_configs:
  - name: con32_isl1024_osl1024
    concurrency: 32
    iterations: 10
    isl: 1024
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_1024_conc256_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 256
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 4096
  max_seq_len: 2068
  client_configs:
  - name: con256_isl1024_osl1024
    concurrency: 256
    iterations: 10
    isl: 1024
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_8192_conc4_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 2048
  max_seq_len: 9284
  client_configs:
  - name: con4_isl1024_osl8192
    concurrency: 4
    iterations: 10
    isl: 1024
    osl: 8192
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_8192_conc16_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 2048
  max_seq_len: 9284
  client_configs:
  - name: con16_isl1024_osl8192
    concurrency: 16
    iterations: 10
    isl: 1024
    osl: 8192
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_1024_8192_conc128_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 256
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 4096
  max_seq_len: 9284
  client_configs:
  - name: con128_isl1024_osl8192
    concurrency: 128
    iterations: 10
    isl: 1024
    osl: 8192
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_8192_1024_conc4_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 8192
  max_seq_len: 9284
  client_configs:
  - name: con4_isl8192_osl1024
    concurrency: 4
    iterations: 10
    isl: 8192
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_8192_1024_conc16_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 8192
  max_seq_len: 9284
  client_configs:
  - name: con16_isl8192_osl1024
    concurrency: 16
    iterations: 10
    isl: 8192
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
- name: warnold_nv_glm_4_7_modelopt_fp8_8192_1024_conc128_gpu8
  model_name: glm_4_7_fp8
  gpus: 8
  match_mode: scenario
  attention_dp_config:
    batching_wait_iters: 0
    enable_balance: true
    timeout_iters: 60
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 256
  enable_attention_dp: false
  kv_cache_config:
    dtype: fp8
    free_gpu_memory_fraction: 0.8
    enable_block_reuse: false
  moe_config:
    backend: TRTLLM
  print_iter_log: true
  tensor_parallel_size: 8
  moe_expert_parallel_size: 8
  trust_remote_code: true
  backend: pytorch
  max_num_tokens: 8192
  max_seq_len: 9284
  client_configs:
  - name: con128_isl8192_osl1024
    concurrency: 128
    iterations: 10
    isl: 8192
    osl: 1024
    random_range_ratio: 0.0
    backend: openai
    streaming: true
