version: 0.0.1
l0_sanity_check:
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      wildcards:
        gpu:
          - '*b100*'
          - '*h100*'
          - '*h200*'
          - '*l40s*'
          - '*a10*'
        linux_distribution_name: ubuntu*
    tests:
      - test_e2e.py::test_llmapi_quickstart
      - test_e2e.py::test_llmapi_example_inference
      - test_e2e.py::test_llmapi_example_inference_async
      - test_e2e.py::test_llmapi_example_inference_async_streaming
      - test_e2e.py::test_llmapi_example_logits_processor
      - test_e2e.py::test_llmapi_example_multilora
      - test_e2e.py::test_llmapi_example_guided_decoding
      - test_e2e.py::test_llmapi_example_customize
      - test_e2e.py::test_llmapi_example_quantization
      - examples/test_llm_api_with_mpi.py::test_llm_api_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      # "No available XQA kernels are found for speculative decoding mode" on B100
      wildcards:
        gpu:
          - '*h100*'
          - '*h200*'
          - '*l40s*'
          - '*a10*'
        linux_distribution_name: ubuntu*
    tests:
      - test_e2e.py::test_llmapi_example_lookahead_decoding
  - condition:
      ranges:
        system_gpu_count:
          gte: 1
          lte: 1
      # Need FP8 support
      # "No available XQA kernels are found for speculative decoding mode" on B100
      wildcards:
        gpu:
          - '*h100*'
          - '*h200*'
          - '*l40s*'
        linux_distribution_name: ubuntu*
    tests:
      - test_e2e.py::test_llmapi_example_medusa_decoding_use_modelopt
