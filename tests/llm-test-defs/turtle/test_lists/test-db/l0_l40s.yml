version: 0.0.1
l0_l40s:
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
  tests:
  # ------------- PyTorch tests ---------------
  - test_unittests.py::test_unittests[_torch]
  - test_unittests.py::test_unittests[_torch-modeling-mllama]
  - test_unittests.py::test_unittests[_torch-modeling-nvsmall]
  - test_unittests.py::test_unittests[_torch-modeling-out-of-tree]
  - test_unittests.py::test_unittests[_torch-modeling-qwen]
  - test_unittests.py::test_unittests[_torch-modeling-vila]
  - test_unittests.py::test_unittests[_torch-auto-deploy-unit-singlegpu]
  - test_e2e.py::test_ptp_scaffolding[DeepSeek-R1-Distill-Qwen-7B-DeepSeek-R1/DeepSeek-R1-Distill-Qwen-7B]
  - test_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-image]
  - test_e2e.py::test_ptp_quickstart_multimodal[NVILA-8B-FP16-vila/NVILA-8B-video]
  - test_e2e.py::test_ptp_quickstart_multimodal[llava-v1.6-mistral-7b-llava-v1.6-mistral-7b-hf-image]
  - test_e2e.py::test_ptp_quickstart_bert[BertForSequenceClassification-bert/bert-base-uncased-yelp-polarity]
  # ------------- TRT tests ---------------
  - test_unittests.py::test_unittests[attention-gpt-partition0]
  - test_unittests.py::test_unittests[attention-gpt-partition1]
  - test_unittests.py::test_unittests[attention-gpt-partition2]
  - test_unittests.py::test_unittests[attention-gpt-partition3]
  - test_unittests.py::test_unittests[attention-gpt-xqa-generic]
  - test_unittests.py::test_unittests[quantization] # 18 mins
  - test_unittests.py::test_unittests[functional] # 37 mins
  - examples/test_llama.py::test_llm_llama_v2_lora_1gpu[chinese-llama-2-lora-13b-llama-v2-13b-hf-lora_fp16-base_fp16]
  - examples/test_llama.py::test_llm_llama_v3_dora_1gpu[commonsense-llama-v3-8b-dora-r32-llama-v3-8b-hf-base_fp16]
  - examples/test_llama.py::test_llm_llama_v1_1gpu_kv_cache_reuse_with_prompt_table[llama-7b]
  - examples/test_llama.py::test_llm_llama_v3_1_1node_single_gpu[llama-3.1-8b-enable_fp8]
  - examples/test_llama.py::test_llm_llama_v3_1_1node_single_gpu[llama-3.1-8b-enable_fp8_meta_recipe]
  - examples/test_llama.py::test_llm_llama_v2_1gpu_fp8_summary_and_mmlu[llama-v2-7b-hf-enable_fp8_paged_fmha-disable_mmlu_test]
  - examples/test_phi.py::test_llm_phi_single_gpu_summary[Phi-3-small-8k-instruct-bfloat16-enable_gemm_plugin-enable_attention_plugin-enable_fmha_with_fp32_acc-nb:1]
  - examples/test_phi.py::test_llm_phi_single_gpu_summary[Phi-3-small-128k-instruct-bfloat16-enable_gemm_plugin-enable_attention_plugin-enable_fmha_with_fp32_acc-nb:1]
  - examples/test_phi.py::test_llm_phi_lora_1gpu[Phi-3-mini-4k-instruct-ru-lora-Phi-3-mini-4k-instruct-lora_fp16-base_fp16]
  - examples/test_qwen.py::test_llm_qwen1_5_7b_single_gpu_lora[qwen1.5_7b_chat-Qwen1.5-7B-Chat-750Mb-lora]
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2-9b-it-other-bfloat16-8]
  - examples/test_nemotron_nas.py::test_nemotron_nas_summary_1gpu[DeciLM-7B]
  - examples/test_gpt.py::test_llm_gpt_starcoder_lora_1gpu[peft-lora-starcoder2-15b-unity-copilot-starcoder2-lora_fp16-base_fp16]
  - test_e2e.py::test_llmapi_quickstart
  - test_e2e.py::test_llmapi_example_inference
  - test_e2e.py::test_llmapi_example_inference_async
  - test_e2e.py::test_llmapi_example_inference_async_streaming
  - test_e2e.py::test_llmapi_example_logits_processor
  - test_e2e.py::test_llmapi_example_multilora
  - test_e2e.py::test_llmapi_example_guided_decoding
  - test_e2e.py::test_llmapi_example_customize
  - test_e2e.py::test_llmapi_example_quantization
  - examples/test_llm_api_with_mpi.py::test_llm_api_single_gpu_with_mpirun[TinyLlama-1.1B-Chat-v1.0]
  - test_e2e.py::test_llmapi_example_lookahead_decoding
  - test_e2e.py::test_llmapi_example_medusa_decoding_use_modelopt
- condition:
    ranges:
      system_gpu_count:
        gte: 1
        lte: 1
    wildcards:
      gpu:
      - '*l40s*'
      linux_distribution_name: ubuntu*
    terms:
      stage: post_merge
  tests:
  - examples/test_phi.py::test_llm_phi_single_gpu_summary[phi-2-bfloat16-enable_gemm_plugin-enable_attention_plugin-enable_fmha_with_fp32_acc-nb:1]
  - examples/test_qwen.py::test_llm_qwen_moe_single_gpu_summary[qwen1.5_moe_a2.7b_chat-enable_paged_kv_cache-enable_remove_input_padding-enable_weight_only-enable_fmha]
  - examples/test_gpt.py::test_llm_gpt2_next_prompt_tuning[use_cpp_session-tp1] # 10 mins
  - examples/test_gpt.py::test_llm_gpt2_next_prompt_tuning[use_py_session-tp1]
  - examples/test_multimodal.py::test_llm_multimodal_general[cogvlm-chat-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[neva-22b-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[video-neva-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:False-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[cogvlm-chat-pp:1-tp:1-bfloat16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_multimodal.py::test_llm_multimodal_general[kosmos-2-pp:1-tp:1-float16-bs:1-cpp_e2e:True-nb:1]
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-it-flax-wo_int8-float16-8] # 8 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-it-flax-wo_int4-bfloat16-8] # 3 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-keras-int8_kv_cache-bfloat16-8] # 7 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-smooth_quant-float16-8] # 9 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-torch-other-bfloat16-8] # 3 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-keras-int8_kv_cache-bfloat16-8] # 19 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-torch-int8_kv_cache-bfloat16-8] # SKIP
  - examples/test_nemotron.py::test_llm_nemotron_3_8b_1gpu[bfloat16-fp8] # 18mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-it-flax-wo_int4-bfloat16-8] # 6 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-smooth_quant-float16-8] # 25 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-torch-other-bfloat16-8] # 6 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-it-flax-smooth_quant-float16-8] # SKIP
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-it-flax-smooth_quant-float16-8] # 10 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_mmlu[gemma-2b-it-flax-other-bfloat16-8] # 2 mins
  - examples/test_gpt.py::test_llm_gpt2_1gpu[enable_gemm_plugin-disable_attention_plugin] # 2.5 mins
  - examples/test_gpt.py::test_llm_gpt2_starcoder2[bfloat16-tp1] # 9 mins
  - examples/test_chatglm.py::test_llm_chatglm2_6b_single_gpu_summary[chatglm2-6b-disable_paged_kv_cache-disable_remove_input_padding-disable_weight_only-enable_fmha-nb:2] # 6 mins
  - examples/test_whisper.py::test_llm_whisper_general[large-v3-disable_gemm_plugin-disable_attention_plugin-disable_weight_only-float16-nb:1-use_python_runtime] # 8 mins
  - test_unittests.py::test_unittests[attention-bert] # 12 mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-2b-int4_awq-float16-8] # 4mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-2b-int8_sq-float16-8]
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-it-flax-int8_kv_cache-bfloat16-8] # 5mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-2b-wo_int8-bfloat16-8] # 4mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-keras-wo_int8-float16-8] # 9mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-7b-int4_awq-bfloat16-8] # 5mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-7b-int4_awq-float16-8] # 5mins
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_py_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_medusa.py::test_llm_medusa_1gpu[use_cpp_session-medusa-vicuna-7b-v1.3-4-heads-bfloat16-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_cpp_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb5-bs8]
  - examples/test_redrafter.py::test_llm_redrafter_1gpu[use_py_session-redrafter-vicuna-7b-v1.3-bfloat16-dl5-nb8-bs8]
  - test_accuracy.py::test_accuracy_bloom[bloom-context-fmha-enabled] # 6 mins
  - test_accuracy.py::test_accuracy_bloom[bloom-mmha-multi-block-mode] # 6 mins
  - test_accuracy.py::test_accuracy_bloom[bloom-context-fmha-disabled] # 6 mins
  - examples/test_chatglm.py::test_llm_chatglm_6b_single_gpu_run # 4 mins
  - examples/test_chatglm.py::test_llm_chatglm_6b_single_gpu_summary[disable_paged_kv_cache-enable_remove_input_padding-disable_weight_only-nb:2]
  - examples/test_chatglm.py::test_llm_glm_10b_single_gpu_run[enable_weight_only] # 12 mins
  - test_unittests.py::test_unittests[model-bloom] # 6 mins
  - test_unittests.py::test_unittests[model-gptneox] # 5 mins
  - examples/test_gemma.py::test_llm_gemma_1gpu_summary[gemma-7b-keras-fp8_kv_cache-float16-8]
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-2b-fp8-bfloat16-8] #4mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-2b-fp8-float16-8] #3mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-7b-fp8-bfloat16-8] # 4mins
  - examples/test_gemma.py::test_llm_hf_gemma_quantization_1gpu[gemma-7b-fp8-float16-8] # 4mins
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-2b-instruct]
  - examples/test_granite.py::test_granite_bf16_lora[granite-3.0-1b-a400m-instruct]
  - examples/test_llama.py::test_llama_3_x_fp8_with_bf16_lora[llama-v3-8b-instruct-hf]
  - test_unittests.py::test_unittests[model-gptj] # 4 mins
