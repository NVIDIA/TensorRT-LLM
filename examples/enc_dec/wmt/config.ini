[encoder]
vocab_size = 44512
d_model = 1024
d_kv = 64
d_ff = 4096
num_layers = 6
num_decoder_layers = 6
num_heads = 16
relative_attention_num_buckets = 32
relative_attention_max_distance = 128
dropout_rate = 0.1
classifier_dropout = 0.0
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = relu
use_cache = False
dense_act_fn = relu
is_gated_act = False
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = False
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
suppress_tokens = None
begin_suppress_tokens = None
architectures = ['T5ForConditionalGeneration']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 2
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}
problem_type = None
_name_or_path = mt/models/wmt
transformers_version = 4.33.1
model_type = wmt
n_positions = 1024
output_past = True
weight_data_type = float16
q_scaling = 0.125
# mt specific
relative_attention= False
residual_scaling = 1.0
has_position_embedding = True
has_embedding_scale = True
has_attention_qkvo_bias = True
has_mlp_bias = True
has_model_final_layernorm = False
layernorm_position = post_layernorm
layernorm_type = LayerNorm

[decoder]
vocab_size = 44512
d_model = 1024
d_kv = 64
d_ff = 4096
num_layers = 6
num_decoder_layers = 6
num_heads = 16
relative_attention_num_buckets = 32
relative_attention_max_distance = 128
dropout_rate = 0.1
classifier_dropout = 0.0
layer_norm_epsilon = 1e-06
initializer_factor = 1.0
feed_forward_proj = relu
use_cache = True
dense_act_fn = relu
is_gated_act = False
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = True
is_encoder_decoder = False
is_decoder = True
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
suppress_tokens = None
begin_suppress_tokens = None
architectures = ['T5ForConditionalGeneration']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = 0
eos_token_id = 1
sep_token_id = None
decoder_start_token_id = 0
task_specific_params = {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}
problem_type = None
_name_or_path = mt/models/mt-teacher-small
transformers_version = 4.33.1
model_type = wmt
n_positions = 1024
output_past = True
weight_data_type = float16
q_scaling = 0.125
# mt specific
relative_attention= False
residual_scaling = 1.0
has_position_embedding = True
has_embedding_scale = True
has_attention_qkvo_bias = True
has_mlp_bias = True
has_model_final_layernorm = False
layernorm_position = post_layernorm
layernorm_type = LayerNorm

[structure]
t5_with_bias = false
use_gated_activation = False
position_embedding_type = relative
# keep it to be t5 to use parse_config_t5
model_type = t5