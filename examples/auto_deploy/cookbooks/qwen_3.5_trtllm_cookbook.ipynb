{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying Qwen3.5-397B-A17B with TensorRT-LLM\n",
        "\n",
        "This notebook walks you through deploying the `Qwen/Qwen3.5-397B-A17B` model using TensorRT-LLM.\n",
        "\n",
        "[TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/) is NVIDIA's open-source library for accelerating and optimizing LLM inference on NVIDIA GPUs. Support for Qwen3.5-397B-A17B is enabled through the AutoDeploy workflow. More details about AutoDeploy can be found [here](https://nvidia.github.io/TensorRT-LLM/torch/auto_deploy/auto-deploy.html).\n",
        "\n",
        "**Model Resources:**\n",
        "- [HuggingFace Model Card](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)\n",
        "- [Qwen3.5 GitHub](https://github.com/QwenLM/Qwen3.5)\n",
        "- [Qwen Blog](https://qwenlm.github.io/blog/qwen3.5/)\n",
        "\n",
        "**Model Highlights:**\n",
        "- 397B-A17B Mixture of Experts (MoE) with Gated Delta Networks\n",
        "- 262,144 token context length\n",
        "- Reasoning and tool calling support\n",
        "- 201 languages and dialects\n",
        "- Apache-2.0 License\n",
        "\n",
        "**Prerequisites:**\n",
        "- 8x NVIDIA B200 GPUs (or equivalent VRAM for BF16) with recent drivers and CUDA 12.x\n",
        "- Python 3.10+\n",
        "- TensorRT-LLM ([container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release) or pip install)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites & Environment\n",
        "\n",
        "Set up a containerized environment for TensorRT-LLM by running the following command in a terminal:\n",
        "\n",
        "```shell\n",
        "docker run --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all -p 8000:8000 nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc1\n",
        "```\n",
        "\n",
        "You now have TensorRT-LLM set up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If pip not found\n",
        "!python -m ensurepip --default-pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify GPU\n",
        "\n",
        "Check that CUDA is available and the GPU is detected correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment check\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI-Compatible Server\n",
        "\n",
        "Start a local OpenAI-compatible server with TensorRT-LLM via the terminal, within the running docker container.\n",
        "\n",
        "Ensure that the following commands are executed from the docker terminal.\n",
        "\n",
        "Start with the Qwen 3.5 YAML here: `examples/auto_deploy/model_registry/configs/qwen3.5_moe_400b.yaml`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the BF16 Model\n",
        "\n",
        "Launch the TensorRT-LLM server with Qwen3.5-397B-A17B:\n",
        "\n",
        "```shell\n",
        "trtllm-serve \"Qwen/Qwen3.5-397B-A17B\" \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  --backend _autodeploy \\\n",
        "  --trust_remote_code \\\n",
        "  --extra_llm_api_options examples/auto_deploy/model_registry/configs/qwen3.5_moe_400b.yaml\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the NVFP4 Model\n",
        "\n",
        "Launch the TensorRT-LLM server with nvidia/Qwen3.5-397B-A17B-NVFP4:\n",
        "\n",
        "Note:\n",
        "nvidia/Qwen3.5-397B-A17B-NVFP4 can run on 4 x B200. You can update `world_size` in examples/auto_deploy/model_registry/configs/qwen3.5_moe_400b.yaml\n",
        "```\n",
        "world_size: 4\n",
        "```\n",
        "\n",
        "Command:\n",
        "\n",
        "```shell\n",
        "trtllm-serve \"nvidia/Qwen3.5-397B-A17B-NVFP4\" \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  --backend _autodeploy \\\n",
        "  --trust_remote_code \\\n",
        "  --extra_llm_api_options examples/auto_deploy/model_registry/configs/qwen3.5_moe_400b.yaml\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your server is now running!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the API\n",
        "\n",
        "Use the OpenAI-compatible client to send requests to the TensorRT-LLM server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Setup client\n",
        "BASE_URL = \"http://0.0.0.0:8000/v1\"\n",
        "API_KEY = \"null\"\n",
        "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3.5-397B-A17B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat Completion Example\n",
            "==================================================\n",
            "Response:\n",
            "To find 15% of 85, I need to multiply 85 by 0.15 (since 15% = 15/100 = 0.15).\n",
            "\n",
            "Let me calculate this:\n",
            "\n",
            "85 × 0.15\n",
            "\n",
            "I can break this down:\n",
            "85 × 0.15 = 85 × (15/100) = (85 × 15) / 100\n",
            "\n",
            "85 × 15:\n",
            "85 × 10 = 850\n",
            "85 × 5 = 425\n",
            "850 + 425 = 1,275\n",
            "\n",
            "So 85 × 15 = 1,275\n",
            "\n",
            "Now divide by 100:\n",
            "1,275 / 100 = 12.75\n",
            "\n",
            "Therefore, 15% of 85 = 12.75\n",
            "</think>\n",
            "\n",
            "To find 15% of 85, I need to multiply 85 by 0.15 (since 15% = 15/100 = 0.15).\n",
            "\n",
            "**Method 1: Direct multiplication**\n",
            "85 × 0.15 = 12.75\n",
            "\n",
            "**Method 2: Breaking it down**\n",
            "- 10% of 85 = 8.5\n",
            "- 5% of 85 = 4.25 (half of 10%)\n",
            "- 15% = 10% + 5% = 8.5 + 4.25 = 12.75\n",
            "\n",
            "**Method 3: Using fractions**\n",
            "15% of 85 = (15/100) × 85 = (85 × 15) / 100 = 1,275 / 100 = 12.75\n",
            "\n",
            "**Answer: 15% of 85 = 12.75**\n"
          ]
        }
      ],
      "source": [
        "# Basic chat completion\n",
        "print(\"Chat Completion Example\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is 15% of 85? Show your reasoning.\"},\n",
        "    ],\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming response:\n",
            "==================================================\n",
            "Thinking Process:\n",
            "\n",
            "1.  **Analyze the Request:** The user is asking for the first 5 prime numbers.\n",
            "\n",
            "2.  **Define \"Prime Number\":** A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\n",
            "\n",
            "3.  **Identify Prime Numbers in Order:**\n",
            "    *   1 is not prime (by definition).\n",
            "    *   2 is prime (divisible only by 1 and 2).\n",
            "    *   3 is prime (divisible only by 1 and 3).\n",
            "    *   4 is not prime (divisible by 1, 2, 4).\n",
            "    *   5 is prime (divisible only by 1 and 5).\n",
            "    *   6 is not prime (divisible by 1, 2, 3, 6).\n",
            "    *   7 is prime (divisible only by 1 and 7).\n",
            "    *   8 is not prime (divisible by 1, 2, 4, 8).\n",
            "    *   9 is not prime (divisible by 1, 3, 9).\n",
            "    *   11 is prime (divisible only by 1 and 11).\n",
            "\n",
            "4.  **List the First 5:**\n",
            "    1.  2\n",
            "    2.  3\n",
            "    3.  5\n",
            "    4.  7\n",
            "    5.  11\n",
            "\n",
            "5.  **Format the Output:** Present the list clearly.\n",
            "\n",
            "6.  **Final Review:** Does the list match the definition? Yes. 2, 3, 5, 7, 11.\n",
            "\n",
            "7.  **Construct Final Response:** State the numbers clearly.cw\n",
            "</think>\n",
            "\n",
            "The first 5 prime numbers are:\n",
            "\n",
            "1.  2\n",
            "2.  3\n",
            "3.  5\n",
            "4.  7\n",
            "5.  11"
          ]
        }
      ],
      "source": [
        "# Streaming chat completion\n",
        "print(\"Streaming response:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"},\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Parameters\n",
        "\n",
        "For optimal results, use the following parameters based on your task:\n",
        "\n",
        "**Default Settings (Most Tasks)**\n",
        "- `temperature`: 1.0\n",
        "- `top_p`: 0.95\n",
        "- `max_tokens`: 262144\n",
        "\n",
        "**Agentic Tasks (SWE-bench, Terminal Bench)**\n",
        "- `temperature`: 0.7\n",
        "- `top_p`: 1.0\n",
        "- `max_tokens`: 16384\n",
        "\n",
        "**Deterministic Tasks**\n",
        "- `temperature`: 0\n",
        "- `max_tokens`: 16384"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [TensorRT-LLM Documentation](https://nvidia.github.io/TensorRT-LLM/)\n",
        "- [AutoDeploy Guide](https://nvidia.github.io/TensorRT-LLM/torch/auto_deploy/auto-deploy.html)\n",
        "- [Qwen3.5-397B-A17B on HuggingFace](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)\n",
        "- [Qwen3.5 GitHub](https://github.com/QwenLM/Qwen3.5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
