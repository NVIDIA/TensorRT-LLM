mode: graph
runtime: trtllm
trust_remote_code: true
compile_backend: torch-cudagraph
cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128]
max_batch_size: 128
model_factory: AutoModelForCausalLM
disable_overlap_scheduler: false
enable_chunked_prefill: true
max_num_tokens: 512 # NOTE: must be > max(tokens_per_block, max_batch_size)
kv_cache_config:
  enable_block_reuse: false
  free_gpu_memory_fraction: 0.88
model_kwargs:
  torch_dtype: bfloat16
