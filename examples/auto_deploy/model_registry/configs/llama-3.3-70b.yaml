# Configuration for Llama 3.3 70B
# For use with --yaml-extra (build_and_run_ad.py) or --extra_llm_api_options (trtllm-bench)

max_batch_size: 1024
max_num_tokens: 2048
kv_cache_free_gpu_memory_fraction: 0.9
tensor_parallel_size: 4
trust_remote_code: true
enable_attention_dp: false
cuda_graph_config:
  enable_padding: true
  max_batch_size: 1024
kv_cache_config:
  dtype: fp8
