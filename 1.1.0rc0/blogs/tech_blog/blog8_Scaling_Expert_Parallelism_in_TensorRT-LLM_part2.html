

<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization) &#8212; TensorRT-LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/nvidia-sphinx-theme.css?v=df3ac72c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=95073da6" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=65e89d2a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blogs/tech_blog/blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = './_static/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.1.0rc0';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.1.0rc0" />


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick-start-guide.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../key-features.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">PyTorch Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../installation/containers.html">Pre-built release container images on NGC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation/linux.html">Installing on Linux via <code class="docutils literal notranslate"><span class="pre">pip</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation/build-from-source-linux.html">Building from Source Code on Linux</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deployment Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-llama4-scout-on-trtllm.html">Quick Start Recipe for Llama4 Scout 17B on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-deepseek-r1-on-trtllm.html">Quick Start Recipe for DeepSeek R1 on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-llama3.3-70b-on-trtllm.html">Quick Start Recipe for Llama3.3 70B on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/index.html">LLM API Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/reference.html">API Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/index.html">LLM Examples Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/customization.html">LLM Common Customizations</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/llm_api_examples.html">LLM Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference.html">Generate text</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async.html">Generate text asynchronously</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async_streaming.html">Generate text in streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_distributed.html">Distributed LLM Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_guided_decoding.html">Generate text with guided decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_logits_processor.html">Control generated text using logits processor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_multilora.html">Generate text with multiple LoRA adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_runtime.html">Runtime Configuration Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_sampling.html">Sampling Techniques Showcase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_llm_distributed.html">Run LLM-API with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_bench.html">Run trtllm-bench with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_serve.html">Run trtllm-serve with pytorch backend on Slurm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/trtllm_serve_examples.html">Online Serving Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client.html">Curl Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client_for_multimodal.html">Curl Chat Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_completion_client.html">Curl Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/deepseek_r1_reasoning_parser.html">Deepseek R1 Reasoning Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/genai_perf_client.html">Genai Perf Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/genai_perf_client_for_multimodal.html">Genai Perf Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client.html">OpenAI Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client_for_multimodal.html">OpenAI Chat Client for Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client.html">OpenAI Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_for_lora.html">Openai Completion Client For Lora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_json_schema.html">OpenAI Completion Client with JSON Schema</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Model Definition API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.functional.html">Functionals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.plugin.html">Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.runtime.html">Runtime</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../_cpp_gen/executor.html">Executor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_cpp_gen/runtime.html">Runtime</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Command-Line Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-bench.html">trtllm-bench</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-build.html">trtllm-build</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../commands/trtllm-serve/index.html">trtllm-serve</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/trtllm-serve.html">trtllm-serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/run-benchmark-with-trtllm-serve.html">Run benchmarking with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../architecture/overview.html">TensorRT-LLM Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/core-concepts.html">Model Definition</a></li>



<li class="toctree-l1"><a class="reference internal" href="../../architecture/checkpoint.html">TensorRT-LLM Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/workflow.html">TensorRT-LLM Build Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/add-model.html">Adding a Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../advanced/gpt-attention.html">Multi-Head, Multi-Query, and Group-Query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/gpt-runtime.html">C++ GPT Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/executor.html">Executor API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/graph-rewriting.html">Graph Rewriting Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/lora.html">Run gpt-2b + LoRA using Executor / cpp runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/expert-parallelism.html">Expert Parallelism in TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/kv-cache-management.html">KV Cache Management: Pools, Blocks, and Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/kv-cache-reuse.html">KV cache reuse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/speculative-decoding.html">Speculative Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/disaggregated-service.html">Disaggregated-Service (Prototype)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../performance/performance-tuning-guide/index.html">Performance Tuning Guide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/benchmarking-default-performance.html">Benchmarking Default Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/useful-build-time-flags.html">Useful Build-Time Flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/tuning-max-batch-size-and-max-num-tokens.html">Tuning Max Batch Size and Max Num Tokens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/deciding-model-sharding-strategy.html">Deciding Model Sharding Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/fp8-quantization.html">FP8 Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/useful-runtime-flags.html">Useful Runtime Options</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-analysis.html">Performance Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../reference/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/precision.html">Numerical Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/memory.html">Memory Usage of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/ci-overview.html">Continuous Integration Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/dev-containers.html">Using Dev Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
<li class="toctree-l1"><a class="reference internal" href="../H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Falcon180B-H200.html">Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization-in-TRT-LLM.html">Speed up inference with SOTA quantization techniques in TRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../XQA-kernel.html">New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html">Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.html">DeepSeek R1 MTP Implementation and Optimization</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization)</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="scaling-expert-parallelism-in-tensorrt-llm-part-2-performance-status-and-optimization">
<h1>Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization)<a class="headerlink" href="#scaling-expert-parallelism-in-tensorrt-llm-part-2-performance-status-and-optimization" title="Link to this heading">#</a></h1>
<p>This blog post continues our previous work on <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md">Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)</a>, where we introduced the fundamental design and implementation of large-scale Expert Parallelism (EP) in TensorRT-LLM. Building upon that foundation, we have made significant performance improvements through various optimizations, achieving better throughput and latency for large-scale MoE models.</p>
<p><em>By NVIDIA TensorRT-LLM Team</em></p>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#scaling-expert-parallelism-in-tensorrt-llm-part-2-performance-status-and-optimization">Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization)</a></p>
<ul>
<li><p><a class="reference internal" href="#table-of-contents">Table of Contents</a></p></li>
<li><p><a class="reference internal" href="#optimization-highlights">Optimization Highlights</a></p>
<ul>
<li><p><a class="reference internal" href="#kernel-optimizations">Kernel Optimizations</a></p>
<ul>
<li><p><a class="reference internal" href="#moe-auxiliary-kernels">MoE Auxiliary Kernels</a></p></li>
<li><p><a class="reference internal" href="#communication-kernels">Communication Kernels</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#expert-parallelism-load-balancer-eplb">Expert Parallelism Load Balancer (EPLB)</a></p>
<ul>
<li><p><a class="reference internal" href="#attempts-at-online-eplb-implementation">Attempts at Online EPLB Implementation</a></p>
<ul>
<li><p><a class="reference internal" href="#1-initial-approach-for-weight-updating---cudamemcpyasync"><span class="xref myst">1. Initial Approach for Weight Updating - cudaMemcpyAsync</span></a></p></li>
<li><p><a class="reference internal" href="#2-avoiding-deadlock---multithreaded-cpu-copy-with-managed-memory"><span class="xref myst">2. Avoiding Deadlock - Multithreaded CPU Copy with Managed Memory</span></a></p></li>
<li><p><a class="reference internal" href="#3-numa-memory-to-prevent-page-migration"><span class="xref myst">3. NUMA Memory to Prevent Page Migration</span></a></p></li>
<li><p><a class="reference internal" href="#4-addressing-the-tlb-thrashing-issue"><span class="xref myst">4. Addressing the TLB Thrashing Issue</span></a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#multi-token-prediction-mtp">Multi-Token Prediction (MTP)</a></p></li>
<li><p><a class="reference internal" href="#host-overhead-optimization">Host Overhead Optimization</a></p>
<ul>
<li><p><a class="reference internal" href="#reduce-binding-and-inter-process-communication-overhead">Reduce Binding and Inter-Process Communication Overhead</a></p></li>
<li><p><a class="reference internal" href="#support-stream-interval">Support Stream Interval</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#end-to-end-performance">End-to-End Performance</a></p></li>
<li><p><a class="reference internal" href="#future-work">Future Work</a></p>
<ul>
<li><p><a class="reference internal" href="#further-performance-optimization">Further Performance Optimization</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#acknowledgements">Acknowledgements</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="optimization-highlights">
<h2>Optimization Highlights<a class="headerlink" href="#optimization-highlights" title="Link to this heading">#</a></h2>
<p>Following the introduction of the fundamental design and implementation of large-scale Expert Parallelism (EP) in TensorRT-LLM in our <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md">previous blog</a>, the TensorRT-LLM team has focused on optimizing the large EP implementation to improve performance.</p>
<p>At the kernel level, we analyzed kernel duration and optimized performance by either improving existing kernels or developing new kernels that perform better. At the system level, we refined and optimized the EPLB implementation (which also helps reduce kernel scalability issues), integrated additional features such as MTP, and optimized host overhead to prevent Python code from slowing down inference.</p>
<section id="kernel-optimizations">
<h3>Kernel Optimizations<a class="headerlink" href="#kernel-optimizations" title="Link to this heading">#</a></h3>
<p>Our initial kernel breakdown and analysis revealed several key observations about performance impacts when Expert Parallelism (EP) scales up:</p>
<ol class="arabic simple">
<li><p><strong>MoE GEMM duration decreases</strong> as EP size increases, which is expected behavior.</p></li>
<li><p><strong>Attention kernel performance</strong> remains unaffected by increased EP size, demonstrating good scalability.</p></li>
<li><p><strong>Communication and some MoE kernels</strong> do not scale well and require optimization.</p></li>
</ol>
<div align="center">
<figure>
  <img src="../media/tech_blog8_kernel_breakdown.png" width="1000">
</figure>
</div>
<p align="center"><sub><em>Figure 1: Kernel breakdown when scaling EP without EPLB.</em></sub></p>
<p>We have made improvements to the MoE auxiliary kernels, including <code class="docutils literal notranslate"><span class="pre">expandInputRowsKernel</span></code>, <code class="docutils literal notranslate"><span class="pre">doActivationKernel</span></code>, and <code class="docutils literal notranslate"><span class="pre">finalizeMoeRoutingKernel</span></code>, and to the communication kernels by replacing <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> with a newly developed <code class="docutils literal notranslate"><span class="pre">AllToAllPrepare</span></code> kernel. Additionally, since the <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code> and <code class="docutils literal notranslate"><span class="pre">AlltoAll</span></code> kernels do not scale well due to EP imbalance, we optimized the EPLB implementation to improve the scalability of those kernels.</p>
<section id="moe-auxiliary-kernels">
<h4>MoE Auxiliary Kernels<a class="headerlink" href="#moe-auxiliary-kernels" title="Link to this heading">#</a></h4>
<p>We observed that given a fixed per-GPU batch size, <code class="docutils literal notranslate"><span class="pre">expandInputRowsKernel</span></code>, <code class="docutils literal notranslate"><span class="pre">doActivationKernel</span></code>, and <code class="docutils literal notranslate"><span class="pre">finalizeMoeRoutingKernel</span></code> showed increased execution time with larger EP size. However, their workload should remain constant regardless of EP size.</p>
<p>Before MoE group GEMMs, <code class="docutils literal notranslate"><span class="pre">M</span></code> tokens are expanded to <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">*</span> <span class="pre">topK</span></code> tokens, which are routed to experts hosted on different ranks. Hence, on average only <code class="docutils literal notranslate"><span class="pre">M</span> <span class="pre">*</span> <span class="pre">topK</span> <span class="pre">/</span> <span class="pre">EP</span></code> expanded tokens are valid on each rank (those routed to experts hosted on that rank). The original kernels launch a thread block for each expanded token. Each thread block detects if the token is valid; if so, it proceeds with the computation; otherwise, the thread block exits. For a large EP size, the valid tokens are sparse (<code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">EP</span></code>), so most thread blocks are launched for invalid tokens and do nothing, which is wasteful.</p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_moe_aux_kernels1.png" width="400">
</figure>
</div>
<p align="center"><sub><em>Figure 2: Sparsity of valid expanded tokens. For DeepSeek-R1 deployed with EP 32, a batch of 12 tokens are expanded to 96 tokens, but only 3 are valid on rank 0.</em></sub></p>
<p>Therefore, we modified the kernels so that thread blocks are launched for valid tokens only. This addressed the scalability issue.</p>
<p>Note that the number of valid tokens is data-dependent. To guarantee CUDA graph compatibility, we cannot rely on any data-dependent information on the host. Thus, we further modified the kernels to use persistent thread blocks, which control the loop based on the valid token number on the device.</p>
<p>This optimization was implemented in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5215">PR 5215</a>, with the following performance improvement:</p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_moe_aux_kernels2.png">
</figure>
</div>
<p align="center"><sub><em>Figure 3: Optimization effect on MoE auxiliary kernels. (Left) Before optimization, kernel time increases with EP size. (Right) After optimization, kernel time remains constant with EP size.</em></sub></p>
</section>
<section id="communication-kernels">
<h4>Communication Kernels<a class="headerlink" href="#communication-kernels" title="Link to this heading">#</a></h4>
<p>As introduced in our <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md#ep-communication-kernels-implementation">previous blog</a>, we developed EP communication kernels to transfer hidden state tensors of MoE. In the original design, each rank needs to determine which tokens it needs to send and receive, along with the expert IDs and scaling factors selected by those tokens. We initially used <code class="docutils literal notranslate"><span class="pre">allgather</span></code> to collect expert IDs and scaling factors, then each rank calculated the required metadata. However, we found that although the transmission size of this data is not large, the performance of <code class="docutils literal notranslate"><span class="pre">allgather</span></code> is unsatisfactory and may become a performance bottleneck when EP size increases. Therefore, we developed new communication kernels to optimize this process.</p>
<p>First, a kernel counts the number of tokens needed to be transferred to another rank and transfers the count to that rank. Then each rank can calculate the index information for subsequent alltoall kernels. Finally, an alltoall kernel transfers expert IDs and scaling factors. These kernels make EP more scalable because the communication size no longer increases with EP size. The implementation of the communication part of these kernels is similar to the previous communication kernel of hidden states, are used in a FIFO manner. But an important difference is that these kernels use release-acquire instructions to ensure memory consistency, which has the advantage of being able to support various forms of data more flexibly. Although it is not as efficient as LL128 primitive in terms of performance, it is more helpful for fast iteration before the functionality converges.</p>
<p>Note that although these kernels achieve better performance compared to <code class="docutils literal notranslate"><span class="pre">allgather</span></code>, there is still considerable room for optimization, especially in latency-bound scenarios.</p>
<p>This optimization was implemented in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5570">PR 5570</a>, with the following performance improvement:</p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_communication_kernel.png">
</figure>
</div>
<p align="center"><sub><em>Figure 4: Optimization effect on communication kernels.</em></sub></p>
</section>
</section>
<section id="expert-parallelism-load-balancer-eplb">
<h3>Expert Parallelism Load Balancer (EPLB)<a class="headerlink" href="#expert-parallelism-load-balancer-eplb" title="Link to this heading">#</a></h3>
<p>As introduced in our <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md#ep-load-balancer">previous blog</a>, EP-level workload imbalance is common for large-scale EP inference across multiple datasets and has significant performance impacts. TensorRT-LLM implements a set of functionalities to address this issue. We have refined the code and improved the usability of this feature, and the benefits of EPLB are directly reflected in kernel duration improvements.</p>
<p>The core challenge with EP scaling is that different experts receive varying amounts of work based on the routing decisions made by the MoE layer. This imbalance becomes more pronounced as EP size increases, leading to scenarios where some GPUs are heavily loaded while others remain underutilized. The Expert Parallelism Load Balancer (EPLB) addresses this by dynamically redistributing expert assignments to achieve better load balance across all participating GPUs.</p>
<p>EPLB operates in two main modes:</p>
<ul class="simple">
<li><p><strong>Static EPLB</strong>: Pre-computed expert-to-GPU mappings based on historical data patterns</p></li>
<li><p><strong>Online EPLB</strong>: Dynamic runtime redistribution that adapts to real-time workload patterns</p></li>
</ul>
<p>While Static EPLB provides good baseline improvements, Online EPLB offers the potential for optimal load balancing by responding to actual runtime patterns. However, implementing Online EPLB presented several unexpected technical challenges, particularly around weight synchronization and memory management in GPU clusters.</p>
<p>In the previous <a class="reference internal" href="#kernel-optimizations">Kernel Optimizations</a> section, we noted that <code class="docutils literal notranslate"><span class="pre">reduce_scatter</span></code> and <code class="docutils literal notranslate"><span class="pre">alltoall</span></code> kernels do not show good scalability, with load imbalance being the major root cause. After applying proper EPLB strategy, those kernels perform well even when EP size scales to larger extents.</p>
<section id="attempts-at-online-eplb-implementation">
<h4>Attempts at Online EPLB Implementation<a class="headerlink" href="#attempts-at-online-eplb-implementation" title="Link to this heading">#</a></h4>
<p>We discussed the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md#high-level-design-introduction">high-level design</a> and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md#online-ep-load-balancer">implementation considerations</a> of Online EPLB in our previous blog. However, several unexpected issues arose during implementation.</p>
<p>These issues primarily stem from the weight updating mechanism.</p>
<section id="initial-approach-for-weight-updating-cudamemcpyasync">
<h5>1. Initial Approach for Weight Updating - cudaMemcpyAsync<a class="headerlink" href="#initial-approach-for-weight-updating-cudamemcpyasync" title="Link to this heading">#</a></h5>
<p>Our initial approach for weight updating was straightforward. Since GPU kernels from the model forward thread read weights, we placed weights directly in GPU memory using <code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code> and used a separate non-blocking stream to invoke multiple <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code> calls for weight updates. After implementing the first version of the prototype, we discovered that with CUDA Graph enabled, the model forward thread and the weight updating thread could deadlock.</p>
<p>After investigation, we found the root cause: both <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> and <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code> were competing for the same mutex inside CUDA. In our implementation with layer-wise weight updating, the GPU needs to synchronize with the CPU during model forward passes. This creates kernels that wait for CPU signals indicating that updates are complete and MoE weights are safe to use. These waiting kernels block subsequent kernels.</p>
<p>Since LLM models contain numerous kernels, <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> may need to wait for previous kernels to finish to acquire sufficient resources for launch completion. When waiting kernels are blocked by the CPU, <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> is also blocked. The CPU thread responsible for unblocking this process is the weight update thread, which should signal completion when weight updating finishes. However, since our initial implementation used <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code> for weight updating, it needed to acquire the CUDA mutex before starting memcpy operations. Unfortunately, this mutex was held by <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> in the model forward thread, which was waiting for the weight updating thread to complete. This created a deadlock scenario.</p>
<p>To resolve the deadlock, we needed to break the dependency cycle. While the model forward thread must depend on the weight updating thread for correctness, the weight updating process should not wait for <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> in the model forward thread. Our solution was to use alternative methods instead of <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code> to avoid competing for the same mutex with <code class="docutils literal notranslate"><span class="pre">cudaGraphLaunch</span></code> and other CUDA APIs.</p>
</section>
<section id="avoiding-deadlock-multithreaded-cpu-copy-with-managed-memory">
<h5>2. Avoiding Deadlock - Multithreaded CPU Copy with Managed Memory<a class="headerlink" href="#avoiding-deadlock-multithreaded-cpu-copy-with-managed-memory" title="Link to this heading">#</a></h5>
<p>Since weight updating is handled by CPU threads and we wanted to avoid interfering with GPU model forward passes while avoiding mutex contention in <code class="docutils literal notranslate"><span class="pre">cudaMemcpyAsync</span></code>, we chose to use CPU threads for copying operations. To achieve this, we needed MoE weights to be accessible by the CPU while remaining physically located on the GPU to provide high bandwidth for MoE forward passes.</p>
<p>On GB200 systems, the C2C link between CPU and GPU allows CPU access to GPU memory, with GPU memory treated as NUMA nodes. Although the CUDA Driver API doesn’t directly support this in CUDA 12.9, one option is to use <code class="docutils literal notranslate"><span class="pre">cudaMallocManaged</span></code> for MoE weights and use <code class="docutils literal notranslate"><span class="pre">cudaMemAdvise</span></code> to set the GPU as the preferred location while enabling CPU access. The CPU copy implementation was straightforward, but we still needed to detect system topology and bind to CPU cores belonging to the same NUMA nodes as the GPU’s host NUMA node.</p>
<p>After completing this implementation, CUDA Graph worked well with weight updating and we began seeing end-to-end performance benefits using Online EPLB in some configurations. However, we soon encountered issues with managed memory. Although the preferred location of managed memory was set to GPU, and on GB200 it typically remains on GPU when accessed by CPU, we still observed page migration when GPU memory usage approached capacity limits. The bottom half of the UVM interrupt service process for each GPU consumed 100% of one CPU core’s time, causing severe slowdowns when approaching GPU memory limits. To address this, we needed GPU memory that was accessible by CPU without triggering page migration.</p>
</section>
<section id="numa-memory-to-prevent-page-migration">
<h5>3. NUMA Memory to Prevent Page Migration<a class="headerlink" href="#numa-memory-to-prevent-page-migration" title="Link to this heading">#</a></h5>
<p>On GB200 systems, the Grace CPU and Blackwell GPU are connected via C2C links, enabling mutual memory access. GPU memories are also exposed to the OS as NUMA nodes. Running <code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">-H</span></code> on GB200 nodes shows output similar to this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># numactl -H
available: 34 nodes (0-33)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
node 0 size: 489935 MB
node 0 free: 370318 MB
node 1 cpus: 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 1 size: 489795 MB
node 1 free: 465004 MB
node 2 cpus:
node 2 size: 188416 MB
node 2 free: 188415 MB
node 3 cpus:
node 3 size: 0 MB
node 3 free: 0 MB
...
node 9 cpus:
node 9 size: 0 MB
node 9 free: 0 MB
node 10 cpus:
node 10 size: 188416 MB
node 10 free: 188416 MB
...
node 18 cpus:
node 18 size: 188416 MB
node 18 free: 188416 MB
...
node 26 cpus:
node 26 size: 188416 MB
node 26 free: 188416 MB
...
node distances:
node   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  0:  10  40  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  120  120  120  120  120  120  120  120  120  120  120  120  120  120  120  120
  1:  40  10  120  120  120  120  120  120  120  120  120  120  120  120  120  120  120  120  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80  80
  2:  80  120  10  11  11  11  11  11  11  11  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40
  3:  80  120  11  10  11  11  11  11  11  11  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40
...
  9:  80  120  11  11  11  11  11  11  11  10  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40  40
...
</pre></div>
</div>
<p>In this configuration, <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">1</span></code> are Grace CPU nodes, each with 72 CPU cores and 480GB of memory. <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">2</span></code>, <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">10</span></code>, <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">18</span></code>, and <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">26</span></code> represent NVIDIA GB200 GPUs, which have no CPU cores but contain memory. Additional NUMA nodes (3-9, 11-17, 19-25, 27-33) are reserved for MIG instances and show 0 MB memory size. For brevity, we only show <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">3</span></code> and <code class="docutils literal notranslate"><span class="pre">node</span> <span class="pre">9</span></code> in the example.</p>
<p>It’s possible to allocate system memory on a GPU’s NUMA node using <code class="docutils literal notranslate"><span class="pre">numa_alloc_onnode</span></code> (e.g., NUMA node 2 for GPU 0), then register that memory with the GPU using <code class="docutils literal notranslate"><span class="pre">cudaHostRegister</span></code> to make it accessible as host system memory. This allows both CPU and GPU to access the memory, and our testing showed that bandwidth appears nearly identical to normal device memory from the GPU’s perspective.</p>
<p>This approach resolved page migration issues, and Online EPLB worked well for large batch sizes per GPU (e.g., 256). However, when investigating smaller batch sizes (32 or 64), we found that MoE GEMM kernel execution time could be higher than without Online EPLB—increasing from 75 µs to 93 µs for the first group GEMM of MoE with EP size 16. Further experiments revealed that when running group GEMM multiple times in the same layer, only the first execution suffered from this slowdown. By adding a warmup kernel that read only one value from 64 KB of weights, we found this simple warmup kernel consumed more than half the execution time of the group GEMM kernel. More interestingly, when running this warmup kernel in parallel with other kernels (using only 14 CTAs), those other kernels also became extremely slow. Based on these observations, we concluded that we were encountering TLB thrashing.</p>
</section>
<section id="addressing-the-tlb-thrashing-issue">
<h5>4. Addressing the TLB Thrashing Issue<a class="headerlink" href="#addressing-the-tlb-thrashing-issue" title="Link to this heading">#</a></h5>
<p>On GB200 systems, the default page size is 64 KB, which can be verified with:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># getconf PAGE_SIZE
65536
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">numa_alloc_onnode</span></code> function may use this page size, which is too small for efficient GPU kernel execution. Linux systems support <a class="reference external" href="https://docs.kernel.org/admin-guide/mm/hugetlbpage.html">HugeTLB Pages</a>, and on GB200 systems, the huge page size is 512 MB:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># cat /proc/meminfo
MemTotal:       1774995776 kB
MemFree:        1651165696 kB
MemAvailable:   1671517696 kB
...
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:     524288 kB
Hugetlb:               0 kB
</pre></div>
</div>
<p>By using huge pages, we can significantly reduce the number of required TLB entries and avoid TLB thrashing. Our implementation approach:</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">mmap</span></code> to allocate address space aligned to 512 MB boundaries</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">mbind</span></code> to bind the memory to the GPU’s NUMA node (e.g., NUMA node 2 for GPU 0)</p></li>
<li><p>Request huge pages using <code class="docutils literal notranslate"><span class="pre">madvise</span></code> with the <code class="docutils literal notranslate"><span class="pre">MADV_HUGEPAGE</span></code> flag</p></li>
<li><p>Register the memory with the GPU using <code class="docutils literal notranslate"><span class="pre">cudaHostRegister</span></code></p></li>
</ul>
<p>This approach provides memory that is located on the GPU, accessible by the host, uses large pages instead of small ones, and doesn’t trigger page migration. One consideration is that huge page allocation requires memory allocation at the granularity of one page (512 MB), which could cause significant memory waste with separate allocations. Since our primary use case involves MoE weights that are allocated at model load time and persist throughout the model’s lifetime, we implemented a simple memory pool to minimize waste.</p>
<p>Since our implementation relies on huge pages and <code class="docutils literal notranslate"><span class="pre">madvise</span></code>, Transparent Hugepages must be enabled on the system. Without this, you may encounter the exception <code class="docutils literal notranslate"><span class="pre">madvise(MADV_HUGEPAGE)</span> <span class="pre">failed.</span></code>. To verify that Transparent Hugepages is properly configured:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;$<span class="w"> </span>cat<span class="w"> </span>/sys/kernel/mm/transparent_hugepage/enabled
always<span class="w"> </span><span class="o">[</span>madvise<span class="o">]</span><span class="w"> </span>never
&gt;$<span class="w"> </span>cat<span class="w"> </span>/sys/kernel/mm/transparent_hugepage/defrag
always<span class="w"> </span>defer<span class="w"> </span>defer+madvise<span class="w"> </span><span class="o">[</span>madvise<span class="o">]</span><span class="w"> </span>never
</pre></div>
</div>
<p>In the output above, the value in square brackets indicates the current setting. If <code class="docutils literal notranslate"><span class="pre">never</span></code> is highlighted instead of <code class="docutils literal notranslate"><span class="pre">madvise</span></code>, you can enable Transparent HugePages with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span>madvise<span class="w"> </span>&gt;<span class="w"> </span>/sys/kernel/mm/transparent_hugepage/enabled
</pre></div>
</div>
<p>After implementing huge pages, we found that warmup kernels now execute in only 4 µs without slowing down other kernels. Additionally, group GEMM kernel performance matches that achieved without Online EPLB, both with and without warmup operations. This optimization was implemented in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5963">PR 5963</a>, and we achieved additional performance improvements using Online EPLB on the Pareto curve.</p>
</section>
</section>
</section>
<section id="multi-token-prediction-mtp">
<h3>Multi-Token Prediction (MTP)<a class="headerlink" href="#multi-token-prediction-mtp" title="Link to this heading">#</a></h3>
<p>MTP allows verifying and accepting several draft tokens in a single iteration, which is very beneficial for scenarios that prefer low latency. TensorRT-LLM has supported MTP, and we refer to our previous <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.md#mtp-implementation-in-tensorrt-llm">MTP blog</a> for more details on the implementation.</p>
<p>For large EP, we have also extended the implementation so that it works well with online EPLB. This was implemented in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5213">PR 5213</a>.</p>
</section>
<section id="host-overhead-optimization">
<h3>Host Overhead Optimization<a class="headerlink" href="#host-overhead-optimization" title="Link to this heading">#</a></h3>
<p>Since large-scale EP enables extensive parallelism that includes both expert parallelism and attention data parallelism, the total batch size of one iteration scales with the number of total GPUs involved in the calculation. One outcome is that this significantly increases the number of requests and responses that the system must handle, putting huge pressure on Python threads. The Global Interpreter Lock (GIL) makes the situation worse, since multi-threading won’t help under heavy system workloads. When the workload prefers higher throughput, it could even appear that highly optimized CUDA kernels are faster than CPU operation execution, and the GPU could be idle waiting for the CPU to finish the work.</p>
<p>To address the increased host overhead when scaling parallelism in the system, we added optimizations to performance hot spots to reduce single-thread pressure.</p>
<section id="reduce-binding-and-inter-process-communication-overhead">
<h4>Reduce Binding and Inter-Process Communication Overhead<a class="headerlink" href="#reduce-binding-and-inter-process-communication-overhead" title="Link to this heading">#</a></h4>
<p>TensorRT-LLM is designed to be composed of both C++ and Python code, so that C++ can handle the most performance-sensitive parts while Python handles higher-level logic. As we try to put more logic into Python to make the program easier to read and debug, there are still frequent conversations through binding interfaces between C++ and Python. Besides, since most of the logic is implemented in Python, there are several layers of implementation that communicate with each other through inter-process communication overhead. Frequent binding calls and serialization/deserialization introduced by inter-process communication slow down the core library.</p>
<p>To improve program efficiency, we used environment variables introduced in the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-analysis.md">performance analysis guidance</a> to measure and profile CPU overhead, and improved performance by reducing and reusing different binding calls as much as possible, and delaying Python object deserialization to avoid duplicated serialization and reduce message size when doing inter-process communication. This optimization was added in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5224">PR 5224</a>. We have also reduced Python garbage collection (GC) impacts in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5141">PR 5141</a>.</p>
<p>To enable powerful NVTX markers for easier analysis of host overheads, TensorRT-LLM provides several useful environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">TLLM_NVTX_DEBUG</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="c1"># enables more NVTX markers</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TLLM_PROFILE_RECORD_GC</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="c1"># enables GC collection hint</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TLLM_PROFILE_START_STOP</span><span class="o">=</span><span class="m">100</span>-150<span class="w"> </span><span class="c1"># enable specific iterations profiling</span>
</pre></div>
</div>
</section>
<section id="support-stream-interval">
<h4>Support Stream Interval<a class="headerlink" href="#support-stream-interval" title="Link to this heading">#</a></h4>
<p>As mentioned previously, one outcome of large-scale workloads is that they significantly increase the number of requests and responses that the system must handle, putting huge pressure on Python threads. When the GPU finishes one iteration of calculation, a batch of responses are generated under streaming mode. For each response, TensorRT-LLM must perform detokenization so that output IDs are converted to strings, and OpenAI API protocol objects need to be initialized so that responses can be returned to the user. This becomes time-consuming, especially when the number of responses is huge and the CPU must process them on each iteration. One observation from the user side will be reduced streaming performance when compared to non-streaming.</p>
<p>To address this problem, TensorRT-LLM has supported a feature called stream interval. Instead of handling all responses on each iteration, a user-specified <code class="docutils literal notranslate"><span class="pre">stream_interval</span></code> <code class="docutils literal notranslate"><span class="pre">N</span></code> indicates that responses will be handled and returned every <code class="docutils literal notranslate"><span class="pre">N</span></code> iterations. This way, on each iteration, there will still be one output ID generated, but it won’t be returned to users immediately (except for the first token for the sake of time-to-first-token latency). Instead, tokens accumulate for <code class="docutils literal notranslate"><span class="pre">N</span></code> iterations, and one response is created to handle those <code class="docutils literal notranslate"><span class="pre">N</span></code> generated tokens, which greatly reduces pressure on the CPU side by giving more time for the CPU to catch up. Meanwhile, users can still get streamed output.</p>
<p>This feature was added in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/5284">PR 5284</a>, and we have verified that it works effectively to reduce host overhead. In most cases, setting <code class="docutils literal notranslate"><span class="pre">stream_interval</span></code> to 2 or 4 should close the gap (if any) between streaming and non-streaming modes. The feature can be enabled by setting the following in the YAML extra config file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">stream_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="end-to-end-performance">
<h2>End-to-End Performance<a class="headerlink" href="#end-to-end-performance" title="Link to this heading">#</a></h2>
<p>To demonstrate the benefits of large-scale EP, we compared performance on EP16 and EP32 with EP4 and EP8 as baselines, on GB200 NVL72 using DeepSeek R1 FP4 <a class="reference external" href="https://huggingface.co/nvidia/DeepSeek-R1-FP4">checkpoints</a>.</p>
<p>We explored different workloads including 1k-ISL 1k-OSL, 4k-ISL 1k-OSL, and 8k-ISL 1k-OSL. To quickly collect these data points and ensure that generation nodes are saturated, we used the <code class="docutils literal notranslate"><span class="pre">TLLM_BENCHMARK_REQ_QUEUES_SIZE</span></code> environment variable when benchmarking so that the workload can quickly reach a balanced point. The numbers are measured on commit <code class="docutils literal notranslate"><span class="pre">0cf2f6f154b4a5765d89945b20aa3449b2be7933</span></code> with a translation-task dataset, and generated by post-processing the per-iteration log.</p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_perf-1k-1k-dep.png" width="800">
</figure>
</div>
<p align="center"><sub><em>Figure 5: DeepSeek R1 throughput on ISL/OSL 1k/1k.</em></sub></p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_perf-4k-1k-dep.png" width="800">
</figure>
</div>
<p align="center"><sub><em>Figure 6: DeepSeek R1 throughput on ISL/OSL 4k/1k.</em></sub></p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_perf-8k-1k-dep.png" width="800">
</figure>
</div>
<p align="center"><sub><em>Figure 7: DeepSeek R1 throughput on ISL/OSL 8k/1k.</em></sub></p>
<p>When enabling MTP, there is an extra performance boost compared to the baseline. We conducted end-to-end experiments and compared to EP4 and EP8 as baselines, seeing up to 6.17x per-GPU output throughput improvement. The numbers are measured with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code> enabling multiple features like large EP, disaggregated serving, EPLB, MTP, and using an OpenAI API client <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/serve/scripts/benchmark_serving.py">tool</a> that sends requests to the server and collects performance metrics.</p>
<div align="center">
<figure>
  <img src="../media/tech_blog8_perf-8k-1k-e2e-mtp.png" width="800">
</figure>
</div>
<p align="center"><sub><em>Figure 8: DeepSeek R1 throughput on ISL/OSL 8k/1k with MTP enabled.</em></sub></p>
<p>To reproduce the numbers, refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/wide_ep/slurm_scripts"><code class="docutils literal notranslate"><span class="pre">examples/wide_ep/slurm_scripts</span></code></a> directory. The scripts there demonstrate how to launch TensorRT-LLM disaggregated serving with large-scale EP and other features enabled on a SLURM cluster.</p>
</section>
<section id="future-work">
<h2>Future Work<a class="headerlink" href="#future-work" title="Link to this heading">#</a></h2>
<section id="further-performance-optimization">
<h3>Further Performance Optimization<a class="headerlink" href="#further-performance-optimization" title="Link to this heading">#</a></h3>
<p>We are planning to implement more performance optimizations for the large EP implementation, including optimizing the <code class="docutils literal notranslate"><span class="pre">concat_qkv</span></code> operation for the context phase, quantizing <code class="docutils literal notranslate"><span class="pre">Wo_GEMM</span></code> to FP4, supporting low-precision <code class="docutils literal notranslate"><span class="pre">All2All</span></code> operations, and fusing some <code class="docutils literal notranslate"><span class="pre">All2All</span></code> kernels into one. We will also explore integrating more features such as PDL.</p>
</section>
</section>
<section id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Link to this heading">#</a></h2>
<p>This work represents an outstanding example of collaborative engineering excellence within the TensorRT-LLM team. The successful implementation and optimization of large-scale Expert Parallelism required coordinated efforts across multiple domains - from low-level CUDA kernel optimizations to high-level system architecture design. The dedication and technical expertise demonstrated by our team members throughout this project has been truly remarkable.</p>
<p>Large-scale Expert Parallelism represents one of the important workloads for users productive scenarios, enabling efficient deployment of large MoE models. The performance improvements achieved through this work demonstrate the transformative potential of expert parallelism at scale, and this work opens new possibilities for deploying increasingly sophisticated AI models in production environments.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-highlights">Optimization Highlights</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-optimizations">Kernel Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#moe-auxiliary-kernels">MoE Auxiliary Kernels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-kernels">Communication Kernels</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expert-parallelism-load-balancer-eplb">Expert Parallelism Load Balancer (EPLB)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attempts-at-online-eplb-implementation">Attempts at Online EPLB Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#initial-approach-for-weight-updating-cudamemcpyasync">1. Initial Approach for Weight Updating - cudaMemcpyAsync</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-deadlock-multithreaded-cpu-copy-with-managed-memory">2. Avoiding Deadlock - Multithreaded CPU Copy with Managed Memory</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#numa-memory-to-prevent-page-migration">3. NUMA Memory to Prevent Page Migration</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#addressing-the-tlb-thrashing-issue">4. Addressing the TLB Thrashing Issue</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-token-prediction-mtp">Multi-Token Prediction (MTP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#host-overhead-optimization">Host Overhead Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-binding-and-inter-process-communication-overhead">Reduce Binding and Inter-Process Communication Overhead</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#support-stream-interval">Support Stream Interval</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#end-to-end-performance">End-to-End Performance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-work">Future Work</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-performance-optimization">Further Performance Optimization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgements">Acknowledgements</a></li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2025, NVidia.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
<div class="extra_footer">
  
  <p>Last updated on August 15, 2025.</p>
  
  <p>This page is generated by TensorRT-LLM commit <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/26f413a">26f413a</a>.</p>
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>