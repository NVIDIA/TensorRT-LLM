# Multi-stage Dockerfile
ARG BASE_IMAGE=nvcr.io/nvidia/pytorch
ARG TRITON_IMAGE=nvcr.io/nvidia/tritonserver
ARG BASE_TAG=25.12-py3
ARG TRITON_BASE_TAG=25.12-py3
ARG DEVEL_IMAGE=devel
ARG RUNTIME_BASE_IMAGE=nvcr.io/nvidia/cuda-dl-base
ARG RUNTIME_BASE_TAG=25.12-cuda13.1-runtime-ubuntu24.04

FROM ${BASE_IMAGE}:${BASE_TAG} AS base

# Add NVIDIA EULA and AI Terms labels
LABEL com.nvidia.eula="https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/"
LABEL com.nvidia.ai-terms="https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/"

# https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html
ARG SH_ENV="/etc/shinit_v2"
ENV ENV=${SH_ENV}
ARG BASH_ENV="/etc/bash.bashrc"
ENV BASH_ENV=${BASH_ENV}

ARG GITHUB_MIRROR=""
RUN echo "Using GitHub mirror: $GITHUB_MIRROR"

ARG PYTHON_VERSION="3.12.3"
RUN echo "Using Python version: $PYTHON_VERSION"

SHELL ["/bin/bash", "-c"]

FROM base AS devel

#
# NB: PyTorch requires this to be < 1.0
ENV PYTORCH_ALLOC_CONF="garbage_collection_threshold:0.99999"

# Copy all installation scripts at once to reduce layers
COPY docker/common/install.sh \
     docker/common/install_base.sh \
     docker/common/install_cmake.sh \
     docker/common/install_ccache.sh \
     docker/common/install_cuda_toolkit.sh \
     docker/common/install_tensorrt.sh \
     docker/common/install_polygraphy.sh \
     docker/common/install_mpi4py.sh \
     docker/common/install_pytorch.sh \
     docker/common/install_ucx.sh \
     docker/common/install_nixl.sh \
     docker/common/install_etcd.sh \
     ./

ARG TRT_VER
ARG CUDA_VER
ARG CUDNN_VER
ARG NCCL_VER
ARG CUBLAS_VER
ARG TORCH_INSTALL_TYPE="skip"
RUN GITHUB_MIRROR=${GITHUB_MIRROR} \
    PYTHON_VERSION=${PYTHON_VERSION} \
    TRT_VER=${TRT_VER} \
    CUDA_VER=${CUDA_VER} \
    CUDNN_VER=${CUDNN_VER} \
    NCCL_VER=${NCCL_VER} \
    CUBLAS_VER=${CUBLAS_VER} \
    TORCH_INSTALL_TYPE=${TORCH_INSTALL_TYPE} \
    bash ./install.sh --base --cmake --ccache --cuda_toolkit --tensorrt --polygraphy --mpi4py --pytorch --opencv && \
    rm install_base.sh && \
    rm install_cmake.sh && \
    rm install_ccache.sh && \
    rm install_cuda_toolkit.sh && \
    rm install_tensorrt.sh && \
    rm install_polygraphy.sh && \
    rm install_mpi4py.sh && \
    rm install_pytorch.sh && \
    rm install.sh

# Copy and install dependencies from constraints.txt
COPY constraints.txt /tmp/constraints.txt
RUN pip3 install --no-cache-dir -r /tmp/constraints.txt && rm /tmp/constraints.txt

# Remove nbconvert to avoid https://github.com/advisories/GHSA-xm59-rqc7-hhvf in the base NGC PyTorch image.
RUN pip3 uninstall -y nbconvert || true

# Install UCX, NIXL, etcd
# TODO: Combine these into the main install.sh script
RUN GITHUB_MIRROR=${GITHUB_MIRROR} bash ./install_ucx.sh && \
    GITHUB_MIRROR=${GITHUB_MIRROR} bash ./install_nixl.sh && \
    bash ./install_etcd.sh && \
    rm install_ucx.sh && \
    rm install_nixl.sh && \
    rm install_etcd.sh && \
    rm -rf /root/.cache/pip && \
    rm -rf /root/.cache/uv/archive-v0 && \
    # WAR against https://github.com/advisories/GHSA-58pv-8j8x-9vj2
    rm -rf /usr/local/lib/python3.12/dist-packages/setuptools/_vendor/jaraco.context-5.3.0.dist-info && \
    # WAR against https://github.com/advisories/GHSA-8rrh-rw8j-w5fx
    rm -rf /usr/local/lib/python3.12/dist-packages/setuptools/_vendor/wheel-0.45.1.dist-info

# Generate OSS attribution file for devel image
ARG TRT_LLM_VER
ARG TARGETARCH
COPY scripts/generate_container_oss_attribution.sh /tmp/generate_container_oss_attribution.sh
RUN bash /tmp/generate_container_oss_attribution.sh "devel" "${TRT_LLM_VER}" "${TARGETARCH}" && \
    rm /tmp/generate_container_oss_attribution.sh

FROM ${TRITON_IMAGE}:${TRITON_BASE_TAG} AS triton

FROM devel AS tritondevel

ARG GITHUB_MIRROR=""
COPY --from=triton /opt/tritonserver/backends/python /opt/tritonserver/backends/python
COPY --from=triton /opt/tritonserver/lib /opt/tritonserver/lib
COPY --from=triton /opt/tritonserver/include /opt/tritonserver/include
COPY --from=triton /opt/tritonserver/bin /opt/tritonserver/bin
COPY --from=triton /opt/tritonserver/caches /opt/tritonserver/caches

# Copy all installation scripts at once to reduce layers
COPY docker/common/install_triton.sh \
     docker/common/install_mooncake.sh \
     ./

# Install Mooncake, after triton handles boost requirement
RUN GITHUB_MIRROR=${GITHUB_MIRROR} bash ./install_triton.sh && \
    if [ -f /etc/redhat-release ]; then \
        echo "Rocky8 detected, skipping mooncake installation"; \
    else \
        bash ./install_mooncake.sh; \
    fi && \
    rm install_triton.sh && \
    rm install_mooncake.sh

FROM ${DEVEL_IMAGE} AS wheel
WORKDIR /src/tensorrt_llm
COPY benchmarks benchmarks
COPY cpp cpp
COPY docker docker
COPY scripts scripts
COPY tensorrt_llm tensorrt_llm
COPY 3rdparty 3rdparty
COPY .gitmodules setup.py requirements.txt requirements-dev.txt constraints.txt README.md ./

# Create cache directories for pip and ccache
RUN mkdir -p /root/.cache/pip /root/.cache/ccache
ENV CCACHE_DIR=/root/.cache/ccache
# Build the TRT-LLM wheel
ARG GITHUB_MIRROR=""
ARG BUILD_WHEEL_ARGS="--clean --benchmarks"
ARG BUILD_WHEEL_SCRIPT="scripts/build_wheel.py"
RUN --mount=type=cache,target=/root/.cache/pip --mount=type=cache,target=${CCACHE_DIR} \
    GITHUB_MIRROR=$GITHUB_MIRROR python3 ${BUILD_WHEEL_SCRIPT} ${BUILD_WHEEL_ARGS}

FROM ${DEVEL_IMAGE} AS release

# Create a cache directory for pip
RUN mkdir -p /root/.cache/pip

WORKDIR /app/tensorrt_llm
RUN --mount=type=cache,target=/root/.cache/pip --mount=type=bind,from=wheel,source=/src/tensorrt_llm/build,target=/tmp/wheel \
    pip install /tmp/wheel/tensorrt_llm*.whl

COPY README.md ./
COPY --from=wheel /src/tensorrt_llm/build/tensorrt_llm*.whl ./
COPY docs docs
COPY cpp/include include

RUN ln -sv $(python3 -c 'import site; print(f"{site.getsitepackages()[0]}/tensorrt_llm/bin")') bin && \
    test -f bin/executorWorker && \
    ln -sv $(python3 -c 'import site; print(f"{site.getsitepackages()[0]}/tensorrt_llm/libs")') lib && \
    test -f lib/libnvinfer_plugin_tensorrt_llm.so && \
    echo "/app/tensorrt_llm/lib" > /etc/ld.so.conf.d/tensorrt_llm.conf && \
    ldconfig && \
    ! ( ldd -v bin/executorWorker | grep tensorrt_llm | grep -q "not found" )

ARG SRC_DIR=/src/tensorrt_llm
COPY --from=wheel ${SRC_DIR}/benchmarks benchmarks
ARG CPP_BUILD_DIR=${SRC_DIR}/cpp/build
COPY --from=wheel \
     ${CPP_BUILD_DIR}/benchmarks/bertBenchmark \
     ${CPP_BUILD_DIR}/benchmarks/gptManagerBenchmark \
     ${CPP_BUILD_DIR}/benchmarks/disaggServerBenchmark \
     benchmarks/cpp/

COPY examples examples
RUN chmod -R a+w examples && \
    rm -v \
      benchmarks/cpp/bertBenchmark.cpp \
      benchmarks/cpp/gptManagerBenchmark.cpp \
      benchmarks/cpp/disaggServerBenchmark.cpp \
      benchmarks/cpp/CMakeLists.txt && \
    rm -rf /root/.cache/pip && \
    rm -rf /root/.cache/uv/archive-v0 && \
    # WAR against https://github.com/advisories/GHSA-58pv-8j8x-9vj2
    rm -rf /usr/local/lib/python3.12/dist-packages/setuptools/_vendor/jaraco.context-5.3.0.dist-info && \
    # WAR against https://github.com/advisories/GHSA-8rrh-rw8j-w5fx
    rm -rf /usr/local/lib/python3.12/dist-packages/setuptools/_vendor/wheel-0.45.1.dist-info

ARG GIT_COMMIT
ARG TRT_LLM_VER
ARG TARGETARCH
ENV TRT_LLM_GIT_COMMIT=${GIT_COMMIT} \
    TRT_LLM_VERSION=${TRT_LLM_VER}

# Generate OSS attribution file for release image
COPY scripts/generate_container_oss_attribution.sh /tmp/generate_container_oss_attribution.sh
RUN bash /tmp/generate_container_oss_attribution.sh "release" "${TRT_LLM_VER}" "${TARGETARCH}" && rm /tmp/generate_container_oss_attribution.sh

FROM ${RUNTIME_BASE_IMAGE}:${RUNTIME_BASE_TAG} AS runtime

LABEL com.nvidia.eula="https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/"
LABEL com.nvidia.ai-terms="https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/"

ARG ARCH_ALT=x86_64
SHELL ["/bin/bash", "-c"]

# Copy CUDA development tools needed for Triton JIT kernel compilation
COPY --from=base /usr/local/cuda/bin/nvcc /usr/local/cuda/bin/nvcc
COPY --from=base /usr/local/cuda/bin/cudafe++ /usr/local/cuda/bin/cudafe++
COPY --from=base /usr/local/cuda/bin/ptxas /usr/local/cuda/bin/ptxas
COPY --from=base /usr/local/cuda/bin/fatbinary /usr/local/cuda/bin/fatbinary
COPY --from=base /usr/local/cuda/include/ /usr/local/cuda/include/
COPY --from=base /usr/local/cuda/nvvm /usr/local/cuda/nvvm
COPY --from=base /usr/local/cuda/bin/cuobjdump /usr/local/cuda/bin/cuobjdump
COPY --from=base /usr/local/cuda/bin/nvdisasm /usr/local/cuda/bin/nvdisasm
COPY --from=base /usr/local/cuda/lib64/libcudart.so* /usr/local/cuda/lib64/
COPY --from=base /usr/local/cuda/lib64/libcupti* /usr/local/cuda/lib64/
COPY --from=base /usr/local/cuda/lib64/libcusparseLt* /usr/local/cuda/lib64/

# Copy HPC-X stack (OpenMPI, UCX, UCC) from the PyTorch base image
COPY --from=base /opt/hpcx /opt/hpcx

# Copy additional system libraries from the PyTorch base image
COPY --from=base /usr/local/lib/lib* /usr/local/lib/
COPY --from=base /usr/lib/${ARCH_ALT}-linux-gnu/libnuma.so* /usr/lib/${ARCH_ALT}-linux-gnu/

# Copy TensorRT from the release stage (installed in devel via install_tensorrt.sh)
COPY --from=release /usr/local/tensorrt /usr/local/tensorrt

# Copy standalone UCX build (installed in devel via install_ucx.sh)
COPY --from=release /usr/local/ucx /usr/local/ucx

# Copy NIXL (installed in devel via install_nixl.sh)
COPY --from=release /opt/nvidia/nvda_nixl /opt/nvidia/nvda_nixl

# Copy etcd binaries (installed in devel via install_etcd.sh)
COPY --from=release /usr/local/bin/etcd /usr/local/bin/etcd
COPY --from=release /usr/local/bin/etcdctl /usr/local/bin/etcdctl
COPY --from=release /usr/local/bin/etcdutl /usr/local/bin/etcdutl

# Install runtime system packages
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        gnupg2 curl ca-certificates && \
    if [ "${ARCH_ALT}" = "x86_64" ]; then ARCH_GPG="x86_64"; else ARCH_GPG="sbsa"; fi && \
    curl -fsSL \
        "https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/${ARCH_GPG}/cuda-archive-keyring.gpg" \
        -o /usr/share/keyrings/cuda-archive-keyring.gpg && \
    echo "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] \
        https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/${ARCH_GPG} /" \
        > /etc/apt/sources.list.d/cuda.repo.list && \
    apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        build-essential \
        g++ \
        ninja-build \
        git \
        git-lfs \
        python3-dev \
        python3-pip \
        jq \
        libcudnn9-cuda-13 \
        libnvshmem3-cuda-13 \
        libzmq3-dev \
        ibverbs-providers \
        ibverbs-utils \
        libibumad3 \
        libibverbs1 \
        libnuma1 \
        numactl \
        librdmacm1 \
        rdma-core \
        openssh-client \
        openssh-server && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    ln -sf /usr/lib/${ARCH_ALT}-linux-gnu/libnccl.so.2 /usr/lib/${ARCH_ALT}-linux-gnu/libnccl.so || true

# Copy Python packages with TRT-LLM and all dependencies from the release stage
COPY --from=release /usr/local/lib/python3.12/dist-packages/ /usr/local/lib/python3.12/dist-packages/

# Set up TRT-LLM application directory
WORKDIR /app/tensorrt_llm
COPY --from=release /app/tensorrt_llm/ /app/tensorrt_llm/

# Register library paths with the dynamic linker
RUN echo "/app/tensorrt_llm/lib" > /etc/ld.so.conf.d/tensorrt_llm.conf && \
    echo "/usr/local/tensorrt/lib" > /etc/ld.so.conf.d/tensorrt.conf && \
    echo "/usr/local/ucx/lib" > /etc/ld.so.conf.d/ucx.conf && \
    echo "/usr/local/ucx/lib/ucx" >> /etc/ld.so.conf.d/ucx.conf && \
    echo "/opt/nvidia/nvda_nixl/lib/${ARCH_ALT}-linux-gnu" > /etc/ld.so.conf.d/nixl.conf && \
    echo "/opt/nvidia/nvda_nixl/lib64" >> /etc/ld.so.conf.d/nixl.conf && \
    ldconfig

ENV CUDA_HOME=/usr/local/cuda \
    NVIDIA_DRIVER_CAPABILITIES=video,compute,utility \
    OMPI_MCA_coll_ucc_enable=0 \
    OPAL_PREFIX=/opt/hpcx/ompi

ENV PATH="/usr/local/ucx/bin:/opt/hpcx/ompi/bin:/usr/local/cuda/bin:/usr/local/cuda/nvvm/bin:${PATH}"

ENV LD_LIBRARY_PATH="\
/app/tensorrt_llm/lib:\
/usr/local/tensorrt/lib:\
/usr/local/ucx/lib:\
/usr/local/ucx/lib/ucx:\
/opt/hpcx/ompi/lib:\
/opt/hpcx/ucc/lib:\
/opt/nvidia/nvda_nixl/lib/${ARCH_ALT}-linux-gnu:\
/opt/nvidia/nvda_nixl/lib64:\
/usr/lib/${ARCH_ALT}-linux-gnu/nvshmem/13/:\
/usr/local/cuda/lib64"

ENV TRITON_CUPTI_PATH=/usr/local/cuda/include \
    TRITON_CUDACRT_PATH=/usr/local/cuda/include \
    TRITON_CUOBJDUMP_PATH=/usr/local/cuda/bin/cuobjdump \
    TRITON_NVDISASM_PATH=/usr/local/cuda/bin/nvdisasm \
    TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas \
    TRITON_CUDART_PATH=/usr/local/cuda/include

# Validate critical binaries and library linkage
RUN test -f /app/tensorrt_llm/bin/executorWorker && \
    ! ( ldd -v /app/tensorrt_llm/bin/executorWorker | grep tensorrt_llm | grep -q "not found" )

ARG GIT_COMMIT
ARG TRT_LLM_VER
ARG TARGETARCH
ENV TRT_LLM_GIT_COMMIT=${GIT_COMMIT} \
    TRT_LLM_VERSION=${TRT_LLM_VER}

COPY scripts/generate_container_oss_attribution.sh /tmp/generate_container_oss_attribution.sh
RUN bash /tmp/generate_container_oss_attribution.sh "runtime" "${TRT_LLM_VER}" "${TARGETARCH}" && \
    rm /tmp/generate_container_oss_attribution.sh


FROM wheel AS tritonbuild

WORKDIR /src/tensorrt_llm
RUN pip install /src/tensorrt_llm/build/tensorrt_llm*.whl
COPY ./triton_backend/ ./triton_backend/
ARG TRITON_BASE_TAG
RUN bash ./triton_backend/inflight_batcher_llm/scripts/build.sh -s "r${TRITON_BASE_TAG%-py3}"


FROM release AS tritonrelease

WORKDIR /app/tensorrt_llm
COPY ./triton_backend/all_models ./triton_backend/all_models
COPY ./triton_backend/scripts ./triton_backend/scripts
COPY ./triton_backend/tools ./triton_backend/tools
COPY ./triton_backend/inflight_batcher_llm/scripts ./triton_backend/inflight_batcher_llm/scripts
COPY ./triton_backend/inflight_batcher_llm/client ./triton_backend/inflight_batcher_llm/client
COPY --from=tritonbuild /opt/tritonserver/backends/tensorrtllm /opt/tritonserver/backends/tensorrtllm
