

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>TensorRT LLM Benchmarking &#8212; TensorRT-LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css?v=df3ac72c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=95073da6" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=65e89d2a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'developer-guide/perf-benchmarking';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = './_static/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.1.0rc5';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Continuous Integration Overview" href="ci-overview.html" />
    <link rel="prev" title="Performance Analysis" href="perf-analysis.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.1.0rc5" />


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick-start-guide.html">Quick Start Guide</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../installation/index.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation/containers.html">Pre-built release container images on NGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation/linux.html">Installing on Linux via <code class="docutils literal notranslate"><span class="pre">pip</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation/build-from-source-linux.html">Building from Source Code on Linux</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deployment Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../examples/llm_api_examples.html">LLM Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_inference.html">Generate text</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_inference_async.html">Generate text asynchronously</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_inference_async_streaming.html">Generate text in streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_inference_distributed.html">Distributed LLM Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_guided_decoding.html">Generate text with guided decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_logits_processor.html">Control generated text using logits processor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_multilora.html">Generate text with multiple LoRA adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_kv_cache_connector.html">KV Cache Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_runtime.html">Runtime Configuration Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_sampling.html">Sampling Techniques Showcase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_mgmn_llm_distributed.html">Run LLM-API with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_mgmn_trtllm_bench.html">Run trtllm-bench with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/llm_mgmn_trtllm_serve.html">Run trtllm-serve with pytorch backend on Slurm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../examples/trtllm_serve_examples.html">Online Serving Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/curl_chat_client.html">Curl Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/curl_chat_client_for_multimodal.html">Curl Chat Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/curl_completion_client.html">Curl Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/deepseek_r1_reasoning_parser.html">Deepseek R1 Reasoning Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/genai_perf_client.html">Genai Perf Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/genai_perf_client_for_multimodal.html">Genai Perf Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/openai_chat_client.html">OpenAI Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/openai_chat_client_for_multimodal.html">OpenAI Chat Client for Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/openai_completion_client.html">OpenAI Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/openai_completion_client_for_lora.html">Openai Completion Client For Lora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/openai_completion_client_json_schema.html">OpenAI Completion Client with JSON Schema</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/dynamo_k8s_example.html">Dynamo K8s Example</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../deployment-guide/index.html">Model Recipes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../deployment-guide/quick-start-recipe-for-deepseek-r1-on-trtllm.html">Quick Start Recipe for DeepSeek R1 on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment-guide/quick-start-recipe-for-llama3.3-70b-on-trtllm.html">Quick Start Recipe for Llama3.3 70B on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment-guide/quick-start-recipe-for-llama4-scout-on-trtllm.html">Quick Start Recipe for Llama4 Scout 17B on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment-guide/quick-start-recipe-for-gpt-oss-on-trtllm.html">Quick Start Recipe for GPT-OSS on TensorRT-LLM - Blackwell Hardware</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../models/supported-models.html">Supported Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="../models/adding-new-model.html">Adding a New Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../commands/trtllm-bench.html">trtllm-bench</a></li>

<li class="toctree-l1"><a class="reference internal" href="../commands/trtllm-eval.html">trtllm-eval</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../commands/trtllm-serve/index.html">trtllm-serve</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../commands/trtllm-serve/trtllm-serve.html">trtllm-serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="../commands/trtllm-serve/run-benchmark-with-trtllm-serve.html">Run benchmarking with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../llm-api/index.html">LLM API Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-api/reference.html">API Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../features/feature-combination-matrix.html">Feature Combination Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/attention.html">Multi-Head, Multi-Query, and Group-Query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/disagg-serving.html">Disaggregated Serving (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/kvcache.html">KV Cache System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/long-sequence.html">Long Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/lora.html">LoRA (Low-Rank Adaptation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/multi-modality.html">Multimodal Support in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/overlap-scheduler.html">Overlap Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/paged-attention-ifb-scheduler.html">Paged Attention, IFB, and Request Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/parallel-strategy.html">Parallelism in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/sampling.html">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/speculative-decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/checkpoint-loading.html">Checkpoint Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/auto_deploy/auto-deploy.html">AutoDeploy (Prototype)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="perf-analysis.html">Performance Analysis</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">TensorRT LLM Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-overview.html">Continuous Integration Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev-containers.html">Using Dev Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog10_ADP_Balance_Strategy.html">ADP Balance Strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog11_GPT_OSS_Eagle3.html">Running GPT-OSS-120B with Eagle3 Speculative Decoding on GB200/B200 (TensorRT LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html">Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.html">DeepSeek R1 MTP Implementation and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html">Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.html">Scaling Expert Parallelism in TensorRT LLM (Part 1: Design and Implementation of Large-scale EP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog5_Disaggregated_Serving_in_TensorRT-LLM.html">Disaggregated Serving in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog6_Llama4_maverick_eagle_guide.html">How to launch Llama4 Maverick + Eagle3 TensorRT LLM server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog7_NGram_performance_Analysis_And_Auto_Enablement.html">N-Gram Speculative Decoding in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2.html">Scaling Expert Parallelism in TensorRT LLM (Part 2: Performance Status and Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/tech_blog/blog9_Deploying_GPT_OSS_on_TRTLLM.html">Running a High Performance GPT-OSS-120B Inference Server with TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/Best_perf_practice_on_DeepSeek-R1_in_TensorRT-LLM.html">How to get best performance on DeepSeek-R1 in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/XQA-kernel.html">New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Quick Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/releases">Releases</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM">Github Code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/issues?q=is%3Aissue%20state%3Aopen%20label%3Aroadmap">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Use TensorRT Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../legacy/tensorrt_quickstart.html">LLM API with TensorRT Engine</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">TensorRT LLM Benchmarking</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="tensorrt-llm-benchmarking">
<span id="perf-benchmarking"></span><h1>TensorRT LLM Benchmarking<a class="headerlink" href="#tensorrt-llm-benchmarking" title="Link to this heading">#</a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This benchmarking suite is a work in progress.
Expect breaking API changes.</p>
</div>
<p>TensorRT LLM provides the <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> CLI, a packaged benchmarking utility that aims to make it
easier for users to reproduce our officially published <a class="reference internal" href="#./perf-overview.md#throughput-measurements"><span class="xref myst">performance overview</span></a>. <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> provides the follows:</p>
<ul class="simple">
<li><p>A streamlined way to build tuned engines for benchmarking for a variety of models and platforms.</p></li>
<li><p>An entirely Python workflow for benchmarking.</p></li>
<li><p>Ability to benchmark various flows and features within TensorRT LLM.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> executes all benchmarks using [in-flight batching] – for more information see
the <a class="reference internal" href="../advanced/gpt-attention.html#in-flight-batching"><span class="std std-ref">in-flight batching section</span></a> that describes the concept
in further detail.</p>
<section id="before-benchmarking">
<h2>Before Benchmarking<a class="headerlink" href="#before-benchmarking" title="Link to this heading">#</a></h2>
<p>For rigorous benchmarking where consistent and reproducible results are critical, proper GPU configuration is essential. These settings help maximize GPU utilization, eliminate performance variability, and ensure optimal conditions for accurate measurements. While not strictly required for normal operation, we recommend applying these configurations when conducting performance comparisons or publishing benchmark results.</p>
<section id="persistence-mode">
<h3>Persistence mode<a class="headerlink" href="#persistence-mode" title="Link to this heading">#</a></h3>
<p>Ensure persistence mode is enabled to maintain consistent GPU state:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>-pm<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="gpu-clock-management">
<h3>GPU Clock Management<a class="headerlink" href="#gpu-clock-management" title="Link to this heading">#</a></h3>
<p>Allow the GPU to dynamically adjust its clock speeds based on workload and temperature. While locking clocks at maximum frequency might seem beneficial, it can sometimes lead to thermal throttling and reduced performance. Reset GPU clocks using:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>-rgc
</pre></div>
</div>
</section>
<section id="set-power-limits">
<h3>Set power limits<a class="headerlink" href="#set-power-limits" title="Link to this heading">#</a></h3>
<p>First query the maximum power limit:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>nvidia-smi<span class="w"> </span>-q<span class="w"> </span>-d<span class="w"> </span>POWER
</pre></div>
</div>
<p>Then configure the GPU to operate at its maximum power limit for consistent performance:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>-pl<span class="w"> </span>&lt;max_power_limit&gt;
</pre></div>
</div>
</section>
<section id="boost-settings">
<h3>Boost settings<a class="headerlink" href="#boost-settings" title="Link to this heading">#</a></h3>
<p>Potentially a GPU may support boost levels. First query available boost levels:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>boost-slider<span class="w"> </span>-l
</pre></div>
</div>
<p>If supported, enable the boost slider using one of the available levels for maximum performance:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>boost-slider<span class="w"> </span>--vboost<span class="w"> </span>&lt;max_boost_slider&gt;
</pre></div>
</div>
</section>
</section>
<section id="throughput-benchmarking">
<h2>Throughput Benchmarking<a class="headerlink" href="#throughput-benchmarking" title="Link to this heading">#</a></h2>
<section id="limitations-and-caveats">
<h3>Limitations and Caveats<a class="headerlink" href="#limitations-and-caveats" title="Link to this heading">#</a></h3>
<section id="validated-networks-for-benchmarking">
<h4>Validated Networks for Benchmarking<a class="headerlink" href="#validated-networks-for-benchmarking" title="Link to this heading">#</a></h4>
<p>While <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> should be able to run any network that TensorRT LLM supports, the following are the list
that have been validated extensively and is the same listing as seen on the
<a class="reference internal" href="#./perf-overview.md"><span class="xref myst">Performance Overview</span></a> page.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-hf">meta-llama/Llama-2-7b-hf</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-70b-hf">meta-llama/Llama-2-70b-hf</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/tiiuae/falcon-180B">tiiuae/falcon-180B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/EleutherAI/gpt-j-6b">EleutherAI/gpt-j-6b</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">meta-llama/Meta-Llama-3-8B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B">meta-llama/Llama-3.1-8B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-70B">meta-llama/Meta-Llama-3-70B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-70B">meta-llama/Llama-3.1-70B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-405B">meta-llama/Llama-3.1-405B</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">mistralai/Mixtral-8x7B-v0.1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-v0.1">mistralai/Mistral-7B-v0.1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">meta-llama/Llama-3.1-8B-Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct">meta-llama/Llama-3.1-70B-Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct">meta-llama/Llama-3.1-405B-Instruct</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1-Instruct">mistralai/Mixtral-8x7B-v0.1-Instruct</a></p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> can automatically download the model from Hugging Face Model Hub.
Export your token in the <code class="docutils literal notranslate"><span class="pre">HF_TOKEN</span></code> environment variable.</p>
</div>
</section>
<section id="supported-quantization-modes">
<h4>Supported Quantization Modes<a class="headerlink" href="#supported-quantization-modes" title="Link to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> supports the following quantization modes:</p>
<ul class="simple">
<li><p>None (no quantization applied)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVFP4</span></code></p></li>
</ul>
<p>For more information about quantization, refer to <a class="reference internal" href="../reference/precision.html"><span class="std std-doc">Numerical Precision</span></a> and
the <a class="reference internal" href="../reference/precision.html#support-matrix"><span class="std std-ref">support matrix</span></a> of the supported quantization methods for each network.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Although TensorRT LLM supports more quantization modes than listed above, <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> currently only configures for
a smaller subset.</p>
</div>
</section>
</section>
<section id="preparing-a-dataset">
<h3>Preparing a Dataset<a class="headerlink" href="#preparing-a-dataset" title="Link to this heading">#</a></h3>
<p>The throughput benchmark utilizes a fixed JSON schema to specify requests. The schema is defined as follows:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Key</p></th>
<th class="head text-center"><p>Required</p></th>
<th class="head text-center"><p>Type</p></th>
<th class="head text-left"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">task_id</span></code></p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-left"><p>Unique identifier for the request.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">prompt</span></code></p></td>
<td class="text-center"><p>N*</p></td>
<td class="text-center"><p>String</p></td>
<td class="text-left"><p>Input text for a generation request.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code></p></td>
<td class="text-center"><p>Y*</p></td>
<td class="text-center"><p>List[Integer]</p></td>
<td class="text-left"><p>List of logits that make up the request prompt.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">output_tokens</span></code></p></td>
<td class="text-center"><p>Y</p></td>
<td class="text-center"><p>Integer</p></td>
<td class="text-left"><p>Number of generated tokens for this request.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>* Specifying <code class="docutils literal notranslate"><span class="pre">prompt</span></code> or <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> is required. However, you can not have both prompts and logits (<code class="docutils literal notranslate"><span class="pre">input_ids</span></code>)
defined at the same time. If you specify <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, the <code class="docutils literal notranslate"><span class="pre">prompt</span></code> entry is ignored for request generation.</p>
</div>
<p>Refer to the following examples of valid entries for the benchmark:</p>
<ul>
<li><p>Entries with a human-readable prompt and no logits.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;task_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Generate an infinite response to the following: This is the song that never ends, it goes on and on my friend.&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;output_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;task_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Generate an infinite response to the following: Na, na, na, na&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;output_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Entries which contain logits.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;task_id&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="nt">&quot;input_ids&quot;</span><span class="p">:[</span><span class="mi">863</span><span class="p">,</span><span class="mi">22056</span><span class="p">,</span><span class="mi">25603</span><span class="p">,</span><span class="mi">11943</span><span class="p">,</span><span class="mi">8932</span><span class="p">,</span><span class="mi">13195</span><span class="p">,</span><span class="mi">3132</span><span class="p">,</span><span class="mi">25032</span><span class="p">,</span><span class="mi">21747</span><span class="p">,</span><span class="mi">22213</span><span class="p">],</span><span class="nt">&quot;output_tokens&quot;</span><span class="p">:</span><span class="mi">128</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;task_id&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;input_ids&quot;</span><span class="p">:[</span><span class="mi">14480</span><span class="p">,</span><span class="mi">13598</span><span class="p">,</span><span class="mi">15585</span><span class="p">,</span><span class="mi">6591</span><span class="p">,</span><span class="mi">1252</span><span class="p">,</span><span class="mi">8259</span><span class="p">,</span><span class="mi">30990</span><span class="p">,</span><span class="mi">26778</span><span class="p">,</span><span class="mi">7063</span><span class="p">,</span><span class="mi">30065</span><span class="p">,</span><span class="mi">21764</span><span class="p">,</span><span class="mi">11023</span><span class="p">,</span><span class="mi">1418</span><span class="p">],</span><span class="nt">&quot;output_tokens&quot;</span><span class="p">:</span><span class="mi">128</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Specify each entry on one line.
To simplify passing the data, a complete JSON entry is on each line so that the benchmarker
can simply read a line and assume a complete entry. When creating a dataset, be sure that a complete
JSON entry is on every line.</p>
</div>
<p>In order to prepare a synthetic dataset, you can use the provided script in the <code class="docutils literal notranslate"><span class="pre">benchmarks/cpp</span></code>
directory. For example, to generate a synthetic dataset of 1000 requests with a uniform ISL/OSL of
128/128 for <a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.1-8B">meta-llama/Llama-3.1-8B</a>, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>benchmarks/cpp/prepare_dataset.py<span class="w"> </span>--stdout<span class="w"> </span>--tokenizer<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span>token-norm-dist<span class="w"> </span>--input-mean<span class="w"> </span><span class="m">128</span><span class="w"> </span>--output-mean<span class="w"> </span><span class="m">128</span><span class="w"> </span>--input-stdev<span class="w"> </span><span class="m">0</span><span class="w"> </span>--output-stdev<span class="w"> </span><span class="m">0</span><span class="w"> </span>--num-requests<span class="w"> </span><span class="m">1000</span><span class="w"> </span>&gt;<span class="w"> </span>/tmp/synthetic_128_128.txt
</pre></div>
</div>
</section>
<section id="running-with-the-pytorch-workflow">
<h3>Running with the PyTorch Workflow<a class="headerlink" href="#running-with-the-pytorch-workflow" title="Link to this heading">#</a></h3>
<p>To benchmark the PyTorch backend (<code class="docutils literal notranslate"><span class="pre">tensorrt_llm._torch</span></code>), use the following command with <a class="reference internal" href="#preparing-a-dataset">dataset</a> generated from previous steps. The <code class="docutils literal notranslate"><span class="pre">throughput</span></code> benchmark initializes the backend by tuning against the
dataset provided via <code class="docutils literal notranslate"><span class="pre">--dataset</span></code> (or the other build mode settings described <a class="reference internal" href="#other-build-modes"><span class="xref myst">above</span></a>).
Note that CUDA graph is enabled by default. You can add additional pytorch config with
<code class="docutils literal notranslate"><span class="pre">--extra_llm_api_options</span></code> followed by the path to a YAML file. For more details, please refer to the
help text by running the command with <code class="docutils literal notranslate"><span class="pre">--help</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The command below specifies the <code class="docutils literal notranslate"><span class="pre">--model_path</span></code> option. The model path is optional and used only when you want to run a locally
stored checkpoint. When using <code class="docutils literal notranslate"><span class="pre">--model_path</span></code>, the <code class="docutils literal notranslate"><span class="pre">--model</span></code> is still required for reporting reasons and in order to look up parameters
for build heuristics.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trtllm-bench<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_path<span class="w"> </span>/Ckpt/Path/To/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dataset<span class="w"> </span>/tmp/synthetic_128_128.txt<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--backend<span class="w"> </span>pytorch

<span class="c1"># Example output</span>
&lt;snip<span class="w"> </span>verbose<span class="w"> </span>logging&gt;
<span class="o">===========================================================</span>
<span class="o">=</span><span class="w"> </span>PyTorch<span class="w"> </span><span class="nv">backend</span>
<span class="o">===========================================================</span>
Model:<span class="w">                  </span>meta-llama/Llama-3.1-8B
Model<span class="w"> </span>Path:<span class="w">             </span>/Ckpt/Path/To/Llama-3.1-8B
TensorRT-LLM<span class="w"> </span>Version:<span class="w">   </span><span class="m">0</span>.17.0
Dtype:<span class="w">                  </span>bfloat16
KV<span class="w"> </span>Cache<span class="w"> </span>Dtype:<span class="w">         </span>None
Quantization:<span class="w">           </span><span class="nv">FP8</span>

<span class="o">===========================================================</span>
<span class="o">=</span><span class="w"> </span>WORLD<span class="w"> </span>+<span class="w"> </span>RUNTIME<span class="w"> </span><span class="nv">INFORMATION</span>
<span class="o">===========================================================</span>
TP<span class="w"> </span>Size:<span class="w">                </span><span class="m">1</span>
PP<span class="w"> </span>Size:<span class="w">                </span><span class="m">1</span>
Max<span class="w"> </span>Runtime<span class="w"> </span>Batch<span class="w"> </span>Size:<span class="w"> </span><span class="m">2048</span>
Max<span class="w"> </span>Runtime<span class="w"> </span>Tokens:<span class="w">     </span><span class="m">4096</span>
Scheduling<span class="w"> </span>Policy:<span class="w">      </span>Guaranteed<span class="w"> </span>No<span class="w"> </span>Evict
KV<span class="w"> </span>Memory<span class="w"> </span>Percentage:<span class="w">   </span><span class="m">90</span>.00%
Issue<span class="w"> </span>Rate<span class="w"> </span><span class="o">(</span>req/sec<span class="o">)</span>:<span class="w">   </span><span class="m">7</span>.6753E+14

<span class="o">===========================================================</span>
<span class="o">=</span><span class="w"> </span>PERFORMANCE<span class="w"> </span><span class="nv">OVERVIEW</span>
<span class="o">===========================================================</span>
Number<span class="w"> </span>of<span class="w"> </span>requests:<span class="w">             </span><span class="m">3000</span>
Average<span class="w"> </span>Input<span class="w"> </span>Length<span class="w"> </span><span class="o">(</span>tokens<span class="o">)</span>:<span class="w">  </span><span class="m">128</span>.0000
Average<span class="w"> </span>Output<span class="w"> </span>Length<span class="w"> </span><span class="o">(</span>tokens<span class="o">)</span>:<span class="w"> </span><span class="m">128</span>.0000
Token<span class="w"> </span>Throughput<span class="w"> </span><span class="o">(</span>tokens/sec<span class="o">)</span>:<span class="w">  </span><span class="m">20685</span>.5510
Request<span class="w"> </span>Throughput<span class="w"> </span><span class="o">(</span>req/sec<span class="o">)</span>:<span class="w">   </span><span class="m">161</span>.6059
Total<span class="w"> </span>Latency<span class="w"> </span><span class="o">(</span>ms<span class="o">)</span>:<span class="w">             </span><span class="m">18563</span>.6825
</pre></div>
</div>
<p>When enabling streaming, time to first token (TTFT) and inter-token latency (ITL) metrics will also be recorded.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trtllm-bench<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_path<span class="w"> </span>/Ckpt/Path/To/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dataset<span class="w"> </span>/tmp/synthetic_128_128.txt<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--backend<span class="w"> </span>pytorch
</pre></div>
</div>
<p>Alternatively, users can benchmark the low latency mode:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trtllm-bench<span class="w"> </span>--model<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_path<span class="w"> </span>/Ckpt/Path/To/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>latency<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dataset<span class="w"> </span>/tmp/synthetic_128_128.txt<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--backend<span class="w"> </span>pytorch
</pre></div>
</div>
<section id="benchmarking-with-lora-adapters-in-pytorch-workflow">
<h4>Benchmarking with LoRA Adapters in PyTorch workflow<a class="headerlink" href="#benchmarking-with-lora-adapters-in-pytorch-workflow" title="Link to this heading">#</a></h4>
<p>The PyTorch workflow supports benchmarking with LoRA (Low-Rank Adaptation) adapters. This requires preparing a dataset with LoRA metadata and configuring the LoRA settings.</p>
<p><strong>Preparing LoRA Dataset</strong></p>
<p>Use <code class="docutils literal notranslate"><span class="pre">prepare_dataset.py</span></code> with LoRA-specific options to generate requests with LoRA metadata:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>benchmarks/cpp/prepare_dataset.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--stdout<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--rand-task-id<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--tokenizer<span class="w"> </span>/path/to/tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lora-dir<span class="w"> </span>/path/to/loras<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>token-norm-dist<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num-requests<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--input-mean<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--output-mean<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--input-stdev<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--output-stdev<span class="w"> </span><span class="m">24</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>&gt;<span class="w"> </span>synthetic_lora_data.json
</pre></div>
</div>
<p>Key LoRA options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--lora-dir</span></code>: Parent directory containing LoRA adapter subdirectories named by their task IDs (e.g., <code class="docutils literal notranslate"><span class="pre">0/</span></code>, <code class="docutils literal notranslate"><span class="pre">1/</span></code>, etc.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rand-task-id</span></code>: Range of LoRA task IDs to randomly assign to requests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--task-id</span></code>: Fixed LoRA task ID for all requests (alternative to <code class="docutils literal notranslate"><span class="pre">--rand-task-id</span></code>)</p></li>
</ul>
<p>The generated dataset will include LoRA request metadata. Below is an example of a single such request data entry:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;task_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_ids&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">3452</span><span class="p">,</span><span class="w"> </span><span class="mi">88226</span><span class="p">,</span><span class="w"> </span><span class="mi">102415</span><span class="p">,</span><span class="w"> </span><span class="err">...</span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;output_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">152</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;lora_request&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;lora_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;lora_0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;lora_int_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;lora_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/path/to/loras/0&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>LoRA Configuration</strong></p>
<p>Create an <code class="docutils literal notranslate"><span class="pre">extra-llm-api-options.yaml</span></code> file with LoRA configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">lora_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">lora_dir</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/path/to/loras/0</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/path/to/loras/1</span>
<span class="w">  </span><span class="nt">max_lora_rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
<span class="w">  </span><span class="nt">lora_target_modules</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attn_q</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attn_k</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attn_v</span>
<span class="w">  </span><span class="nt">trtllm_modules_to_hf_modules</span><span class="p">:</span>
<span class="w">    </span><span class="nt">attn_q</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">q_proj</span>
<span class="w">    </span><span class="nt">attn_k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k_proj</span>
<span class="w">    </span><span class="nt">attn_v</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v_proj</span>
</pre></div>
</div>
<p><strong>Running LoRA Benchmark</strong></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>trtllm-bench<span class="w"> </span>--model<span class="w"> </span>/path/to/base/model<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dataset<span class="w"> </span>synthetic_lora_data.json<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--backend<span class="w"> </span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--extra_llm_api_options<span class="w"> </span>extra-llm-api-options.yaml
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The LoRA directory structure should have task-specific subdirectories named by their task IDs (e.g., <code class="docutils literal notranslate"><span class="pre">loras/0/</span></code>, <code class="docutils literal notranslate"><span class="pre">loras/1/</span></code>).
Each subdirectory should contain the LoRA adapter files for that specific task.</p>
</div>
</section>
<section id="running-multi-modal-models-in-the-pytorch-workflow">
<h4>Running multi-modal models in the PyTorch Workflow<a class="headerlink" href="#running-multi-modal-models-in-the-pytorch-workflow" title="Link to this heading">#</a></h4>
<p>To benchmark multi-modal models with PyTorch workflow, you can follow the similar approach as above.</p>
<p>First, prepare the dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">./</span><span class="n">benchmarks</span><span class="o">/</span><span class="n">cpp</span><span class="o">/</span><span class="n">prepare_dataset</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">--</span><span class="n">tokenizer</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="o">-</span><span class="n">VL</span><span class="o">-</span><span class="mi">2</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
  <span class="o">--</span><span class="n">stdout</span> \
  <span class="n">dataset</span> \
  <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">name</span> <span class="n">lmms</span><span class="o">-</span><span class="n">lab</span><span class="o">/</span><span class="n">MMMU</span> \
  <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">split</span> <span class="n">test</span> \
  <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">image</span><span class="o">-</span><span class="n">key</span> <span class="n">image</span> \
  <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">prompt</span><span class="o">-</span><span class="n">key</span> <span class="n">question</span> \
  <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">requests</span> <span class="mi">10</span> \
  <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="nb">len</span><span class="o">-</span><span class="n">dist</span> <span class="mi">128</span><span class="p">,</span><span class="mi">5</span> <span class="o">&gt;</span> <span class="n">mm_data</span><span class="o">.</span><span class="n">jsonl</span>
</pre></div>
</div>
<p>It will download the media files to <code class="docutils literal notranslate"><span class="pre">/tmp</span></code> directory and prepare the dataset with their paths. Note that the <code class="docutils literal notranslate"><span class="pre">prompt</span></code> fields are texts and not tokenized ids. This is due to the fact that
the <code class="docutils literal notranslate"><span class="pre">prompt</span></code> and the media (image/video) are processed by a preprocessor for multimodal files.</p>
<p>Sample dataset for multimodal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;task_id&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span><span class="s2">&quot;Brahma Industries sells vinyl replacement windows to home improvement retailers nationwide. The national sales manager believes that if they invest an additional $25,000 in advertising, they would increase sales volume by 10,000 units. &lt;image 1&gt; What is the total contribution margin?&quot;</span><span class="p">,</span><span class="s2">&quot;media_paths&quot;</span><span class="p">:[</span><span class="s2">&quot;/tmp/tmp9so41y3r.jpg&quot;</span><span class="p">],</span><span class="s2">&quot;output_tokens&quot;</span><span class="p">:</span><span class="mi">126</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;task_id&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span><span class="s2">&quot;Let us compute for the missing amounts under work in process inventory, what is the cost of goods manufactured? &lt;image 1&gt;&quot;</span><span class="p">,</span><span class="s2">&quot;media_paths&quot;</span><span class="p">:[</span><span class="s2">&quot;/tmp/tmpowsrb_f4.jpg&quot;</span><span class="p">],</span><span class="s2">&quot;output_tokens&quot;</span><span class="p">:</span><span class="mi">119</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;task_id&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span><span class="s2">&quot;Tsuji is reviewing the price of a 3-month Japanese yen/U.S. dollar currency futures contract, using the currency and interest rate data shown below. Because the 3-month Japanese interest rate has just increased to .50%, Itsuji recognizes that an arbitrage opportunity exists nd decides to borrow $1 million U.S. dollars to purchase Japanese yen. Calculate the yen arbitrage profit from Itsuji&#39;s strategy, using the following data: &lt;image 1&gt; &quot;</span><span class="p">,</span><span class="s2">&quot;media_paths&quot;</span><span class="p">:[</span><span class="s2">&quot;/tmp/tmpxhdvasex.jpg&quot;</span><span class="p">],</span><span class="s2">&quot;output_tokens&quot;</span><span class="p">:</span><span class="mi">126</span><span class="p">}</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Run the benchmark:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trtllm</span><span class="o">-</span><span class="n">bench</span> <span class="o">--</span><span class="n">model</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="o">-</span><span class="n">VL</span><span class="o">-</span><span class="mi">2</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
  <span class="n">throughput</span> \
  <span class="o">--</span><span class="n">dataset</span> <span class="n">mm_data</span><span class="o">.</span><span class="n">jsonl</span> \
  <span class="o">--</span><span class="n">backend</span> <span class="n">pytorch</span> \
  <span class="o">--</span><span class="n">num_requests</span> <span class="mi">10</span> \
  <span class="o">--</span><span class="n">max_batch_size</span> <span class="mi">4</span> \
  <span class="o">--</span><span class="n">modality</span> <span class="n">image</span>
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">REQUEST</span> <span class="n">DETAILS</span>
<span class="o">===========================================================</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">requests</span><span class="p">:</span>             <span class="mi">10</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">concurrent</span> <span class="n">requests</span><span class="p">:</span>  <span class="mf">5.3019</span>
<span class="n">Average</span> <span class="n">Input</span> <span class="n">Length</span> <span class="p">(</span><span class="n">tokens</span><span class="p">):</span>  <span class="mf">411.6000</span>
<span class="n">Average</span> <span class="n">Output</span> <span class="n">Length</span> <span class="p">(</span><span class="n">tokens</span><span class="p">):</span> <span class="mf">128.7000</span>
<span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">WORLD</span> <span class="o">+</span> <span class="n">RUNTIME</span> <span class="n">INFORMATION</span>
<span class="o">===========================================================</span>
<span class="n">TP</span> <span class="n">Size</span><span class="p">:</span>                <span class="mi">1</span>
<span class="n">PP</span> <span class="n">Size</span><span class="p">:</span>                <span class="mi">1</span>
<span class="n">EP</span> <span class="n">Size</span><span class="p">:</span>                <span class="kc">None</span>
<span class="n">Max</span> <span class="n">Runtime</span> <span class="n">Batch</span> <span class="n">Size</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Max</span> <span class="n">Runtime</span> <span class="n">Tokens</span><span class="p">:</span>     <span class="mi">12288</span>
<span class="n">Scheduling</span> <span class="n">Policy</span><span class="p">:</span>      <span class="n">GUARANTEED_NO_EVICT</span>
<span class="n">KV</span> <span class="n">Memory</span> <span class="n">Percentage</span><span class="p">:</span>   <span class="mf">90.00</span><span class="o">%</span>
<span class="n">Issue</span> <span class="n">Rate</span> <span class="p">(</span><span class="n">req</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>   <span class="mf">1.4117E+17</span>

<span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">PERFORMANCE</span> <span class="n">OVERVIEW</span>
<span class="o">===========================================================</span>
<span class="n">Request</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">req</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>                     <span class="mf">1.4439</span>
<span class="n">Total</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>             <span class="mf">185.8351</span>
<span class="n">Per</span> <span class="n">User</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="o">/</span><span class="n">user</span><span class="p">):</span>     <span class="mf">38.1959</span>
<span class="n">Per</span> <span class="n">GPU</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="o">/</span><span class="n">gpu</span><span class="p">):</span>       <span class="mf">185.8351</span>
<span class="n">Total</span> <span class="n">Token</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>              <span class="mf">780.1607</span>
<span class="n">Total</span> <span class="n">Latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                               <span class="mf">6925.4963</span>
<span class="n">Average</span> <span class="n">request</span> <span class="n">latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                     <span class="mf">3671.8441</span>

<span class="o">--</span> <span class="n">Request</span> <span class="n">Latency</span> <span class="n">Breakdown</span> <span class="p">(</span><span class="n">ms</span><span class="p">)</span> <span class="o">-----------------------</span>

<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">P50</span>    <span class="p">:</span> <span class="mf">3936.3022</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">P90</span>    <span class="p">:</span> <span class="mf">5514.4701</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">P95</span>    <span class="p">:</span> <span class="mf">5514.4701</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">P99</span>    <span class="p">:</span> <span class="mf">5514.4701</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">MINIMUM</span><span class="p">:</span> <span class="mf">2397.1047</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">MAXIMUM</span><span class="p">:</span> <span class="mf">5514.4701</span>
<span class="p">[</span><span class="n">Latency</span><span class="p">]</span> <span class="n">AVERAGE</span><span class="p">:</span> <span class="mf">3671.8441</span>

<span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">DATASET</span> <span class="n">DETAILS</span>
<span class="o">===========================================================</span>
<span class="n">Dataset</span> <span class="n">Path</span><span class="p">:</span>         <span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">tensorrt_llm</span><span class="o">/</span><span class="n">mm_data</span><span class="o">.</span><span class="n">jsonl</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">Sequences</span><span class="p">:</span>  <span class="mi">10</span>

<span class="o">--</span> <span class="n">Percentiles</span> <span class="n">statistics</span> <span class="o">---------------------------------</span>

        <span class="n">Input</span>              <span class="n">Output</span>           <span class="n">Seq</span><span class="o">.</span> <span class="n">Length</span>
<span class="o">-----------------------------------------------------------</span>
<span class="n">MIN</span><span class="p">:</span>   <span class="mf">167.0000</span>           <span class="mf">119.0000</span>           <span class="mf">300.0000</span>
<span class="n">MAX</span><span class="p">:</span>  <span class="mf">1059.0000</span>           <span class="mf">137.0000</span>          <span class="mf">1178.0000</span>
<span class="n">AVG</span><span class="p">:</span>   <span class="mf">411.6000</span>           <span class="mf">128.7000</span>           <span class="mf">540.3000</span>
<span class="n">P50</span><span class="p">:</span>   <span class="mf">299.0000</span>           <span class="mf">128.0000</span>           <span class="mf">427.0000</span>
<span class="n">P90</span><span class="p">:</span>  <span class="mf">1059.0000</span>           <span class="mf">137.0000</span>          <span class="mf">1178.0000</span>
<span class="n">P95</span><span class="p">:</span>  <span class="mf">1059.0000</span>           <span class="mf">137.0000</span>          <span class="mf">1178.0000</span>
<span class="n">P99</span><span class="p">:</span>  <span class="mf">1059.0000</span>           <span class="mf">137.0000</span>          <span class="mf">1178.0000</span>
<span class="o">===========================================================</span>
</pre></div>
</div>
<p><strong>Notes and Limitations</strong>:</p>
<ul class="simple">
<li><p>Only image datasets are supported for now.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output-len-dist</span></code> is a required argument for multimodal datasets.</p></li>
<li><p>Tokenizer is unused during the prepare step but it is still a required argument.</p></li>
<li><p>Since the images are converted to tokens when the model is run, <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> uses a default large value for the maximum input sequence length when setting up the execution settings.
You can also modify the behavior by specifying a different value with the flag <code class="docutils literal notranslate"><span class="pre">--max_input_len</span></code> that suits your use-case.</p></li>
</ul>
</section>
<section id="quantization-in-the-pytorch-flow">
<h4>Quantization in the PyTorch Flow<a class="headerlink" href="#quantization-in-the-pytorch-flow" title="Link to this heading">#</a></h4>
<p>To run a quantized benchmark with <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> utilizing the PyTorch flow, you will need to use a pre-quantized
checkpoint. For the Llama-3.1 models, TensorRT LLM provides the following checkpoints via HuggingFace:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8"><code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3.1-8B-Instruct-FP8</span></code></a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/nvidia/Llama-3.1-70B-Instruct-FP8"><code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3.1-70B-Instruct-FP8</span></code></a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP8"><code class="docutils literal notranslate"><span class="pre">nvidia/Llama-3.1-405B-Instruct-FP8</span></code></a></p></li>
</ul>
<p>To understand more about how to quantize your own checkpoints, refer to ModelOpt <a class="reference external" href="https://nvidia.github.io/TensorRT-Model-Optimizer/getting_started/3_quantization.html">documentation</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> utilizes the <code class="docutils literal notranslate"><span class="pre">hf_quant_config.json</span></code> file present in the pre-quantized checkpoints above. The configuration
file is present in checkpoints quantized with <a class="reference external" href="https://github.com/NVIDIA/TensorRT-Model-Optimizer">TensorRT Model Optimizer</a>
and describes the compute and KV cache quantization that checkpoint was compiled with. For example, from the checkpoints
above:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;producer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;modelopt&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0.23.0rc1&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;quantization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;quant_algo&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;FP8&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;kv_cache_quant_algo&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The checkpoints above are quantized to run with a compute precision of <code class="docutils literal notranslate"><span class="pre">FP8</span></code> and default to no KV cache quantization (full
<code class="docutils literal notranslate"><span class="pre">FP16</span></code> cache). When running <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span> <span class="pre">throughput</span></code>. The benchmark will select a KV cache quantization that is best suited
for the compute precision in the checkpoint automatically if <code class="docutils literal notranslate"><span class="pre">kv_cache_quant_algo</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">null</span></code>, otherwise it will
be forced to match the specified non-null KV cache quantization. The following are the mappings that <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> will
follow when a checkpoint does not specify a KV cache quantization algorithm:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Checkpoint Compute Quant</p></th>
<th class="head"><p>Checkpoint KV Cache Quant</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code></p></th>
<th class="head"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">null</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">null</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">null</span></code></p></td>
<td><p>In this case, a quantization config doesn’t exist.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p>Matches the checkpoint</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">null</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">FP8</span></code> via benchmark</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NVFP4</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">null</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">FP8</span></code></p></td>
<td><p>Set to <code class="docutils literal notranslate"><span class="pre">FP8</span></code> via benchmark</p></td>
</tr>
</tbody>
</table>
</div>
<p>If you would like to force the KV cache quantization, you can specify the following in the YAML file to force the precision
when the checkpoint precision is <code class="docutils literal notranslate"><span class="pre">null</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">kv_cache_dtype</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;fp8&quot;</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The two valid values for <code class="docutils literal notranslate"><span class="pre">kv_cache_dtype</span></code> are <code class="docutils literal notranslate"><span class="pre">auto</span></code> and <code class="docutils literal notranslate"><span class="pre">fp8</span></code>.</p>
</div>
</section>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="perf-analysis.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Performance Analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="ci-overview.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Continuous Integration Overview</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#before-benchmarking">Before Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#persistence-mode">Persistence mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-clock-management">GPU Clock Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-power-limits">Set power limits</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#boost-settings">Boost settings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#throughput-benchmarking">Throughput Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-and-caveats">Limitations and Caveats</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#validated-networks-for-benchmarking">Validated Networks for Benchmarking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-quantization-modes">Supported Quantization Modes</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-a-dataset">Preparing a Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-with-the-pytorch-workflow">Running with the PyTorch Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-with-lora-adapters-in-pytorch-workflow">Benchmarking with LoRA Adapters in PyTorch workflow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#running-multi-modal-models-in-the-pytorch-workflow">Running multi-modal models in the PyTorch Workflow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-in-the-pytorch-flow">Quantization in the PyTorch Flow</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2025, NVidia.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
<div class="extra_footer">
  
  <p>Last updated on September 15, 2025.</p>
  
  <p>This page is generated by TensorRT-LLM commit <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/0c9430e">0c9430e</a>.</p>
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>