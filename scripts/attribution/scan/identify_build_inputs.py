#!/usr/bin/env python3
"""Build Input Identifier for TensorRT-LLM.

Identifies all input files used by the build system by scanning:
  - setup.py package_data to determine files to be bundled
  - Binaries (.so files) and their dynamic dependencies
  - Headers transitively included (via ninja -t inputs)
  - Libraries linked into binaries (via linker .d files)

This script focuses solely on collecting input files without performing
dependency resolution. For dependency mapping, use map_dependencies.py.

Only files that are transitively included in the wheel are collected.
Test code and other non-shipped artifacts are excluded.

Output:
  - YAML to stdout (default)
  - Or YAML file (--output)

Usage:
  python identify_build_inputs.py --build-dir build/
  python identify_build_inputs.py --build-dir build/ --output artifacts.yml
"""

import argparse
import os
import re
import shutil
import subprocess
import sys
from concurrent.futures import ProcessPoolExecutor
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import yaml

# Header file extensions to recognize
HEADER_EXTENSIONS = {
    ".h",
    ".hpp",
    ".hxx",
    ".h++",
    ".hh",
    ".cuh",  # CUDA headers
    ".inc",
    ".inl",
    ".tpp",  # Template implementations
}


def _parse_depfile_headers(depfile_path: Path) -> Set[str]:
    """Parse a compiler dependency (.d) file to extract header paths.

    This is a module-level function for use with ProcessPoolExecutor.

    Args:
        depfile_path: Path to the .d file to parse

    Returns:
        Set of header file paths found in the depfile
    """
    headers: Set[str] = set()
    content = depfile_path.read_text(encoding="utf-8", errors="ignore")
    # Remove line continuations (backslash + newline)
    content = content.replace("\\\n", " ")
    # Split by colon to get dependencies part (after the target)
    if ":" in content:
        _, deps_part = content.split(":", 1)
        for token in deps_part.split():
            token = token.strip()
            if token:
                _, ext = os.path.splitext(token.lower())
                if ext in HEADER_EXTENSIONS:
                    headers.add(token)
    return headers


@dataclass
class Artifact:
    """Represents a discovered build artifact (header, library, or binary)."""

    path: str  # Canonical resolved path
    type: str  # 'header', 'library', 'binary'
    source: str  # Which file discovered it (e.g., "foo.cpp.o.d", "link.d")
    context_dir: Optional[str] = None  # For relative path resolution
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return asdict(self)


class ArtifactCollector:
    """Collects artifacts from build directory, filtered by setup.py contents.

    Uses setup.py package_data to determine which binaries to track, then
    uses ninja to trace transitive dependencies, ensuring only files that
    contribute to the shipped wheel are collected.

    Args:
        build_dir: Path to build directory to scan
        num_workers: Number of parallel workers for file parsing (default: 1)
    """

    def __init__(self, build_dir: Path, num_workers: int = 1):
        self.build_dir = build_dir
        self.num_workers = num_workers

    def collect_all(self) -> List[Artifact]:
        """Collect all artifacts transitively included in wheels.

        Algorithm:
          1. Parse setup.py to get package_data patterns
          2. Find build artifacts (.so files) matching those patterns
          3. Scan binaries for NEEDED dependencies
          4. Use ninja -t inputs to get all transitive inputs (headers)
          5. Parse linker dependency files (link.d) for library dependencies
          6. Return combined deduplicated list

        The link.d files are generated by the linker via -Wl,--dependency-file
        and contain the exact resolved paths of all libraries used during linking.

        Raises:
            SystemExit: If setup.py not found, ninja not available, or ninja fails
        """
        artifacts: List[Artifact] = []

        # Check ninja is available
        if not shutil.which("ninja"):
            print("Error: ninja not found in PATH", file=sys.stderr)
            sys.exit(1)

        # Step 1: Parse setup.py to determine expected wheel contents
        setup_py_path = self._find_setup_py()
        if not setup_py_path:
            print("Error: setup.py not found in repository", file=sys.stderr)
            sys.exit(1)

        package_data_patterns = self._extract_package_data_from_setup(setup_py_path)
        if not package_data_patterns:
            print("Error: Could not extract package_data from setup.py", file=sys.stderr)
            sys.exit(1)

        # Step 2: Find build artifacts that match package_data patterns
        wheel_so_targets = self._find_so_targets_from_patterns(package_data_patterns)

        if not wheel_so_targets:
            print(
                "Error: Could not find .so build artifacts matching setup.py patterns",
                file=sys.stderr,
            )
            sys.exit(1)

        # Step 3: Scan for binary dependencies in built .so files
        for so_target in wheel_so_targets:
            so_path = (
                self.build_dir / so_target if not Path(so_target).is_absolute() else Path(so_target)
            )
            if so_path.exists():
                needed_libs = self._get_needed_libraries(so_path)
                artifacts.append(
                    Artifact(
                        path=str(so_target),
                        type="binary",
                        source="setup.py",
                        metadata={"needed": needed_libs},
                    )
                )
                for needed_lib in needed_libs:
                    artifacts.append(
                        Artifact(
                            path=needed_lib,
                            type="library",
                            source="setup.py",
                            metadata={
                                "from_binary": str(so_target),
                                "dynamic_dependency": True,
                            },
                        )
                    )

        # Step 4: Use ninja to get transitive inputs (source files, object files)
        ninja_inputs = self._get_all_ninja_inputs(wheel_so_targets)

        if not ninja_inputs:
            print(
                "Error: ninja -t inputs returned no results for wheel targets",
                file=sys.stderr,
            )
            sys.exit(1)

        # Parse .d files to get ALL headers (including system headers)
        # The .d files contain complete dependency information generated by the
        # compiler, including glibc, libstdc++, CUDA, and all other headers.
        depfile_headers = self._get_headers_from_depfiles(ninja_inputs)
        for header_path in depfile_headers:
            artifacts.append(self._create_header_artifact(header_path, source="depfile (.d)"))

        # Step 5: Parse linker dependency files (link.d) for library dependencies
        # These files are generated by the linker and contain resolved library paths
        link_depfiles = self._find_link_depfiles(wheel_so_targets)
        artifacts.extend(self._get_libraries_from_link_depfiles(link_depfiles))

        # Step 6: Deduplicate by path
        return self._deduplicate_artifacts(artifacts)

    def _find_setup_py(self) -> Optional[Path]:
        """Find setup.py in the repository root.

        Searches upward from build_dir to find the repository root.
        """
        current = self.build_dir.resolve()

        # Try current directory first
        setup_py = current / "setup.py"
        if setup_py.exists():
            return setup_py

        # Search parent directories
        for parent in current.parents:
            setup_py = parent / "setup.py"
            if setup_py.exists():
                return setup_py

        return None

    def _extract_package_data_from_setup(self, setup_py_path: Path) -> List[str]:
        """Extract package_data patterns from setup.py.

        Returns a list of file patterns that should be included in the wheel.
        """
        patterns = []

        content = setup_py_path.read_text(encoding="utf-8")

        # Look for package_data assignment
        # Match both inline list and multi-line list definitions
        in_package_data = False
        in_list = False
        bracket_depth = 0

        for line in content.split("\n"):
            stripped = line.strip()

            # Start of package_data section
            if "package_data" in stripped and "=" in stripped:
                in_package_data = True
                # Check if list starts on same line
                if "[" in stripped:
                    in_list = True
                    bracket_depth = stripped.count("[") - stripped.count("]")

            if in_package_data and in_list:
                # Extract string literals from this line
                import re

                # Match single or double quoted strings
                string_matches = re.findall(r"['\"]([^'\"]+)['\"]", stripped)
                patterns.extend(string_matches)

                # Track bracket depth
                bracket_depth += stripped.count("[") - stripped.count("]")

                # End of list
                if bracket_depth <= 0:
                    break

            if in_package_data and not in_list and "[" in stripped:
                in_list = True
                bracket_depth = stripped.count("[") - stripped.count("]")

        # Filter to .so patterns only (binaries)
        so_patterns = [p for p in patterns if ".so" in p or "lib" in p]

        return so_patterns

    def _find_so_targets_from_patterns(self, patterns: List[str]) -> List[str]:
        """Find .so files in build directory matching package_data patterns.

        Returns paths relative to build_dir suitable for ninja queries.
        """
        import fnmatch

        so_targets: List[str] = []
        seen: Set[str] = set()

        # Search for .so files in build directory
        for so_file in self.build_dir.rglob("*.so*"):
            if not so_file.is_file():
                continue

            basename = so_file.name

            # Check if basename matches any pattern
            for pattern in patterns:
                # Extract the filename part from pattern (e.g., 'libs/*.so' -> '*.so')
                pattern_basename = os.path.basename(pattern)

                if fnmatch.fnmatch(basename, pattern_basename):
                    try:
                        rel_path = so_file.relative_to(self.build_dir)
                        rel_path_str = str(rel_path)
                        if rel_path_str not in seen:
                            seen.add(rel_path_str)
                            so_targets.append(rel_path_str)
                    except ValueError:
                        # File is outside build_dir, use absolute path
                        abs_path = str(so_file)
                        if abs_path not in seen:
                            seen.add(abs_path)
                            so_targets.append(abs_path)
                    break

        return so_targets

    @staticmethod
    def _get_needed_libraries(binary_path: Path) -> List[str]:
        """Extract NEEDED entries from ELF binary using readelf."""
        needed = []

        result = subprocess.run(
            ["readelf", "-d", str(binary_path)], capture_output=True, text=True, timeout=10
        )

        if result.returncode == 0:
            for line in result.stdout.split("\n"):
                if "(NEEDED)" in line and "Shared library:" in line:
                    match = re.search(r"\[([^\]]+)\]", line)
                    if match:
                        needed.append(match.group(1))

        return needed

    def _get_ninja_inputs(self, target: str) -> List[str]:
        """Get transitive inputs for a single ninja target.

        Runs: ninja -t inputs <target>
        Returns list of input file paths (relative to build directory).
        """
        try:
            result = subprocess.run(
                ["ninja", "-t", "inputs", target],
                cwd=str(self.build_dir),
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                return [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]
            else:
                # Target might not exist in ninja build
                return []

        except subprocess.TimeoutExpired:
            print(f"Error: ninja -t inputs timed out for {target}", file=sys.stderr)
            sys.exit(1)
        except FileNotFoundError:
            print("Error: ninja not found", file=sys.stderr)
            sys.exit(1)

    def _get_all_ninja_inputs(self, targets: List[str]) -> Set[str]:
        """Get all transitive inputs for ninja targets.

        Uses 'ninja -t inputs' to get the complete list of files
        required to build the given targets, including headers from depfiles.
        """
        all_inputs: Set[str] = set()

        for target in targets:
            inputs = self._get_ninja_inputs(target)
            all_inputs.update(inputs)

        return all_inputs

    def _is_header_file(self, path: str) -> bool:
        """Check if a file path is a header file based on extension."""
        _, ext = os.path.splitext(path.lower())
        return ext in HEADER_EXTENSIONS

    def _is_library_file(self, path: str) -> bool:
        """Check if a path looks like a library file (.so, .a, or .so.N)."""
        # Check for static libraries
        if path.endswith(".a"):
            return True
        # Check for shared libraries (.so or .so.N.N.N)
        basename = os.path.basename(path)
        if re.search(r"\.so(\.\d+)*$", basename):
            return True
        return False

    def _parse_depfile_tokens(self, depfile: Path) -> Set[str]:
        r"""Parse a Make-style dependency (.d) file and return all dependency tokens.

        The format is:
            target: dep1 dep2 dep3 \
                dep4 dep5 ...

        This is used for both compiler .d files (headers) and linker .d files (libraries).

        Args:
            depfile: Path to the .d file to parse

        Returns:
            Set of all dependency tokens (file paths) from the depfile
        """
        tokens: Set[str] = set()

        content = depfile.read_text(encoding="utf-8", errors="ignore")

        # Remove line continuations (backslash + newline)
        content = content.replace("\\\n", " ")

        # Split by colon to get dependencies part (after the target)
        if ":" in content:
            _, deps_part = content.split(":", 1)

            for token in deps_part.split():
                token = token.strip()
                if token:
                    tokens.add(token)

        return tokens

    def _parse_header_depfile(self, depfile: Path) -> Set[str]:
        """Parse a compiler dependency (.d) file to extract header paths."""
        return {t for t in self._parse_depfile_tokens(depfile) if self._is_header_file(t)}

    def _parse_link_depfile(self, depfile: Path) -> Set[str]:
        """Parse a linker dependency (.d) file to extract library paths."""
        return {t for t in self._parse_depfile_tokens(depfile) if self._is_library_file(t)}

    def _get_headers_from_depfiles(self, ninja_inputs: Set[str]) -> Set[str]:
        """Extract all headers from compiler dependency (.d) files.

        The .d files are generated by the compiler (-MD/-MMD flags) and contain
        all headers transitively included by each source file, including system
        headers like glibc, libstdc++, CUDA headers, etc.

        Args:
            ninja_inputs: Set of ninja inputs to find associated .d files for

        Returns:
            Set of header file paths found in depfiles
        """
        # Collect all depfiles to process
        depfiles_to_process: Set[Path] = set()

        # Find .d files associated with .o files in ninja inputs
        # Only scan depfiles for object files that are actually part of wheel targets,
        # avoiding false positives from test/benchmark builds
        for input_path in ninja_inputs:
            if input_path.endswith(".o"):
                # Depfile is typically at the same path with .d extension
                depfile_path = self.build_dir / (input_path + ".d")
                if depfile_path.exists():
                    depfiles_to_process.add(depfile_path)

        # Parse depfiles (parallel if num_workers > 1)
        headers: Set[str] = set()
        depfile_list = list(depfiles_to_process)

        if self.num_workers > 1 and len(depfile_list) > 10:
            with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
                chunksize = max(1, len(depfile_list) // self.num_workers)
                for result_headers in executor.map(
                    _parse_depfile_headers, depfile_list, chunksize=chunksize
                ):
                    headers.update(result_headers)
        else:
            # Sequential processing
            for depfile in depfile_list:
                headers.update(self._parse_header_depfile(depfile))

        return headers

    def _create_header_artifact(
        self, input_path: str, source: str = "ninja -t inputs"
    ) -> Optional[Artifact]:
        """Create a header artifact from a file path.

        Args:
            input_path: Path to the header file (absolute or relative to build dir)
            source: Source description for the artifact (e.g., "ninja -t inputs", "depfile (.d)")
        """
        # Resolve path relative to build directory
        if os.path.isabs(input_path):
            full_path = input_path
        else:
            full_path = os.path.join(self.build_dir, input_path)

        canonical_path = os.path.realpath(full_path)

        return Artifact(
            path=canonical_path,
            type="header",
            source=source,
            metadata={
                "original_path": input_path,
                "path_exists": os.path.exists(canonical_path),
            },
        )

    def _get_libraries_from_link_depfiles(self, link_depfiles: List[Path]) -> List[Artifact]:
        """Extract library artifacts from linker dependency files (link.d).

        The link.d files are generated by the linker via -Wl,--dependency-file
        and have the same Make-style format as compiler .d files. They contain
        the exact resolved paths of all libraries used during linking.

        Args:
            link_depfiles: List of paths to link.d files

        Returns:
            List of library Artifact objects
        """
        artifacts: List[Artifact] = []

        for depfile in link_depfiles:
            library_paths = self._parse_link_depfile(depfile)
            for lib_path in library_paths:
                # Only collect static libraries (.a)
                if not lib_path.endswith(".a"):
                    continue
                try:
                    # Resolve relative paths relative to build directory
                    if not os.path.isabs(lib_path):
                        lib_path = str(self.build_dir / lib_path)
                    canonical_path = os.path.realpath(lib_path)
                    artifacts.append(
                        Artifact(
                            path=canonical_path,
                            type="library",
                            source=f"link.d ({depfile.name})",
                            metadata={
                                "path_exists": os.path.exists(canonical_path),
                            },
                        )
                    )
                except Exception:
                    continue

        return artifacts

    def _find_link_depfiles(self, wheel_targets: List[str]) -> List[Path]:
        """Find linker dependency files for wheel targets.

        These files are generated by the linker via -Wl,--dependency-file and
        contain the exact resolved paths of all libraries used during linking.

        Supports two naming conventions:
        - CMakeFiles/<target>.dir/link.d (older CMake convention)
        - <output>.d (e.g., target.so.d) - generated by our CMake config

        Args:
            ninja_inputs: Set of ninja input paths
            wheel_targets: List of wheel .so targets

        Returns:
            List of paths to linker dependency files
        """
        link_depfiles: Set[Path] = set()

        # Find <output>.d files adjacent to wheel .so targets
        # These are generated by -Wl,--dependency-file,<TARGET>.d in CMakeLists.txt
        for so_target in wheel_targets:
            so_path = (
                self.build_dir / so_target if not Path(so_target).is_absolute() else Path(so_target)
            )
            depfile = Path(str(so_path) + ".d")
            if depfile.exists():
                link_depfiles.add(depfile)

        return list(link_depfiles)

    def _deduplicate_artifacts(self, artifacts: List[Artifact]) -> List[Artifact]:
        """Remove duplicate artifacts by path."""
        seen: Set[str] = set()
        unique_artifacts: List[Artifact] = []
        for artifact in artifacts:
            if artifact.path not in seen:
                seen.add(artifact.path)
                unique_artifacts.append(artifact)
        return unique_artifacts


def collect_build_inputs(
    build_dir: Path, strict: bool = False, num_workers: int = 1
) -> List[Artifact]:
    """Collect all build input artifacts from the given build directory.

    This is the main API function for programmatic use.
    Only collects files that are transitively included in wheels,
    as determined by setup.py package_data patterns.

    All linker flags (-l<name>) are resolved to actual library paths during
    collection. Unresolved linker flags are not included in the results.

    Args:
        build_dir: Path to build directory to scan
        strict: If True, fail on missing files; if False, warn only (default: False)
        num_workers: Number of parallel workers for file parsing (default: 1)

    Returns:
        List of Artifact objects representing all discovered build inputs

    Raises:
        SystemExit: If setup.py not found, ninja not available, or (strict=True) files missing
    """
    collector = ArtifactCollector(build_dir, num_workers=num_workers)
    artifacts = collector.collect_all()

    # Check for missing files (skip relative paths like binary names from wheels)
    missing_files = []
    for artifact in artifacts:
        if not os.path.isabs(artifact.path):
            continue
        if not os.path.exists(artifact.path):
            missing_files.append(artifact.path)

    if missing_files:
        msg = f"Warning: {len(missing_files)} input files not found (from depfiles):"
        print(msg, file=sys.stderr)
        for path in missing_files[:10]:  # Show first 10
            print(f"  {path}", file=sys.stderr)
        if len(missing_files) > 10:
            print(f"  ... and {len(missing_files) - 10} more", file=sys.stderr)
        print(
            "  Note: This may happen when the build was done in a different environment.",
            file=sys.stderr,
        )
        if strict:
            sys.exit(1)

    return artifacts


def get_input_file_paths(build_dir: Path, num_workers: int = 1) -> List[str]:
    """Get a list of input file paths from the build directory.

    Convenience function that returns just the file paths suitable for
    passing to the attribute generate command.

    Args:
        build_dir: Path to build directory to scan
        num_workers: Number of parallel workers for file parsing (default: 1)

    Returns:
        List of absolute file paths

    Raises:
        SystemExit: If setup.py not found, ninja not available, or any files missing
    """
    artifacts = collect_build_inputs(build_dir, strict=True, num_workers=num_workers)

    # Extract absolute file paths (skip relative paths like binary names from wheels)
    paths: List[str] = []
    seen: Set[str] = set()
    for artifact in artifacts:
        path = artifact.path
        # Skip relative paths (binary names from wheels)
        if not os.path.isabs(path):
            continue
        # Deduplicate
        if path not in seen:
            seen.add(path)
            paths.append(path)

    return paths


def main():
    """Main entry point for build input identification."""
    parser = argparse.ArgumentParser(
        description="Identify build input files from TensorRT-LLM build artifacts",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Output artifacts as YAML to stdout
  python identify_build_inputs.py --build-dir build/

  # Output artifacts to YAML file
  python identify_build_inputs.py --build-dir build/ --output artifacts.yml
        """,
    )

    parser.add_argument(
        "--build-dir",
        type=Path,
        default=Path(__file__).parents[3] / "cpp" / "build",
        help="Build directory to scan for artifacts (default: cpp/build/)",
    )

    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        help="Output file for artifact info (YAML format). "
        "If not specified, outputs YAML to stdout.",
    )

    parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="Suppress progress messages",
    )

    parser.add_argument(
        "--strict",
        action="store_true",
        help="Fail if any input files are missing (default: warn only)",
    )

    args = parser.parse_args()

    # Validate inputs
    if not args.build_dir.exists():
        print(f"Error: Build directory not found: {args.build_dir}", file=sys.stderr)
        sys.exit(1)

    if not args.quiet:
        print(f"Scanning build directory: {args.build_dir}", file=sys.stderr)

    # Collect artifacts
    artifacts = collect_build_inputs(args.build_dir, strict=args.strict)

    if not args.quiet:
        print(f"Found {len(artifacts)} unique artifacts", file=sys.stderr)
        print(f"  - Headers: {sum(1 for a in artifacts if a.type == 'header')}", file=sys.stderr)
        print(f"  - Libraries: {sum(1 for a in artifacts if a.type == 'library')}", file=sys.stderr)
        print(f"  - Binaries: {sum(1 for a in artifacts if a.type == 'binary')}", file=sys.stderr)

    # Build output data
    output_data = {
        "summary": {
            "total": len(artifacts),
            "headers": sum(1 for a in artifacts if a.type == "header"),
            "libraries": sum(1 for a in artifacts if a.type == "library"),
            "binaries": sum(1 for a in artifacts if a.type == "binary"),
        },
        "artifacts": [a.to_dict() for a in artifacts],
    }

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, "w") as f:
            yaml.dump(output_data, f, default_flow_style=False, sort_keys=False)
        if not args.quiet:
            print(f"Written to: {args.output}", file=sys.stderr)
    else:
        # Output YAML to stdout
        print(yaml.dump(output_data, default_flow_style=False, sort_keys=False))


if __name__ == "__main__":
    main()
