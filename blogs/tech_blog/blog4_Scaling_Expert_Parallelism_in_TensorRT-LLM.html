

<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP) &#8212; TensorRT-LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/nvidia-sphinx-theme.css?v=df3ac72c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=95073da6" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=65e89d2a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = './_static/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.1.0rc2';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Disaggregated Serving in TensorRT-LLM" href="blog5_Disaggregated_Serving_in_TensorRT-LLM.html" />
    <link rel="prev" title="Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers" href="blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.1.0rc2" />


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT-LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT-LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT-LLM</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick-start-guide.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../key-features.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">PyTorch Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../installation/containers.html">Pre-built release container images on NGC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation/linux.html">Installing on Linux via <code class="docutils literal notranslate"><span class="pre">pip</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation/build-from-source-linux.html">Building from Source Code on Linux</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deployment Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-llama4-scout-on-trtllm.html">Quick Start Recipe for Llama4 Scout 17B on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-deepseek-r1-on-trtllm.html">Quick Start Recipe for DeepSeek R1 on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-llama3.3-70b-on-trtllm.html">Quick Start Recipe for Llama3.3 70B on TensorRT-LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deployment-guide/quick-start-recipe-for-gpt-oss-on-trtllm.html">Quick Start Recipe for GPT-OSS on TensorRT-LLM - Blackwell Hardware</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/index.html">LLM API Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/reference.html">API Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/index.html">LLM Examples Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/customization.html">LLM Common Customizations</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/llm_api_examples.html">LLM Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference.html">Generate text</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async.html">Generate text asynchronously</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async_streaming.html">Generate text in streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_distributed.html">Distributed LLM Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_guided_decoding.html">Generate text with guided decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_logits_processor.html">Control generated text using logits processor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_multilora.html">Generate text with multiple LoRA adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_kv_cache_connector.html">KV Cache Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_runtime.html">Runtime Configuration Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_sampling.html">Sampling Techniques Showcase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_llm_distributed.html">Run LLM-API with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_bench.html">Run trtllm-bench with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_serve.html">Run trtllm-serve with pytorch backend on Slurm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/trtllm_serve_examples.html">Online Serving Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client.html">Curl Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client_for_multimodal.html">Curl Chat Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_completion_client.html">Curl Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/deepseek_r1_reasoning_parser.html">Deepseek R1 Reasoning Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/genai_perf_client.html">Genai Perf Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/genai_perf_client_for_multimodal.html">Genai Perf Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client.html">OpenAI Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client_for_multimodal.html">OpenAI Chat Client for Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client.html">OpenAI Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_for_lora.html">Openai Completion Client For Lora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_json_schema.html">OpenAI Completion Client with JSON Schema</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Model Definition API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.functional.html">Functionals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.plugin.html">Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/tensorrt_llm.runtime.html">Runtime</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../_cpp_gen/executor.html">Executor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_cpp_gen/runtime.html">Runtime</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Command-Line Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-bench.html">trtllm-bench</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-build.html">trtllm-build</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../commands/trtllm-serve/index.html">trtllm-serve</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/trtllm-serve.html">trtllm-serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/run-benchmark-with-trtllm-serve.html">Run benchmarking with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../architecture/overview.html">TensorRT-LLM Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/core-concepts.html">Model Definition</a></li>



<li class="toctree-l1"><a class="reference internal" href="../../architecture/checkpoint.html">TensorRT-LLM Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/workflow.html">TensorRT-LLM Build Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/add-model.html">Adding a Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../advanced/gpt-attention.html">Multi-Head, Multi-Query, and Group-Query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/gpt-runtime.html">C++ GPT Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/executor.html">Executor API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/graph-rewriting.html">Graph Rewriting Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/lora.html">Run gpt-2b + LoRA using Executor / cpp runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/expert-parallelism.html">Expert Parallelism in TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/kv-cache-management.html">KV Cache Management: Pools, Blocks, and Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/kv-cache-reuse.html">KV cache reuse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/speculative-decoding.html">Speculative Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/disaggregated-service.html">Disaggregated-Service (Prototype)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../performance/performance-tuning-guide/index.html">Performance Tuning Guide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/benchmarking-default-performance.html">Benchmarking Default Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/useful-build-time-flags.html">Useful Build-Time Flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/tuning-max-batch-size-and-max-num-tokens.html">Tuning Max Batch Size and Max Num Tokens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/deciding-model-sharding-strategy.html">Deciding Model Sharding Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/fp8-quantization.html">FP8 Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../performance/performance-tuning-guide/useful-runtime-flags.html">Useful Runtime Options</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance/perf-analysis.html">Performance Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../reference/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/precision.html">Numerical Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/memory.html">Memory Usage of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/ci-overview.html">Continuous Integration Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/dev-containers.html">Using Dev Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
<li class="toctree-l1"><a class="reference internal" href="../H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Falcon180B-H200.html">Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization-in-TRT-LLM.html">Speed up inference with SOTA quantization techniques in TRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../XQA-kernel.html">New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog10_ADP_Balance_Strategy.html">ADP Balance Strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html">Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.html">DeepSeek R1 MTP Implementation and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html">Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog5_Disaggregated_Serving_in_TensorRT-LLM.html">Disaggregated Serving in TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog6_Llama4_maverick_eagle_guide.html">How to launch Llama4 Maverick + Eagle3 TensorRT-LLM server</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog7_NGram_performance_Analysis_And_Auto_Enablement.html">N-Gram Speculative Decoding in TensorRT‑LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2.html">Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog9_Deploying_GPT_OSS_on_TRTLLM.html">Running a High Performance GPT-OSS-120B Inference Server with TensorRT-LLM</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="scaling-expert-parallelism-in-tensorrt-llm-part-1-design-and-implementation-of-large-scale-ep">
<h1>Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)<a class="headerlink" href="#scaling-expert-parallelism-in-tensorrt-llm-part-1-design-and-implementation-of-large-scale-ep" title="Link to this heading">#</a></h1>
<p>By NVIDIA TensorRT-LLM Team</p>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#scaling-expert-parallelism-in-tensorrt-llm-part-1-design-and-implementation-of-large-scale-ep">Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)</a></p>
<ul>
<li><p><a class="reference internal" href="#table-of-contents">Table of Contents</a></p></li>
<li><p><a class="reference internal" href="#motivation-for-large-scale-ep">Motivation for large-scale EP</a></p>
<ul>
<li><p><a class="reference internal" href="#observations-over-one-machine-translation-dataset">Observations over one machine translation dataset</a></p></li>
<li><p><a class="reference internal" href="#observation-over-gsm8k-dataset">Observation over GSM8K dataset</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#high-level-design-introduction">High-level design introduction</a></p></li>
<li><p><a class="reference internal" href="#ep-communication-kernels">EP communication kernels</a></p>
<ul>
<li><p><a class="reference internal" href="#motivation-of-ep-communication-kernels-for-gb200">Motivation of EP communication kernels for GB200</a></p></li>
<li><p><a class="reference internal" href="#ep-communication-kernels-implementation">EP communication kernels implementation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#ep-load-balancer">EP Load Balancer</a></p>
<ul>
<li><p><a class="reference internal" href="#python-interface">Python Interface</a></p></li>
<li><p><a class="reference internal" href="#c-extension">C++ extension</a></p></li>
<li><p><a class="reference internal" href="#core-implementations-of-the-host-logic">Core implementations of the host logic</a></p></li>
<li><p><a class="reference internal" href="#core-implementations-of-the-gpu-logic">Core implementations of the GPU logic</a></p></li>
<li><p><a class="reference internal" href="#online-ep-load-balancer">Online EP Load Balancer</a></p></li>
<li><p><a class="reference internal" href="#offline-ep-load-balancer">Offline EP Load Balancer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#e2e-evaluation">E2E evaluation</a></p>
<ul>
<li><p><a class="reference internal" href="#the-effect-of-ep-load-balancer">The effect of EP Load Balancer</a></p>
<ul>
<li><p><a class="reference internal" href="#id1">Offline EP Load Balancer</a></p></li>
<li><p><a class="reference internal" href="#id2">Online EP Load Balancer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#performance-study">Performance study</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#reproducing-steps">Reproducing steps</a></p>
<ul>
<li><p><a class="reference internal" href="#id3">The effect of EP Load Balancer</a></p>
<ul>
<li><p><a class="reference internal" href="#step-1-run-inference-and-collect-statistics"><span class="xref myst">Step 1: Run inference and collect statistics</span></a></p></li>
<li><p><a class="reference internal" href="#step-2-generate-the-eplb-configuration"><span class="xref myst">Step 2: Generate the EPLB configuration</span></a></p></li>
<li><p><a class="reference internal" href="#step-3-run-inference-with-the-eplb-configuration"><span class="xref myst">Step 3: Run inference with the EPLB configuration</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#miscellaneous">Miscellaneous</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#expanded-thoughts">Expanded thoughts</a></p></li>
<li><p><a class="reference internal" href="#acknowledgement">Acknowledgement</a></p></li>
</ul>
</li>
</ul>
<p>The development of model like DeepSeek-V3/R1, which use large-scale fine-grained Mixture-of-Experts (MoE) designs, has significantly advanced open-source model quality. Newly released open-source models such as LLaMA4 and Qwen3 also adopt a similar large-scale fine-grained MoE design principle. However, large-scale MoE models introduce new challenges for inference systems, including high memory demands and inherent expert-level workload imbalance.</p>
<p>In the past, we have shared TensorRT-LLM’s optimization experience to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.md">push the latency boundary</a> of DeepSeek R1 model, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.md">the implementation and optimization of MTP</a>(Multi-Token Prediction) and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md">the optimizations for DeepSeek R1 throughput oriented performance</a>.</p>
<p>The DeepSeek team has also shared their valuable experience and practice on how to optimize this kind of large-scale Expert Parallelism (EP) model, including <a class="reference external" href="https://github.com/deepseek-ai/DeepEP">DeepEP</a> and <a class="reference external" href="https://github.com/deepseek-ai/EPLB">EPLB</a>. Also, the DeepSeek team has shared their concrete design considerations in <a class="reference external" href="https://arxiv.org/abs/2412.19437">this</a> tech report. On top of those great sharings, there are also nice community efforts to implement large-scale EP in other inference engines, such as <a class="reference external" href="https://lmsys.org/blog/2025-05-05-large-scale-ep/">this</a> effort from the SGLang team.</p>
<p>In this tech blog, we will introduce the details of the design and implementation to support E2E large-scale EP in TensorRT-LLM. This blog post mainly covers the following:</p>
<ul class="simple">
<li><p>How to leverage NVIDIA GB200 Multi-Node NVLink (MNNVL) HW features to implement high-performance communication kernels.</p></li>
<li><p>How to design and implement an online expert workload balancer to dynamically balance the expert load distribution and adapt to the changes of online traffic patterns. We present:</p>
<ul>
<li><p>The empirical data analysis demonstrating the need to do so.</p></li>
<li><p>The implementation of the online traffic data statistic module.</p></li>
<li><p>The design and implementation of the replication/placement strategy.</p></li>
<li><p>The MoE weight load/re-distributer to balance the online workload across multiple GPUs.</p></li>
<li><p>The changes needed to the MoE router and computation module to adapt to the expert load balancer needs.</p></li>
<li><p>Some preliminary data demonstrating the effectiveness of the current implementation in TensorRT-LLM.</p></li>
</ul>
</li>
</ul>
<p>In future tech blogs, we will also cover the following topics:</p>
<ul class="simple">
<li><p>The introduction of performance tuning and optimization for TensorRT-LLM large-scale EP GB200 implementation.</p></li>
<li><p>How to implement efficient large-scale EP support for B200/Hopper and other NVIDIA GPUs without MNNVL.</p></li>
<li><p>The best practices to leverage large-scale EP and get performance gains.</p></li>
<li><p>How to combine large-scale EP with other system optimization techniques.</p></li>
</ul>
<p>Even if, in this tech blog, we focus on TensorRT-LLM, we believe the core ideas and implementation can also be applied to other inference engines to help the inference performance on NVIDIA GPUs. Also, with the help of the community, we would like to figure out how to better modularize the current TensorRT-LLM large-scale EP implementation and make it more easily reusable by the community.</p>
<p>Finally, in this tech blog, there are implementation details which are targeted towards the GB200 system, such as the communication components leveraging the GB200 MNNVL inter-GPU connection, and the MoE weight load/re-distributer module leveraging the high bandwidth C2C connection between Grace CPU and Blackwell GPU. Nevertheless, the overall design principle and software architecture can still apply to non-GB200 NVIDIA GPU systems. To facilitate the extension to other non-GB200 system, we have, on purpose, paid attention to the generalization of the design and implementation. These changes should be easily composable with other existing components.</p>
</section>
<section id="motivation-for-large-scale-ep">
<h2>Motivation for large-scale EP<a class="headerlink" href="#motivation-for-large-scale-ep" title="Link to this heading">#</a></h2>
<p>The main motivation of introducing large-scale EP (here means EP &gt; 8) comes from the following system considerations:</p>
<ul class="simple">
<li><p>We expect to reduce the execution latency thanks to the increased aggregated memory bandwidth to load the expert weights.</p></li>
<li><p>We expect to increase the effective batch size to saturate the GPU computing power.</p></li>
</ul>
<p>Note that <strong>when the E2E execution time is dominated by the MoE GroupGEMM computation, by introducing large-scale EP, it is expected to see clear performance benefits. But if the E2E execution time is not dominated by the MoE GroupGEMM computation, then large-scale EP may bring limited performance benefit.</strong></p>
<p>Also there isn’t free lunch in the system design. When the EP size increases up to greater than 8 (sometimes even less than 8), due to the sparsity execution nature of MoE models, it can inherently trigger the EP-level workload imbalance issue.</p>
<p>And here are some empirical observations based on some datasets (<em>all the analyses below are done with the <strong>DeepSeek R1 model</strong>, on <strong>32 GB200 GPUs</strong>).</em></p>
<section id="observations-over-one-machine-translation-dataset">
<h3>Observations over one machine translation dataset<a class="headerlink" href="#observations-over-one-machine-translation-dataset" title="Link to this heading">#</a></h3>
<p>Firstly let’s have an overview of the overall imbalance issues across layers:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture1.png">
</figure>
</div>
<p align="center"><sub><em>Figure 1: The routed token count from rank 0 to all the ranks(including rank 0), for decode iteration 1950, and all the MoE layers</em></sub></p>
<p>In Figure 1, it can be seen clearly that for the MoE in layer 36, many more tokens are sent from <strong>rank 0</strong> to <strong>rank 13.</strong></p>
<p>If we zoom on the MoE in the layer 36 and record its activated expert rank distribution, there clearly is a rank that is more heavily activated:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture2.png">
</figure>
</div>
<p align="center"><sub><em>Figure 2: The tokens received for each expert rank for layer 36</em></sub></p>
<p>If we flatten the data to see the routed tokens for each expert, we can see that a few experts are more active than others:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture3.png">
</figure>
</div>
<p align="center"><sub><em>Figure 3: The tokens received for each expert for layer 36</em></sub></p>
<p>It is also interesting to see that this kind of imbalance issue is very stable across multiple iterations, as shown on the following figure:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture4.png">
</figure>
</div>
<p align="center"><sub><em>Figure 4: The accumulated token counts received for each expert for layer 36, within 50 decode steps, and the local batch size=256.</em></sub></p>
<p>Clearly, the hot experts in Figure 4 are actually the same as in Figure 3 which only have data for a single decode iteration.
We have also done the duration-based analysis for local batch size=1 which correspond to a single request with observing the similar pattern:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture5.png">
</figure>
</div>
<p align="center"><sub><em>Figure 5: The accumulated token counts received for each expert  for layer 36, within 400 decode iterations, and the local batch size \= 1\.</em></sub></p>
<p>To conclude the findings from this study over this machine translation dataset, we could say that:</p>
<ul class="simple">
<li><p>There are hot spots in some layers where the workload of some EP ranks can be much higher than others.</p></li>
<li><p>This may be caused by the hottest expert or some hot experts to be located on the same rank.</p></li>
<li><p>The routed token distributions can be the same for tens to hundreds of iteration steps or even more.</p></li>
<li><p>For the execution of a single request, it also has the same hot experts between steps.</p></li>
</ul>
<p>And another natural question is whether the above observation can change significantly on other datasets. So we have done a similar analysis with the GSM8K dataset.</p>
</section>
<section id="observation-over-gsm8k-dataset">
<h3>Observation over GSM8K dataset<a class="headerlink" href="#observation-over-gsm8k-dataset" title="Link to this heading">#</a></h3>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture6.png">
</figure>
</div>
<p align="center"><sub><em>Figure 6: The routed token count from rank 0 to all the ranks, for iteration 1950, and all the MoE layers</em></sub></p>
<p>In Figure 6, compared with Figure 1, it can be seen that for GSM8K, the hot layer becomes layer 57 instead of layer 36. Then what about the concrete status of layer 36 for the GSM8K dataset?</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture7.png">
</figure>
</div>
<p align="center"><sub><em>Figure 7: routed token counts from EP rank 0 to other EP ranks, still taking the iteration 1950, MoE layer 36 as the example</em></sub></p>
<p>Clearly from Figure 7, it can be observed that the workload imbalance is different from what was observed for the different dataset (in Figure 2).
Based on Figure 8, it can be observed that the workload imbalance is relatively stable across multiple iterations on the GSM8K dataset too. It is the same as the previous machine translation dataset.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture8.png">
</figure>
</div>
<p align="center"><sub><em>Figure 8: The accumulated token counts sent from EP Rank 0 to all the ranks, for MoE layer 57 within 50 decode steps, local batch size=256</em></sub></p>
<p>If we flatten the EP rank level data to expert-level data, we can have the following figure.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture9.png">
</figure>
</div>
<p align="center"><sub><em>Figure 9: The accumulated token counts received for each expert for layer 57, within 50 decode steps, and the local batch size=256.</em></sub></p>
<p>The similar imbalance pattern also exists for a single request.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture10.png">
</figure>
</div>
<p align="center"><sub><em>Figure 10: The accumulated token counts received for each expert for layer 57, within 400 decode steps, for a single request</em></sub></p>
<p>If we use another request, then we can still observe the expert imbalance issue, while the hot experts can be different with some in common (in this example it is expert 10).</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture11.png">
</figure>
</div>
<p align="center"><sub><em>Figure 11: The accumulated token counts received for each expert for layer 57, within 400 decode steps, for a single request</em></sub></p>
<p>So combining the data analysis of two datasets, we have the following findings:</p>
<ul class="simple">
<li><p>EP level workload imbalance issue is common for large-scale EP inference on multiple datasets. And the EP imbalance severity can be different per layer. Also the EP imbalance issue is dataset sensitive.</p></li>
<li><p>The EP rank level imbalance issue can be caused by a certain hottest expert or multiple hot experts staying on the same EP rank.</p></li>
<li><p>The EP rank imbalance distribution is relatively stable across tens to hundreds of iterations.</p></li>
<li><p>Though there is time-dimension stability of EP rank imbalance distribution, clearly different requests can have different EP imbalance distribution.</p></li>
</ul>
<p>Based on these findings, they can lead to our design consideration of TensorRT-LLM’s large-scale EP implementation:</p>
<ul class="simple">
<li><p>By design the EP imbalance issue needs to be considered to assure great E2E performance.</p></li>
<li><p>Online EP Load Balancer(rather than only a Offline EP Load Balancer implementation) based on the real-time online request traffic is essential to ensure the robustness of EP balancer.</p></li>
<li><p>The time-dimension stability of EP rank imbalance distribution can be leveraged to re-distribute the MoE weights to different EP ranks in an efficient manner.</p></li>
</ul>
<p>In the next section we will illustrate the high-level design.</p>
</section>
</section>
<section id="high-level-design-introduction">
<h2>High-level design introduction<a class="headerlink" href="#high-level-design-introduction" title="Link to this heading">#</a></h2>
<p>Based on the detailed analysis and study in section <a class="reference internal" href="#motivation-of-large-scale-ep"><span class="xref myst">Motivation of large-scale EP</span></a>, it can clearly be observed that expert imbalance in EP is a common pattern for large-scale EP. This EP imbalance can clearly impede the overall system performance in the following ways:</p>
<ul class="simple">
<li><p>The hot EP rank will consume more memory (for activations) which can limit the effective max batch size scheduled during the inference process.</p></li>
<li><p>More data will be sent to/received from the hot EP rank.</p></li>
</ul>
<p>Those issues can clearly result into a system-level congestion effect in which the hot EP rank will delay the overall E2E execution.</p>
<p>To make sure large-scale EP can run well, careful considerations are needed to minimize the EP imbalance issue. The overall design is as follows:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture12.png">
</figure>
</div>
<p align="center"><sub><em>Figure 12: the high-level design of TensorRT-LLM large-scale EP</em></sub></p>
<p>In this design, there are both CPU and GPU side logics:</p>
<ul class="simple">
<li><p>CPU side</p>
<ul>
<li><p>Implement the Replication &amp; Placement algorithms <strong>(Replication &amp; Placement Compute</strong> component) to achieve a more balanced EP strategy. Those are rather classical algorithms for which CPU computation is more suitable. Furthermore, by offloading this computation to the CPU, the interference with the GPU can be reduced. In the future, machine-learning based algorithms may also be explored and additional design consideration may be needed. The <strong>Replication &amp; Placement Compute</strong> component will generate the <strong>“Placement Info”</strong> which will then be consumed by both the GPU <strong>Routing</strong> logic and the CPU <strong>Update Weights &amp; Placement</strong> component. The <strong>Replication &amp; Placement Compute</strong> component will consume the <strong>Statistics Data</strong> generated by the <strong>Statistics</strong> component which runs on the GPU.</p></li>
<li><p>Orchestrate the process (<strong>Update Weights &amp; Placemen</strong>t component) to update and reload the MoE weights from CPU host memory to GPU device memory. This component will also consume the <strong>Placement Info</strong> generated by the <strong>Replication &amp; Placement Compute</strong> component. Our scalable design allows us to reload the MoE weights from remote GPU memory via MNNVL or NIC.</p></li>
</ul>
</li>
<li><p>GPU side</p>
<ul>
<li><p>This is the main execution workflow of inference. The following new GPU components are introduced with our design:</p>
<ul>
<li><p>EP communication kernels. In Figure 11, those are the <strong>Dispatch</strong> and <strong>Combine</strong> components.</p></li>
<li><p>Online traffic data statistics collector (the <strong>Statistics</strong> component). This component collects the <strong>Statistics Data</strong> which is to be consumed by the <strong>Replication &amp; Placement Compute</strong> component.</p></li>
<li><p>The MoE router logic (the <strong>Routing</strong> component). It sends tokens to the activated experts. It needs to be adjusted to support the dynamic placement of MoE weights. It also consumes the <strong>Placement Info</strong> generated by the <strong>Replication &amp; Placement Compute</strong> component.</p></li>
<li><p>The MoE computation logic (the <strong>MoE</strong> component) also needs to be adjusted correspondingly.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Careful synchronization between CPU and GPU components is needed to ensure the validity of the entire execution process ; particularly, to avoid hangs, as well as invalid or sub-optimal executions.</p></li>
</ul>
<p>For the <strong>Update Weights &amp; Placemen</strong>t component, we identified two design choices:</p>
<ul class="simple">
<li><p>Bulk approach</p>
<ul>
<li><p>In this approach, when the MoE weight redistribution logic starts, the inference taking place on the current serving instance will have to be paused until the MoE weight redistribution process finishes. We estimate that it can lead to approximately <strong>0.5 ~</strong> <strong>1 second</strong> online serving stalls ; causing in the worst-cases request timeouts.  This kind of timeout or stalls can be mitigated at the system level by routing the requests to other serving instances or just request replays.</p></li>
</ul>
</li>
<li><p>Layer-wise approach</p>
<ul>
<li><p>In this approach, the MoE weight redistribution is done layer by layer such that at each decode iteration only certain layers (it can be configured) will be impacted by a redistribution of their MoE weights. With this design, it will take several iterations to re-balance the MoE weights of all the layers. We expect this approach to have almost no impact on the user experience.</p></li>
</ul>
</li>
</ul>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture13.png">
</figure>
</div>
<p align="center"><sub><em>Figure 13: One example of the layer-wise MoE weight re-distribution</em></sub></p>
<p>In our current system, we choose to implement <strong>the layer-wise approach</strong> to minimize the impact on the online user experience. The bulk approach should be much easier to implement and we will not discuss it in this tech blog.
To implement the layer-wise approach properly, we need to carefully evaluate the capability of different underlying HWs to decide on the concrete implementation.
Let’s use GB200 as an example. In Figure 14, we illustrate the communication bandwidth of different HW elements in a GB200 node.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture14.png" width="500" >
</figure>
</div>
<p align="center"><sub><em>Figure 14: high-level topology of GB200 system</em></sub></p>
<p>Using the DeepSeek R1 model as an example, with FP4 precision, each MoE expert occupies 24MiB of memory space. There are 256 experts per layer. In total, that’s 58 MoE layers, plus 1 MTP layer. So the maximum amount of MoE weights which need to be redistributed, to achieve EP balance, is 348GiB.
One GB200 node has 480GB LPDDR5X memory for each Grace CPU. In total, that’s 960GB of host memory across a NUMA domain. One GB200 node can host the entire MoE weights of a model like the DeepSeek R1 LLM in its CPU host memory. Based on it, the MoE weight redistribution can be done by moving the corresponding MoE weights from CPU host memory to GPU device memory.</p>
<p>Let’s assume that we target <strong>50ms</strong> inter-token-latency (ITL) as our main latency constraint. Using back-of-the-envelope calculation, it can be computed that the amount of expert weights which can be moved from the MoE weight pool (can be kept in Grace CPU memory or GPU memory on another node) to the Blackwell GPU (to do the real MoE inference) for each decode iteration is:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture15.png" width="300" >
</figure>
</div>
<p align="center"><sub><em>Figure 15: The theoretical expert count to be updated for each iteration with following 50ms ITL constraints, by using different HW as pools to store the full MoE weight</em></sub></p>
<p>Based on this analysis, and, if we rely on the Grace CPU memory on each node to store the MoE weight pool, for each decode iteration, the weights of up to 300 experts can be redistributed to each GPU on the same GB200 node.
Assuming our goal is to finish the MoE weight re-balancing for the full model within 5 decode iterations, here are some more concrete use-case studies:</p>
<ul class="simple">
<li><p>Use-case 1 (with balanced expert placement and no expert replication)</p>
<ul>
<li><p>64 GPUs with 4 Experts per GPU</p></li>
<li><p>58 layers, 232 Experts per GPU</p></li>
<li><p>Need 47 Expert Update / Iter, all the methods can satisfy the latency goal.</p></li>
</ul>
</li>
<li><p>Use-case 2 (with both balanced expert placement and replication)</p>
<ul>
<li><p>64 GPUs or 72 GPUs with 5 Experts per GPU</p></li>
<li><p>58 layers, 290 Experts per GPU</p></li>
<li><p>Need 58 Expert Update / Iter, all the methods can satisfy the latency goal.</p></li>
</ul>
</li>
<li><p>Use-case 3 (with both balanced expert placement and replication)</p>
<ul>
<li><p>36 GPUs with 8 Experts per GPU</p></li>
<li><p>58 layers, 464 Experts per GPU</p></li>
<li><p>Need 93 Expert Update / Iter, all the method can satisfy the latency goal.</p></li>
</ul>
</li>
</ul>
<p>In summary, based on the theoretical analysis, using Grace CPU memory as the pool to hold the full size MoE weights should allow us to achieve the EP (Expert-Parallelism) re-balancing within 5 decode iterations. If we relax the requirements to 10 or more iterations, there can be even more system implementation flexibility.</p>
<p>Next we will introduce the implementation details of our large-scale EP system.</p>
</section>
<section id="ep-communication-kernels">
<h2>EP communication kernels<a class="headerlink" href="#ep-communication-kernels" title="Link to this heading">#</a></h2>
<p>We have evaluated multiple ways of implementing the EP communication kernels needed by large-scale EP, including DeepEP, other solutions and the development of an approach from scratch.</p>
<p>The current technical decision is:</p>
<ul class="simple">
<li><p>For GB200, we implemented a new set of <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/3504">custom EP communication kernels</a>.</p></li>
<li><p>For non-GB200 systems (such as B200 or Hopper), we chose to integrate DeepEP directly, with some potential enhancement.</p></li>
</ul>
<p>The considerations are:</p>
<ul class="simple">
<li><p>DeepEP is a great piece of work done by the DeepSeek team. When we started the TensorRT-LLM large-scale EP efforts, our first focus was on GB200. We chose to implement our own custom EP communication kernels as it was easier to introduce optimizations requiring the GB200 MNNVL capability. Also, based on our current evaluation, DeepEP does not provide CUDA graph compatibility for all the scenarios. We believe that CUDA graph is needed for the scenario we are interested in.</p></li>
<li><p>When we started the efforts to enable large-scale EP on Hopper, we concluded that DeepEP could be adapted and meet our needs on this platform. We plan to extend DeepEP to work for B200 in the future.</p></li>
</ul>
<p>We are also actively evaluating the possibility of consolidating GB200 and non-GB200 EP communication kernels into a single solution to make the system simpler, and we will keep the community posted on the status.
Now let’s talk a little bit more about the optimizations introduced into the custom EP communication kernel implementations.</p>
<section id="motivation-of-ep-communication-kernels-for-gb200">
<h3>Motivation of EP communication kernels for GB200<a class="headerlink" href="#motivation-of-ep-communication-kernels-for-gb200" title="Link to this heading">#</a></h3>
<p>In the Decoding Phase with Prefill-Decoding (PD) separation, we observed that the batch size may not be very large, such that latency is a significant concern. In this context, compatibility with CUDA Graph is a strong requirement.
<a class="reference external" href="https://github.com/NVIDIA/nccl">NCCL</a> is a great GPU communication library which provides highly efficient communication kernels and primitives.
For now, its Send and Recv operations require the data size to be explicitly specified when invoking with <code class="docutils literal notranslate"><span class="pre">ncclSend</span></code>/<code class="docutils literal notranslate"><span class="pre">ncclRecv</span></code>.
However, in large expert parallel (large-EP) scenarios, the data size to be transferred is determined dynamically based on the model’s output at each iteration.
With the current NCCL’s communication interface, an explicit synchronization is required to send the communication size back to the CPU and launch NCCL calls from the CPU with the corresponding data size. This would break CUDA Graph compatibility.
This limitation has forced us to develop high performance communication kernels compatible with CUDA graph and that can accept communication sizes directly from GPU memory.
We also wanted those kernels, for GB200, to take of advantage of the MNNVL’s memory bandwidth.</p>
</section>
<section id="ep-communication-kernels-implementation">
<h3>EP communication kernels implementation<a class="headerlink" href="#ep-communication-kernels-implementation" title="Link to this heading">#</a></h3>
<p>Our kernels adopt a communication approach similar to NCCL’s LL128 primitive. As this approach strikes a good balance between latency and bandwidth, it is well-suited for LLM inference.
Our custom kernels can read the communication size directly from GPU memory and are compatible with CUDA Graph even when the data size varies across runs.</p>
<p>In our implementation, we use the CUDA’s Driver API to establish a peer-to-peer (P2P) buffer via MNNVL as a workspace.
Each GPU can access the workspace of other GPUs. The workspace is divided into multiple channels, each assigned to a remote GPU as a write buffer.
Those write buffers are used in a FIFO manner, with flags used to synchronize FIFO status and avoid data corruption.
More details can be found in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/3504">PR 3504</a>.</p>
</section>
</section>
<section id="ep-load-balancer">
<h2>EP Load Balancer<a class="headerlink" href="#ep-load-balancer" title="Link to this heading">#</a></h2>
<p>TensorRT-LLM implements a set of functionalities to achieve EP Load Balancing. There are several key components:</p>
<section id="python-interface">
<h3>Python Interface<a class="headerlink" href="#python-interface" title="Link to this heading">#</a></h3>
<p>The Python interface layer provides a user-friendly PyTorch/Python native interface to access the MoE Load Balancing implementations, such as the Python wrapper for the GPU/CPU synchronization logics and the online data statistics collection, and other logics implemented in 4.2 to 4.4.</p>
</section>
<section id="c-extension">
<h3>C++ extension<a class="headerlink" href="#c-extension" title="Link to this heading">#</a></h3>
<p>The C++ extension acts as the bridge between the PyTorch/Python interface and the C++/CUDA core implementations.</p>
</section>
<section id="core-implementations-of-the-host-logic">
<h3>Core implementations of the host logic<a class="headerlink" href="#core-implementations-of-the-host-logic" title="Link to this heading">#</a></h3>
<p>The host-side core logic implements the following key parts:</p>
<ul class="simple">
<li><p>Load balancing algorithms</p>
<ul>
<li><p>Replication algorithm</p></li>
<li><p>Placement algorithm</p></li>
</ul>
</li>
<li><p>Orchestration logic of MoE weight updates</p></li>
<li><p>MoE weight update logic</p></li>
</ul>
</section>
<section id="core-implementations-of-the-gpu-logic">
<h3>Core implementations of the GPU logic<a class="headerlink" href="#core-implementations-of-the-gpu-logic" title="Link to this heading">#</a></h3>
<p>The GPU core logic contains the following components:</p>
<ul class="simple">
<li><p>Online traffic statistics collection</p>
<ul>
<li><p>To reduce the CPU-GPU back-and-forth synchronization cost, we choose to implement the online traffic statistic logic on the GPU side.</p></li>
</ul>
</li>
<li><p>Expert routing logic</p>
<ul>
<li><p>The MoE routing logic needs to be enhanced to adapt with the dynamic EP balance impact.</p></li>
</ul>
</li>
</ul>
<p>There are GPU/CPU synchronization components implemented. More details can be found in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/4384">PR 4384</a> and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/4495">PR 4495</a>.</p>
<p>Based on these core utilities, there are two versions of EP Load Balancer in TensorRT-LLM: Offline EP Load Balancer and Online EP Load Balancer.</p>
</section>
<section id="online-ep-load-balancer">
<h3>Online EP Load Balancer<a class="headerlink" href="#online-ep-load-balancer" title="Link to this heading">#</a></h3>
<p>For production deployment needs, Online EP Load Balancer is recommended since it can adapt itself to the change in the online traffic pattern, dynamically, thus with more performance guarantees.</p>
<p>However, the Online EP Load Balancer faces several challenges.</p>
<p>First, load balancing introduces dynamic Expert placement. A single Expert’s location may shift based on current workload. For example, if Expert 0 and Expert 1, originally assigned to Rank 0, both become hot experts, the load balancing policy might redistribute them to different ranks alongside cold experts, which necessitates timely updates to the weight data.</p>
<p>We aim for the Online Load Balancer to react swiftly to changes in request patterns and adjust Expert assignments to avoid load imbalance issues. Importantly, we do not want the balancing process to interfere with the online inference execution process, nor do we want to employ a “Stop-The-World” (Bulk) strategy for updating weights.</p>
<p>In large MoE models (such as DeepSeek R1) during the decoding phase, batch sizes are often small, making CUDA Graph an effective acceleration method; especially when high TPS per user is required. This benefit is even more pronounced on platforms like GB200. For this reason, we want the entire load balancing mechanism to be compatible with CUDA Graph.</p>
<p>To avoid invalidating pre-captured CUDA Graphs, we perform in-place weight updates by writing new Expert weights into the same memory locations, rather than swapping out tensor pointers. This ensures the weights tensor address remains unchanged in the Model Engine.</p>
<p>In this design, each Expert Slot serves as a container for holding an Expert’s weights, decoupled from any specific Expert. The number of Expert Slots must be greater than or equal to the total number of Experts so that each Expert always has at least one available Slot. Hot Experts may occupy multiple Slots. Each Slot is identified by a SlotId.</p>
<p>Since the MoE model’s routing logic outputs ExpertIds (not SlotIds), we maintain a routing table from ExpertId to SlotId which is updated by the load balancing policy, periodically. The Load Balancer Routing module uses the current routing table (Expert replication information and slots) to map each token to a suitable Expert Slot.</p>
<p>To make weight updates non-blocking and avoid “Stop-The-World”, we use a layer-wise update approach. After a layer’s forward pass completes and before its next forward pass starts, we perform the weight balancing for that layer; the next forward pass for the same layer should wait until the last update is done if it happens at this iteration.</p>
<p>As the forward execution is typically driven by a single Python thread invoking a sequence of PyTorch operations, we offload the weight update routine to a background C++ thread. The Python side only initializes the Expert Slots and registers Expert Weights in shared host memory.</p>
<p>During forward execution, we insert lightweight lock/unlock kernels before and after MoE computations, as well as kernels for collecting statistics and assigning SlotIds to ExpertIds. These kernels must be short and overlap-friendly to minimize performance impact. As long as the CPU weights update thread can finish its work on time, the lock/unlock will be very short. All, except for the routing kernel, are lightweight and can easily overlap with forward kernels in different CUDA streams; the routing kernel is the primary optimization focus.</p>
<p>On GB200, we utilize MNNVL for inter-GPU communication during Expert dispatch and combine. Expert weights reside in host memory and are brought into GPU memory via C2C to support asynchronous updates. A multi-threaded Host Copy Engine manages this process, auto-detecting NUMA topology and choosing optimal CPU cores, enabling full asynchrony with model forward passes.</p>
<p>On servers without C2C but with PCIe, if cross-node communication is required, network and weight updates may compete for PCIe bandwidth, requiring additional tuning and design consideration. We have not implemented the copy engine for PCIe servers yet and it is in list of future tasks.</p>
</section>
<section id="offline-ep-load-balancer">
<h3>Offline EP Load Balancer<a class="headerlink" href="#offline-ep-load-balancer" title="Link to this heading">#</a></h3>
<p>Online EP balancer is more suitable for production deployment needs to react timely to online traffic changes. However, Offline EP Balancer provides a lightweight way for performance study/debugging and validation. You can refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/4695">this PR</a> to learn more about the implementation of the Offline EP Load Balancer. Also there is a tool provided to collect statistics about the expert activation distribution which can be used as the input to deduce the EP balancing placement strategy. You can refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/feat/large-ep/examples/ep_load_balancer#offline-ep-load-balancer">this</a> doc to learn more details as well as how how to run through the Offline EP Load Balancer in E2E approach.</p>
</section>
</section>
<section id="e2e-evaluation">
<h2>E2E evaluation<a class="headerlink" href="#e2e-evaluation" title="Link to this heading">#</a></h2>
<section id="the-effect-of-ep-load-balancer">
<h3>The effect of EP Load Balancer<a class="headerlink" href="#the-effect-of-ep-load-balancer" title="Link to this heading">#</a></h3>
<section id="id1">
<h4>Offline EP Load Balancer<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>As shown by Figure 1, on the machine translation dataset, MoE layer 36 suffers from extreme expert load imbalance issues, so we use that layer to illustrate the effect of EP Load Balancer. We still run DeepSeek-R1 with 32-way expert parallelism on 32 GB200 GPUs.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture16.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 16: The routed token count by receiving ranks (x-axis) and iterations (y-axis) at layer 36 (No EPLB)</em></sub></p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture17.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 17: The routed token count by experts (x-axis) and iterations (y-axis) at layer 36 (No EPLB)</em></sub></p>
<p>Figure 16 displays the routed token count by receiving ranks over 50 iterations, which could represent the workload for each rank. Rank 13 receives significantly more tokens than all other ranks, and such an imbalanced workload distribution is almost constant over iterations. Figure 17 breaks down the workload to experts. Clearly, two hot experts on rank 13 cause the excessive pressure on this rank.</p>
<p>With the above statistics, we can perform offline EPLB. One potential strategy is to maintain the 32-way expert parallelism while increasing expert slots from 8 to 9 per rank. This results in 32 redundant experts and 288 expert slots in total. Figures 18 and 19 show the routed token count after EPLB. Clearly, the per-rank token distribution is much more balanced, and there are no hot experts anymore.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture18.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 18: The routed token count by receiving ranks (x-axis) and iterations (y-axis) at layer 36 (EPLB with 9 per-rank slots and EP 32)</em></sub></p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture19.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 19: The routed token count by experts (x-axis) and iterations (y-axis) at layer 36 (EPLB with 9 per-rank slots and EP 32)</em></sub></p>
<p>Another EPLB strategy is to maintain 8 expert slots per rank while increasing expert parallelism to 36 ways. This strategy also results in 32 redundant experts and 288 expert slots in total. As displayed by Figures 20 and 21, the workloads also become balanced across ranks or expert slots.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture20.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 20: The routed token count by receiving ranks (x-axis) and iterations (y-axis) at layer 36 (EPLB with 8 per-rank slots and EP 36)</em></sub></p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture21.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 21: The routed token count by experts (x-axis) and iterations (y-axis) at layer 36 (EPLB with 8 per-rank slots and EP 36)</em></sub></p>
<p>For each layer and iteration, the load imbalance can be measured using simple metrics such as the standard deviation or the imbalance ratio. Given the routed token counts for all ranks (or experts), the imbalance ratio is defined as $(max - mean) / mean$, which represents the excessive workload received by the hottest rank (or expert). A perfectly balanced load would have an imbalance ratio of 0.</p>
<p>Table 1 reports the standard deviation and imbalance ratio for the aforementioned cases. Each number is averaged from the per-layer per-iteration metrics. Without EPLB, the load imbalance is significant – on average, the hottest rank receives 1.56 times more routed tokens than the mean. EPLB can effectively reduced the load imbalance – on average, the hottest rank receives only about 0.11 times more routed tokens than the mean.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p></p></th>
<th class="head text-center"><p>By rank</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p>By expert slot</p></th>
<th class="head text-center"><p></p></th>
<th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p>Average</p></td>
<td class="text-center"><p>Std. Dev.</p></td>
<td class="text-center"><p>Imb. Ratio</p></td>
<td class="text-center"><p>Average</p></td>
<td class="text-center"><p>Std. Dev.</p></td>
<td class="text-center"><p>Imb. Ratio</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>No EPLB (8 per-rank slots and EP 32)</p></td>
<td class="text-center"><p>1024</p></td>
<td class="text-center"><p>491.6</p></td>
<td class="text-center"><p>1.564</p></td>
<td class="text-center"><p>128</p></td>
<td class="text-center"><p>164.1</p></td>
<td class="text-center"><p>10.948</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>EPLB (9 per-rank slots and EP 32)</p></td>
<td class="text-center"><p>1024</p></td>
<td class="text-center"><p>52.0</p></td>
<td class="text-center"><p>0.109</p></td>
<td class="text-center"><p>114</p></td>
<td class="text-center"><p>77.8</p></td>
<td class="text-center"><p>1.792</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>EPLB (8 per-rank slots and EP 36)</p></td>
<td class="text-center"><p>1024</p></td>
<td class="text-center"><p>53.9</p></td>
<td class="text-center"><p>0.115</p></td>
<td class="text-center"><p>128</p></td>
<td class="text-center"><p>87.5</p></td>
<td class="text-center"><p>1.791</p></td>
</tr>
</tbody>
</table>
</div>
<p><em>Table 1: The standard deviation and imbalance ratio (average of per-layer and per-iteration metrics)</em></p>
</section>
<section id="id2">
<h4>Online EP Load Balancer<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>In the previous section, we demonstrated the impact of the Offline EP Load Balancer. Given our implementation of the Online EP Load Balancer, we further examine the dynamic patterns of EP balancing in online conditions.
Let’s still use the machine translation dataset, DeepSeek R1 model,  layer 36 (which is shown in Figure 1) as the example to understand the online behaviour:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture22.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 22: The token count sent from rank 0 to all the ranks, run on GB200, with EP32, local batch size=256, with 256 slots(no replication), so each rank hosts 8 experts</em></sub></p>
<p>From Figure 22, it is clear that from iteration 1963, since the EPLB has taken into effect, the original hottest rank 13 is no longer the hot rank and the original workload sent to rank 13 has been redistributed to rank 0 and rank 1.</p>
<p>In Figure 22, only placement adjustment has been done by the Online EPLB. If we further introduce expert replication, the balancing can be further improved, as shown on the following figure:</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture23.png" >
</figure>
</div>
<p align="center"><sub><em>Figure 23: The token count sent from rank 0 to all the ranks, run on GB200, with EP32, local batch size=256, with 288 slots(with replication), so each rank hosts 9 experts</em></sub></p>
<p>Clearly, by introducing expert replication when doing the EPLB, the EP balancing can be further improved.
Further complicated experiments can be designed to observe the Online EPLB taking into effect periodically during the online serving process to balance the EP workload in a dynamic way and we welcome the community to report any interesting EPLB pattern observation to us.</p>
</section>
</section>
<section id="performance-study">
<h3>Performance study<a class="headerlink" href="#performance-study" title="Link to this heading">#</a></h3>
<p>Note: all the representative workloads illustrated in this section are from the performance traces extracted from DeepSeek R1 inference execution. The E2E performance tuning/optimization is still ongoing and we will discuss them in the future technical blogs.</p>
<p>Let’s use some representative workloads to illustrate the performance impact with large-scale EP.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture24.png" width="500" >
</figure>
</div>
<p align="center"><sub><em>Figure 24: EP impact over MoE Group GEMM and EP communication</em></sub></p>
In Figure 24, it can be observed that by increasing the EP size from 4 to 72, the MoE Group GEMM computation time gets reduced, while the EP communication time (for EP4/EP8 Reduce/Scatter is used, while for EP>8 All2All is used) stays almost constant.
When the EP size increases from 18 to 72, the speed-up diminishes. We are working on optimizing it.
<p>Next, let’s use some representative workloads to understand the performance impact with EPLB.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog4_Picture25.png" width="500" >
</figure>
</div>
<p align="center"><sub><em>Figure 25: EPLB performance impact</em></sub></p>
Clearly in Figure 25, we can see that EPLB brings a clear performance improvement when the EP size increases, for both MoE GroupGEMM and EP communication times.
</section>
</section>
<section id="reproducing-steps">
<h2>Reproducing steps<a class="headerlink" href="#reproducing-steps" title="Link to this heading">#</a></h2>
<p>The code and scripts required in the reproducing steps described in this section have been merged to the main branch.</p>
<section id="id3">
<h3>The effect of EP Load Balancer<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Please, refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/feat/large-ep/examples/ep_load_balancer">EP Load Balancer example</a> for how to reproduce the results for the offline EP Load Balancer.</p>
<section id="step-1-run-inference-and-collect-statistics">
<h4>Step 1: Run inference and collect statistics<a class="headerlink" href="#step-1-run-inference-and-collect-statistics" title="Link to this heading">#</a></h4>
<p>To generate the necessary statistics for load rebalancing, run your model on a target dataset and count the routed expert IDs during inference. Once the counting process is complete, the statistics will be saved for further processing.</p>
<p>Set up some environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">MODEL_NAME</span><span class="o">=</span>deepseek-ai/DeepSeek-R1
<span class="nb">export</span><span class="w"> </span><span class="nv">MODEL_PATH</span><span class="o">=</span>&lt;YOUR_MODEL_PATH&gt;
<span class="c1"># Set the expert statistic data path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">EXPERT_STATISTIC_PATH</span><span class="o">=</span>./expert_statistic
<span class="c1"># Enable counting of routed expert IDs from iteration 100 to iteration 200</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">EXPERT_STATISTIC_ITER_RANGE</span><span class="o">=</span><span class="m">100</span>-200
</pre></div>
</div>
<p>Prepare a dataset following the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-benchmarking.md#preparing-a-dataset">benchmarking documentation</a> and save it as <code class="docutils literal notranslate"><span class="pre">./dataset.json</span></code>.</p>
<p>Run 32-way expert parallelism inference on the prepared dataset. Please refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-api/llm_mgmn_trtllm_bench.sh">LLM API MGMN example</a> for details on running <code class="docutils literal notranslate"><span class="pre">trtllm-bench</span></code> on Slurm.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>&gt;<span class="w"> </span>./extra_llm_api_options.yaml<span class="w"> </span><span class="s">&lt;&lt;EOF</span>
<span class="s">enable_attention_dp: true</span>
<span class="s">EOF</span>

trtllm-llmapi-launch<span class="w"> </span><span class="se">\</span>
trtllm-bench<span class="w"> </span>--model<span class="w"> </span><span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_path<span class="w"> </span><span class="si">${</span><span class="nv">MODEL_PATH</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tp<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ep<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--extra_llm_api_options<span class="w"> </span>./extra_llm_api_options.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_free_gpu_mem_fraction<span class="w"> </span><span class="m">0</span>.75<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--backend<span class="w"> </span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span>./dataset.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eos_id<span class="w"> </span>-1
</pre></div>
</div>
<p>After inference, review the dumped statistic files in <code class="docutils literal notranslate"><span class="pre">$EXPERT_STATISTIC_PATH</span></code>. Run the <code class="docutils literal notranslate"><span class="pre">examples/ep_load_balancer/report_load_statistics.py</span></code> script to show the standard deviation and imbalance ratio metrics:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>examples/ep_load_balancer/report_load_statistics.py<span class="w"> </span>--expert_statistic_path<span class="w"> </span><span class="nv">$EXPERT_STATISTIC_PATH</span>
</pre></div>
</div>
<p>The output would look like:</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>Load statistics:
           mean         std  imbalance-ratio
3        1024.0  187.955200         0.498043
4        1024.0  202.728516         0.537602
5        1024.0  209.339981         0.458676
...
58       1024.0  570.880676         2.461014
59       1024.0  341.339447         0.717498
60       1024.0  381.045471         1.119648
average  1024.0  491.651199         1.564272
</pre></div>
</div>
</section>
<section id="step-2-generate-the-eplb-configuration">
<h4>Step 2: Generate the EPLB configuration<a class="headerlink" href="#step-2-generate-the-eplb-configuration" title="Link to this heading">#</a></h4>
<p>Use the provided <code class="docutils literal notranslate"><span class="pre">examples/ep_load_balancer/generate_eplb_config.py</span></code> script to convert the collected statistics into an EPLB configuration file. Specify the target expert parallelism size (<code class="docutils literal notranslate"><span class="pre">--ep_size</span></code>) and the total number of slots (<code class="docutils literal notranslate"><span class="pre">--num_slots</span></code>) that will be used for deployment. For example, if we choose to maintain 8 expert slots per rank while increasing expert parallelism to 36 ways, there should be 32 redundant experts and 288 expert slots in total.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>examples/ep_load_balancer/generate_eplb_config.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ep_size<span class="w"> </span><span class="m">36</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_slots<span class="w"> </span><span class="m">288</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--expert_statistic_path<span class="w"> </span><span class="nv">$EXPERT_STATISTIC_PATH</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_path<span class="w"> </span>./moe_load_balancer.yaml
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">./moe_load_balancer.yaml</span></code> file would look like:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">initial_global_assignments</span><span class="p">:</span>
<span class="w">  </span><span class="nt">3</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">138</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">81</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">60</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">69</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">250</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">77</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">4</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">24</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">243</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">72</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">90</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">251</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">52</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">5</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">120</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">162</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">246</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">14</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">192</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">171</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<span class="w">  </span><span class="nt">58</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">67</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">70</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">160</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">212</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">103</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">125</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">59</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">45</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">142</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">152</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">99</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">205</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">49</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">60</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">34</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">162</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">119</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">...</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">234</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">26</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">129</span><span class="p p-Indicator">]</span>
<span class="nt">num_slots</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">288</span>
<span class="nt">layer_updates_per_iter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
</pre></div>
</div>
</section>
<section id="step-3-run-inference-with-the-eplb-configuration">
<h4>Step 3: Run inference with the EPLB configuration<a class="headerlink" href="#step-3-run-inference-with-the-eplb-configuration" title="Link to this heading">#</a></h4>
<p>Set up some environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set a new expert statistic data path</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">EXPERT_STATISTIC_PATH</span><span class="o">=</span>./expert_statistic_eplb
<span class="c1"># Enable counting of routed expert IDs from iteration 100 to iteration 200</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">EXPERT_STATISTIC_ITER_RANGE</span><span class="o">=</span><span class="m">100</span>-200
</pre></div>
</div>
<p>Run 36-way expert parallelism inference with the EPLB configuration incorporated:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>&gt;<span class="w"> </span>./extra_llm_api_options_eplb.yaml<span class="w"> </span><span class="s">&lt;&lt;EOF</span>
<span class="s">enable_attention_dp: true</span>
<span class="s">moe_config:</span>
<span class="s">  load_balancer: ./moe_load_balancer.yaml</span>
<span class="s">EOF</span>

trtllm-llmapi-launch<span class="w"> </span><span class="se">\</span>
trtllm-bench<span class="w"> </span>--model<span class="w"> </span><span class="si">${</span><span class="nv">MODEL_NAME</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_path<span class="w"> </span><span class="si">${</span><span class="nv">MODEL_PATH</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>throughput<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tp<span class="w"> </span><span class="m">36</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ep<span class="w"> </span><span class="m">36</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--extra_llm_api_options<span class="w"> </span>./extra_llm_api_options_eplb.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kv_cache_free_gpu_mem_fraction<span class="w"> </span><span class="m">0</span>.75<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--backend<span class="w"> </span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset<span class="w"> </span>./dataset.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--eos_id<span class="w"> </span>-1
</pre></div>
</div>
<p>Run the <code class="docutils literal notranslate"><span class="pre">examples/ep_load_balancer/report_load_statistics.py</span></code> script again:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>examples/ep_load_balancer/report_load_statistics.py<span class="w"> </span>--expert_statistic_path<span class="w"> </span><span class="nv">$EXPERT_STATISTIC_PATH</span>
</pre></div>
</div>
<p>The output would look like:</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>Load statistics:
           mean        std  imbalance-ratio
3        1024.0  37.612328         0.081947
4        1024.0  42.367714         0.093256
5        1024.0  42.623219         0.092623
...
58       1024.0  49.167507         0.113420
59       1024.0  44.529514         0.092314
60       1024.0  48.408348         0.101029
average  1024.0  53.976442         0.115378
</pre></div>
</div>
<blockquote>
<div><p><strong>Note:</strong> Counting expert IDs can significantly hurt performance, so remember to disable it by unsetting <code class="docutils literal notranslate"><span class="pre">EXPERT_STATISTIC_ITER_RANGE</span></code> when running inference for benchmarking or production purposes.</p>
</div></blockquote>
</section>
</section>
<section id="miscellaneous">
<h3>Miscellaneous<a class="headerlink" href="#miscellaneous" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>GB200 NUMA binding</strong>: Since on GB200, GPU memory are also on NUMA nodes, system can also use GPU’s memory. It is suggested to use <code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">-m</span> <span class="pre">0,1</span></code> to bind memory to CPU nodes if you don’t want that happen.</p></li>
<li><p><strong>Shared Memory Clean Up</strong>: To achieve online load balance, all expert weights are stored in shared host memory. 4 ranks on same GB200 node share the same expert weights to save memory. Normally, these shared host memory will be cleaned up at process exit. But if an abnormal exit happens, they may not get chance to be cleaned. In that case, you may need to manually check <code class="docutils literal notranslate"><span class="pre">/dev/shm</span></code> directory and delete <code class="docutils literal notranslate"><span class="pre">/dev/shm/moe_shared_*</span></code> if any.</p></li>
</ul>
</section>
</section>
<section id="expanded-thoughts">
<h2>Expanded thoughts<a class="headerlink" href="#expanded-thoughts" title="Link to this heading">#</a></h2>
<p>We deeply acknowledge the system innovation from the DeepSeek team. The introduction of the large-scale EP support into their in-house inference system and their open spirit of sharing their engineering insights with the community is extremely valuable and has already boost the performance of inference system design.
<strong>Also we want to point out that there are no magical solutions when doing system design and optimization, such as large-scale EP.</strong>
Based on our current performance analysis, when you plan to apply large-scale EP, you should take the following factors into considerations:</p>
<ul class="simple">
<li><p>Is the MoE GroupGEMM computation time an E2E performance bottleneck?</p>
<ul>
<li><p>Large-scale EP mainly helps reduce the MoE GroupGEMM execution time by reducing expert weight loading pressure and, thus, increases the compute intensity of the MoE GroupGEMM layer. For your workload setting, if the MoE GroupGEMM computation is not the bottleneck, then large-scale EP may not help much.</p></li>
</ul>
</li>
<li><p>The latency constraints.</p>
<ul>
<li><p>Large-scale EP mostly helps when there are strict latency constraints, especially on GB200/B200 with more memory capacity.  For GPUs with less memory capacity, for scenarios with less latency constraints, large-scale EP can still help as it helps achieve higher concurrency and better tokens/s/GPU.</p></li>
</ul>
</li>
<li><p>The available HW spec.</p>
<ul>
<li><p>The optimal configuration for large-scale EP depends on GPU specifications - including memory bandwidth, capacity, inter-GPU bandwidth, and compute power - which determine both whether to employ large-scale EP and the ideal degree of parallelism.</p></li>
</ul>
</li>
<li><p>System complexity and the production deployment constraints.</p>
<ul>
<li><p>Without fault tolerance guarantee, large-scale EP can increase the online system failure ratio. Even if it is possible to do cluster level coordination to route the traffic to other running serving instances when certain large-scale EP serving instances fail, the large number of GPUs required for a single-instance deployment of large-scale EP can increase system level deployment challenges.</p></li>
</ul>
</li>
</ul>
<p><strong>In the future, we plan to summarize and share more of the best practices of deploying with large-scale EP techniques.</strong></p>
<p><strong>Please use your own judgement to decide whether to use large-scale EP into your system or not, and when you use it, what is the suitable EP size and concrete deployment settings suitable for your own requirements.</strong></p>
<p>The current TensorRT-LLM large-scale EP implementation is not perfect and there are still known limitations (community contributions are welcome to help us improve). For example, we need:</p>
<ul class="simple">
<li><p>More platforms coverage</p>
<ul>
<li><p>Extending the support to cover other non-GB200 NVIDIA GPU HWs. <strong>We are actively working on this now.</strong></p></li>
<li><p>Currently the large-EP support only covers NVFP4 data precision, incremental efforts are needed to cover FP8 and INT8/INT4 data precision.</p></li>
</ul>
</li>
<li><p>Performance</p>
<ul>
<li><p>Further performance tuning and optimizations. <strong>We are actively working on this now.</strong></p></li>
<li><p>More validation with workloads close to production traffic. <strong>Here we highly welcome the community’s feedback to help us calibrate TensorRT-LLM large-scale EP implementation based on more concrete workloads.</strong></p></li>
<li><p>The thorough validation of combination with other inference core features, such as dis-aggregated serving, speculative decoding, validation on more MoE model families, etc. <strong>We are actively working on this now.</strong></p></li>
</ul>
</li>
<li><p>Ease-of-use</p>
<ul>
<li><p>Easy customization</p>
<ul>
<li><p>We believe large-scale EP can be decomposed into at least two layers:</p>
<ul>
<li><p>A core layer which developed by inference engine developers. This layer contains the customized EP communication kernels, the synchronization logic between CPU and GPU, the MoE weight re-distributed logic.</p></li>
<li><p>A strategy layer which can be co-developed by the inference engine developers as well as machine learning researchers. This part contains tools to collect the online traffic statistics in different approaches, and algorithms for the optimal replication and placement of experts.</p></li>
</ul>
</li>
<li><p>Based on this understanding, we plan to make components close to the strategy layer easier to be extended and customized by community users. We hope to encourage better ideas to emerge.</p></li>
</ul>
</li>
<li><p>Based on user inputs of the deployment requirements (ISL/OSL, latency constraints, HW spec), we hope to be able to automatically recommend the best EP setting.</p></li>
</ul>
</li>
<li><p>Fault tolerance</p>
<ul>
<li><p>Because large-scale EP deployment solution may lead to an increased fault ratio of the online deployment system, it may increase the need for cross-layer interactions with multiple components of the E2E LLM inference system on NVIDIA GPUs. This includes the low-level communication kernel, the cluster-level orchestrator and scheduler, etc. We are actively working with various NVIDIA engineering teams to push forward on this.</p></li>
</ul>
</li>
</ul>
<p>We believe the current implementation can be viewed as a reasonable E2E large-scale EP implementation and we encourage the community to try new ideas and performance validation. We encourage the community to share feedback to help us move fast in this area.  We are actively tracking the TensorRT-LLM large-scale EP execution in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/issues/4127">this</a> GitHub issue to ensure transparency to the community.</p>
</section>
<section id="acknowledgement">
<h2>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Link to this heading">#</a></h2>
<p>The large-scale EP work is another great team effort, spanning kernel-level optimizations, runtime enhancements, and systematic performance analysis and tuning. While we cannot individually acknowledge every contributor, we are proud to recognize the dedicated team of engineers whose collective expertise has helped advance the state-of-the-art in terms of performance in TensorRT-LLM.</p>
<p>Through this collaborative endeavor, we have developed valuable insights to allow us improve GPU utilization for large language model inference. We hope that the techniques and the experience shared in this blog will help the developer community to better leverage NVIDIA GPU capabilities in their mission-critical LLM inference applications.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers</p>
      </div>
    </a>
    <a class="right-next"
       href="blog5_Disaggregated_Serving_in_TensorRT-LLM.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Disaggregated Serving in TensorRT-LLM</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-large-scale-ep">Motivation for large-scale EP</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observations-over-one-machine-translation-dataset">Observations over one machine translation dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observation-over-gsm8k-dataset">Observation over GSM8K dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-design-introduction">High-level design introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ep-communication-kernels">EP communication kernels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-of-ep-communication-kernels-for-gb200">Motivation of EP communication kernels for GB200</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ep-communication-kernels-implementation">EP communication kernels implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ep-load-balancer">EP Load Balancer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-interface">Python Interface</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-extension">C++ extension</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-implementations-of-the-host-logic">Core implementations of the host logic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-implementations-of-the-gpu-logic">Core implementations of the GPU logic</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-ep-load-balancer">Online EP Load Balancer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#offline-ep-load-balancer">Offline EP Load Balancer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e2e-evaluation">E2E evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-effect-of-ep-load-balancer">The effect of EP Load Balancer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Offline EP Load Balancer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Online EP Load Balancer</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-study">Performance study</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducing-steps">Reproducing steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">The effect of EP Load Balancer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-run-inference-and-collect-statistics">Step 1: Run inference and collect statistics</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-generate-the-eplb-configuration">Step 2: Generate the EPLB configuration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-run-inference-with-the-eplb-configuration">Step 3: Run inference with the EPLB configuration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expanded-thoughts">Expanded thoughts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgement">Acknowledgement</a></li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2025, NVidia.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
<div class="extra_footer">
  
  <p>Last updated on August 29, 2025.</p>
  
  <p>This page is generated by TensorRT-LLM commit <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/15ec2b8">15ec2b8</a>.</p>
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>