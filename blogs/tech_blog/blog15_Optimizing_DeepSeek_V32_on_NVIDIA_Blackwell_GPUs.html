

<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs &#8212; TensorRT LLM</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/nvidia-sphinx-theme.css?v=933278ad" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../../_static/config_selector.css?v=e17d8078" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=19d20f17" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />



    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=65e89d2a"></script>
    <script src="../../_static/config_selector.js?v=aaf6cd4a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blogs/tech_blog/blog15_Optimizing_DeepSeek_V32_on_NVIDIA_Blackwell_GPUs';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = './_static/switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.3.0rc2';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>

    <link rel="icon" href="../../_static/favicon.png"/>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs" href="blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html" />
    <link rel="prev" title="Scaling Expert Parallelism in TensorRT LLM (Part 3: Pushing the Performance Boundary)" href="blog14_Scaling_Expert_Parallelism_in_TensorRT-LLM_part3.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.3.0rc2" />


  </head>

  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>


  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT LLM</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="TensorRT LLM - Home"/>
    <img src="../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="TensorRT LLM - Home"/>
  
  
    <p class="title logo__title">TensorRT LLM</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick-start-guide.html">Quick Start Guide</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../installation/index.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation/containers.html">Pre-built release container images on NGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation/linux.html">Installing on Linux via <code class="docutils literal notranslate"><span class="pre">pip</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation/build-from-source-linux.html">Building from Source Code on Linux</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deployment Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/llm_api_examples.html">LLM Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference.html">Generate text</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async.html">Generate text asynchronously</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_async_streaming.html">Generate text in streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_inference_distributed.html">Distributed LLM Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_guided_decoding.html">Generate text with guided decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_logits_processor.html">Control generated text using logits processor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_multilora.html">Generate text with multiple LoRA adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_sparse_attention.html">Sparse Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_speculative_decoding.html">Speculative Decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_kv_cache_connector.html">KV Cache Connector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_kv_cache_offloading.html">KV Cache Offloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_runtime.html">Runtime Configuration Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_sampling.html">Sampling Techniques Showcase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_llm_distributed.html">Run LLM-API with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_bench.html">Run trtllm-bench with pytorch backend on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/llm_mgmn_trtllm_serve.html">Run trtllm-serve with pytorch backend on Slurm</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../examples/trtllm_serve_examples.html">Online Serving Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/aiperf_client.html">Aiperf Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/aiperf_client_for_multimodal.html">Aiperf Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client.html">Curl Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_chat_client_for_multimodal.html">Curl Chat Client For Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_completion_client.html">Curl Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/curl_responses_client.html">Curl Responses Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/deepseek_r1_reasoning_parser.html">Deepseek R1 Reasoning Parser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client.html">OpenAI Chat Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_chat_client_for_multimodal.html">OpenAI Chat Client for Multimodal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client.html">OpenAI Completion Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_for_lora.html">Openai Completion Client For Lora</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_completion_client_json_schema.html">OpenAI Completion Client with JSON Schema</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/openai_responses_client.html">OpenAI Responses Client</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/dynamo_k8s_example.html">Dynamo K8s Example</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deployment-guide/index.html">Model Recipes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-deepseek-r1-on-trtllm.html">Deployment Guide for DeepSeek R1 on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-llama3.3-70b-on-trtllm.html">Deployment Guide for Llama3.3 70B on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-llama4-scout-on-trtllm.html">Deployment Guide for Llama4 Scout 17B on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-gpt-oss-on-trtllm.html">Deployment Guide for GPT-OSS on TensorRT-LLM - Blackwell Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-qwen3-on-trtllm.html">Deployment Guide for Qwen3 on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-qwen3-next-on-trtllm.html">Deployment Guide for Qwen3 Next on TensorRT LLM - Blackwell &amp; Hopper Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment-guide/deployment-guide-for-kimi-k2-thinking-on-trtllm.html">Deployment Guide for Kimi K2 Thinking on TensorRT LLM - Blackwell</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../models/supported-models.html">Supported Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../models/adding-new-model.html">Adding a New Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-bench.html">trtllm-bench</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../commands/trtllm-eval.html">trtllm-eval</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../commands/trtllm-serve/index.html">trtllm-serve</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/trtllm-serve.html">trtllm-serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../commands/trtllm-serve/run-benchmark-with-trtllm-serve.html">Run benchmarking with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code></a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/index.html">LLM API Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../llm-api/reference.html">API Reference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../features/feature-combination-matrix.html">Feature Combination Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/attention.html">Multi-Head, Multi-Query, and Group-Query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/disagg-serving.html">Disaggregated Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/kvcache.html">KV Cache System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/long-sequence.html">Long Sequences</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/lora.html">LoRA (Low-Rank Adaptation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/multi-modality.html">Multimodal Support in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/overlap-scheduler.html">Overlap Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/paged-attention-ifb-scheduler.html">Paged Attention, IFB, and Request Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/parallel-strategy.html">Parallelism in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/sampling.html">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/additional-outputs.html">Additional Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/guided-decoding.html">Guided Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/speculative-decoding.html">Speculative Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/checkpoint-loading.html">Checkpoint Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/auto_deploy/auto-deploy.html">AutoDeploy (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/ray-orchestrator.html">Ray Orchestrator (Prototype)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/torch_compile_and_piecewise_cuda_graph.html">Torch Compile &amp; Piecewise CUDA Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/helix.html">Helix Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../features/kv-cache-connector.html">KV Cache Connector</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/perf-analysis.html">Performance Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/perf-benchmarking.html">TensorRT LLM Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/ci-overview.html">Continuous Integration Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/dev-containers.html">Using Dev Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/api-change.html">LLM API Change Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guide/kv-transfer.html">Introduction to KV Cache Transmission</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="blog10_ADP_Balance_Strategy.html">ADP Balance Strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog11_GPT_OSS_Eagle3.html">Running GPT-OSS-120B with Eagle3 Speculative Decoding on GB200/B200 (TensorRT LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog12_Combining_Guided_Decoding_and_Speculative_Decoding.html">Combining Guided Decoding and Speculative Decoding: Making CPU and GPU Cooperate Seamlessly</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog13_Inference_Time_Compute_Implementation_in_TensorRT-LLM.html">Inference Time Compute Implementation in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog14_Scaling_Expert_Parallelism_in_TensorRT-LLM_part3.html">Scaling Expert Parallelism in TensorRT LLM (Part 3: Pushing the Performance Boundary)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html">Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.html">DeepSeek R1 MTP Implementation and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.html">Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.html">Scaling Expert Parallelism in TensorRT LLM (Part 1: Design and Implementation of Large-scale EP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog5_Disaggregated_Serving_in_TensorRT-LLM.html">Disaggregated Serving in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog6_Llama4_maverick_eagle_guide.html">How to launch Llama4 Maverick + Eagle3 TensorRT LLM server</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog7_NGram_performance_Analysis_And_Auto_Enablement.html">N-Gram Speculative Decoding in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2.html">Scaling Expert Parallelism in TensorRT LLM (Part 2: Performance Status and Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="blog9_Deploying_GPT_OSS_on_TRTLLM.html">Running a High Performance GPT-OSS-120B Inference Server with TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Best_perf_practice_on_DeepSeek-R1_in_TensorRT-LLM.html">How to get best performance on DeepSeek-R1 in TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../XQA-kernel.html">New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget</a></li>
<li class="toctree-l1"><a class="reference internal" href="../H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Quick Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/releases">Releases</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM">Github Code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/issues?q=is%3Aissue%20state%3Aopen%20label%3Aroadmap">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Use TensorRT Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../legacy/tensorrt_quickstart.html">LLM API with TensorRT Engine</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimizing-deepseek-v3-2-on-nvidia-blackwell-gpus">
<h1>Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs<a class="headerlink" href="#optimizing-deepseek-v3-2-on-nvidia-blackwell-gpus" title="Link to this heading">#</a></h1>
<p>By NVIDIA TensorRT LLM team</p>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#optimizing-deepseek-v3-2-on-nvidia-blackwell-gpus">Optimizing DeepSeek-V3.2 on NVIDIA Blackwell GPUs</a></p>
<ul>
<li><p><a class="reference internal" href="#table-of-contents">Table of Contents</a></p></li>
<li><p><a class="reference internal" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference internal" href="#deepseek-sparse-attention-dsa">DeepSeek Sparse Attention (DSA)</a></p></li>
<li><p><a class="reference internal" href="#precision-strategy">Precision Strategy</a></p></li>
<li><p><a class="reference internal" href="#parallel-strategy">Parallel Strategy</a></p></li>
<li><p><a class="reference internal" href="#key-features">Key Features</a></p>
<ul>
<li><p><a class="reference internal" href="#mtp">MTP</a></p></li>
<li><p><a class="reference internal" href="#disaggregated-serving">Disaggregated Serving</a></p></li>
<li><p><a class="reference internal" href="#chunked-prefill-and-kv-cache-reuse">Chunked Prefill and KV Cache Reuse</a></p></li>
<li><p><a class="reference internal" href="#wide-expert-parallelism-wide-ep">Wide Expert Parallelism (Wide-EP)</a></p></li>
<li><p><a class="reference internal" href="#chat-template-and-tool-parser">Chat Template and Tool Parser</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#key-optimizations">Key Optimizations</a></p>
<ul>
<li><p><a class="reference internal" href="#kernel-optimizations">Kernel Optimizations</a></p>
<ul>
<li><p><a class="reference internal" href="#sparse-mla-kernel">Sparse MLA Kernel</a></p></li>
<li><p><a class="reference internal" href="#indexer-top-k-kernel">Indexer Top-K Kernel</a></p></li>
<li><p><a class="reference internal" href="#deepgemm-mqa-kernel">DeepGEMM MQA Kernel</a></p></li>
<li><p><a class="reference internal" href="#kernel-fusion">Kernel Fusion</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#system-optimizations">System Optimizations</a></p>
<ul>
<li><p><a class="reference internal" href="#multi-steams">Multi-steams</a></p></li>
<li><p><a class="reference internal" href="#a-fast-path-for-short-sequences">A Fast Path for Short Sequences</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#how-to-reproduce">How to Reproduce</a></p>
<ul>
<li><p><a class="reference internal" href="#accuracy-evaluation">Accuracy Evaluation</a></p></li>
<li><p><a class="reference internal" href="#benchmark-on-b200">Benchmark on B200</a></p>
<ul>
<li><p><a class="reference internal" href="#min-latency">Min-latency</a></p></li>
<li><p><a class="reference internal" href="#max-throughput">Max-throughput</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#benchmark-with-wide-ep-on-gb200">Benchmark with Wide-EP on GB200</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#future-works">Future Works</a></p></li>
<li><p><a class="reference internal" href="#acknowledgement">Acknowledgement</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>The open-sourced <a class="reference external" href="https://api-docs.deepseek.com/news/news251201">DeepSeek-V3.2</a> series models proposed a new architecture with a fine-grained sparse attention mechanism, called DeepSeek Sparse Attention (DSA). It can help the DeepSeek-V3.2 model achieve better efficiency, especially in long sequence scenarios. Although DSA uses a lightweight indexer for prediction, realizing actual speedup from attention sparsity is still challenging. This blog introduces how TensorRT LLM supports key LLM inference features for DeepSeek-v3.2 and optimizes its performance on NVIDIA Blackwell GPUs.</p>
</section>
<section id="deepseek-sparse-attention-dsa">
<h2>DeepSeek Sparse Attention (DSA)<a class="headerlink" href="#deepseek-sparse-attention-dsa" title="Link to this heading">#</a></h2>
<p>DSA serves as a core component of the DeepSeek-v3.2 model, and it is the only architectural modification compared to its predecessors (DeepSeek-V3/R1/V3.1). It is a fine-grained sparse attention mechanism that only selects the important key-value entries for attention computation.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog15_dsa_architecture.png" alt="tech_blog15_dsa_architecture" width="700" height="auto">
</figure>
</div>
<p align="center"><sub><em>Figure 1. The architecture of DSA. The green part illustrates how DSA selects the Top-K key-value entries according to the indexer.</em></sub></p>
<p>Figure 1 illustrates the overall architecture: a lightning indexer first determines the importance of all key-value entries for each query token. Subsequently, the Top-K Selector retains only the top-<span class="math notranslate nohighlight">\(k\)</span> entries (typically <span class="math notranslate nohighlight">\(k=2048\)</span>) based on the index scores. Finally, attention is computed exclusively between the query token and these selected entries.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog15_indexer_topk.png" alt="tech_blog15_indexer_topk" width="900" height="auto">
</figure>
</div>
<p align="center"><sub><em>Figure 2. The architecture of the DSA indexer and Top-K logics.</em></sub></p>
<p>Figure 2 illustrates the DSA indexer and the Top-K selection mechanism. Firstly, two low-rank linear layers project <span class="math notranslate nohighlight">\(c_t^Q\)</span> and the input <span class="math notranslate nohighlight">\(h_t\)</span> into lower-dimensional tensors. Following operations of LayerNorm to the K tensor and RoPE to both Q and K, we obtain the tensors <span class="math notranslate nohighlight">\(Q_t^I\)</span> and <span class="math notranslate nohighlight">\(K_t^I\)</span>. Simultaneously, a separate weight projection layer processes <span class="math notranslate nohighlight">\(h_t\)</span> to generate the weights <span class="math notranslate nohighlight">\(W_t^I\)</span>. These tensors are then used to compute the index scores (labeled as MQA Logits in Figure 2):</p>
<div class="math notranslate nohighlight">
\[I_{t} = \sum_{j=1}^{h}W_j^I \cdot \text{ReLU}(Q_{t, j}^I (K_t^I)^T)\]</div>
<p>Finally, a Top-K operation is applied to the index scores to identify the most relevant indices, which are subsequently used for the sparse MLA computation. To reduce computational overhead, the K tensor <span class="math notranslate nohighlight">\(K_t^I\)</span> is stored in the indexer K cache, allowing for reuse in subsequent iterations.</p>
<p>Regarding implementation, DSA diverges from the MLA used in DeepSeek-V3/R1/V3.1 models, which alternates between MHA mode (prefill) and MQA mode (decoding) as discussed in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md">Tech Blog 3</a>. Instead, our current DSA implementation operates only in MQA mode for both prefill and decoding phases to maximize kernel efficiency. We are continuing to explore further optimizations, including potential support for MHA mode in future iterations.</p>
<p>The DSA implementation is built upon the TensorRT LLM sparse attention framework, which is designed to provide flexible and extensible support for various sparse attention methods. For more information, please refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/features/sparse-attention.md">sparse attention documentation</a>, and a technical blog providing further details will be released soon.</p>
</section>
<section id="precision-strategy">
<h2>Precision Strategy<a class="headerlink" href="#precision-strategy" title="Link to this heading">#</a></h2>
<p>Because the DSA is the only architectural modification of DeepSeek-V3.2 from the DeepSeek-R1 model, the mixed precision recipe for other modules is the same as what is used for the DeepSeek-R1. This is the NVFP4 precision strategy used in the DSA module:</p>
<ul class="simple">
<li><p>Indexer</p>
<ul>
<li><p>Low-rank linear layers: BF16</p></li>
<li><p>Weight projection layer: FP32, for model accuracy</p></li>
<li><p>MQA:</p>
<ul>
<li><p>Indexer K cache: Blockwise FP8</p></li>
<li><p>Math: Blockwise FP8</p></li>
</ul>
</li>
<li><p>Top-K: FP32</p></li>
</ul>
</li>
<li><p>QKV projection layer: BF16</p></li>
<li><p>Output projection layer: NVFP4</p></li>
<li><p>Sparse MLA</p>
<ul>
<li><p>KV cache: Per-tensor FP8</p></li>
<li><p>Math: Per-tensor FP8</p></li>
</ul>
</li>
</ul>
<p>The MoE layers use NVFP4, which is the same as the DeepSeek-R1. Please refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.md">Tech Blog 1</a> and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md">Tech Blog 3</a> for the MoE precision strategy. In addition to the NVFP4 version of DeepSeek-V3.2, TensorRT-LLM also supports the original FP8 model, as well as both BF16 and per-tensor FP8 KV caches.</p>
<p>We evaluated the accuracy of this NVFP4 checkpoint on the same datasets:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-left"><p>GSM8k</p></th>
<th class="head text-left"><p>MMLU</p></th>
<th class="head text-left"><p>GPQA-Diamond</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2">deepseek-ai/DeepSeek-V3.2</a></p></td>
<td class="text-left"><p>95.91</p></td>
<td class="text-left"><p>87.84</p></td>
<td class="text-left"><p>84.34</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>nvidia/DeepSeek-V3.2-NVFP4<sup>*</sup></p></td>
<td class="text-left"><p>95.26</p></td>
<td class="text-left"><p>87.54</p></td>
<td class="text-left"><p>84.85</p></td>
</tr>
</tbody>
</table>
</div>
<p><sub><em>* Currently, the NVFP4 checkpoint has not yet been published on Hugging Face. Please stay tuned, or refer to the <a class="reference internal" href="#how-to-reproduce">How to reproduce</a> section to learn how to quantize the model to NVFP4.<br />
** Note there are some run-to-run variance for these evaluations. Our experiments indicate that the NVFP4 recipe delivers accuracy on par with FP8 on these datasets.</em></sub></p>
</section>
<section id="parallel-strategy">
<h2>Parallel Strategy<a class="headerlink" href="#parallel-strategy" title="Link to this heading">#</a></h2>
<p>To achieve optimal throughput, DeepSeek-V3.2 adopts the same parallel strategy as DeepSeek-R1. Please refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md">Tech Blog 3</a> for a detailed explanation of the performance benefits:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Components</p></th>
<th class="head text-left"><p>Parallel Patterns</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Attention Modules</p></td>
<td class="text-left"><p>Data Parallelism 8 (DP8)</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MoE Sparse Experts</p></td>
<td class="text-left"><p>Expert Parallelism 8 (EP8)</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MoE Shared Experts</p></td>
<td class="text-left"><p>DP8</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Router GEMM</p></td>
<td class="text-left"><p>DP8</p></td>
</tr>
</tbody>
</table>
</div>
<p>To scale DeepSeek-V3.2 inference on high-performance systems such as the GB200 NVL72, the model also leverages the parallel strategy from DeepSeek-R1. Please refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md">Tech Blog 4</a>, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2.md">Tech Blog 8</a>, and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog14_Scaling_Expert_Parallelism_in_TensorRT-LLM_part3.md">Tech Blog 14</a> for more details.</p>
<p>The difference lies in the DSA indexer. When utilizing Tensor Parallelism (TP) for attention modules, typically in latency-oriented scenarios, TP is not applied to the indexer layers. Instead, it is applied exclusively to the MLA components (i.e., the remaining layers of the attention module).</p>
</section>
<section id="key-features">
<h2>Key Features<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h2>
<p>In TensorRT LLM, there are many advanced features that are crucial for maximizing LLM inference performance, such as CUDA Graph, Overlap Scheduler, Speculative Decoding, etc. Given the architectural innovations in DeepSeek-V3.2, ensuring its compatibility with these features is important.</p>
<p>As illustrated in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md">Tech Blog 3</a>, both CUDA Graph and the Overlap Scheduler offer significant throughput improvements. For CUDA Graph support, which is typically enabled during decoding-only iterations where all requests are in the decoding phase, we must ensure that kernels in the DSA module support graph capture and that input/output tensor shapes remain consistent for a given batch size. Regarding the Overlap Scheduler, it is critical to eliminate any CPU-GPU synchronization within the DSA forward, as this would disrupt the execution pipeline. Other key features are discussed in the following subsections.</p>
<section id="mtp">
<h3>MTP<a class="headerlink" href="#mtp" title="Link to this heading">#</a></h3>
<p>Multi-Token Prediction (MTP) is a speculative decoding method used in DeepSeek series models. It verifies and accepts multiple draft tokens in a single iteration, significantly improving inference performance in both low-latency and high-throughput scenarios. The DeepSeek-V3.2 also supports MTP. For latency-critical scenarios, as detailed in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.md">Tech Blog 1</a>, MTP-3 is recommended to maximize GPU utilization and achieve optimal performance. For other scenarios, MTP-1 typically offers performance gains as well.</p>
<p>However, the decoding indexer MQA kernel supports sequence lengths of only 1 or 2, limiting native support to MTP-off or MTP-1. To enable MTP &gt; 1, we offer two solutions. The long-term solution involves updating the MQA kernel to support larger sequence lengths, which will be introduced in the MQA kernel optimization section. The immediate workaround (in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9045">PR-9045</a>) uses the existing kernel by flattening the sequence length dimension into the batch dimension, treating the input as a tensor with a sequence length of 1. While this approach ignores the causal mask during the indexer MQA forward, causing discrepancies in the diagonal regions compared to ground truth, the subsequent Top-K kernel handles causal masking correctly. Therefore, the final Top-K indices remain unaffected, allowing this workaround to support MTP-N for any N.</p>
</section>
<section id="disaggregated-serving">
<h3>Disaggregated Serving<a class="headerlink" href="#disaggregated-serving" title="Link to this heading">#</a></h3>
<p>Disaggregated serving decouples the prefill and decoding phases, allowing them to run on separate GPU pools with optimized parallel strategies. This feature is crucial for deploying LLMs on high-performance systems like GB200 NVIDIA GPU HWs. However, it requires transferring KV cache blocks from the prefill to the decoding GPUs. DeepSeek-V3.2 introduces an additional ‘indexer K cache,’ which presents unique challenges for cache management and transmission in a disaggregated setup.</p>
<p>To address this, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8699">PR-8699</a> integrated indexer K cache support into the existing kvCacheManager, enabling it to inherit existing cache features. Subsequently, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8735">PR-8735</a> extended disaggregated serving capabilities to DeepSeek-V3.2, allowing TensorRT LLM to handle the transmission of the indexer K cache. Currently, the implementation specifically targets the indexer K cache, but we plan to generalize this support in future updates.</p>
</section>
<section id="chunked-prefill-and-kv-cache-reuse">
<h3>Chunked Prefill and KV Cache Reuse<a class="headerlink" href="#chunked-prefill-and-kv-cache-reuse" title="Link to this heading">#</a></h3>
<p>Two additional critical features are chunked prefill and KV cache reuse. Chunked prefill removes input length constraints for long prompts and enables prefill chunks to be batched alongside more decoding requests, boosting throughput. KV cache reuse allows requests sharing common prefixes (e.g., system prompts or multi-turn conversations) to share cached blocks, drastically reducing time-to-first-token (TTFT).</p>
<p>On the implementation side, kvCacheManager already supports the newly introduced indexer K cache, extending compatibility to both chunked prefill and KV cache reuse. Then <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9376">PR-9376</a> enabled DSA to perform prefill computation with past tokens saved in the cache, thereby unlocking chunked prefill support. Building on this, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9383">PR-9383</a> implemented KV cache reuse for DeepSeek-V3.2 by reusing the chunked prefill changes.</p>
</section>
<section id="wide-expert-parallelism-wide-ep">
<h3>Wide Expert Parallelism (Wide-EP)<a class="headerlink" href="#wide-expert-parallelism-wide-ep" title="Link to this heading">#</a></h3>
<p>The Wide-EP is an important feature for boosting inference throughput in large-scale Mixture-of-Experts (MoE) models. For the DeepSeek-V3.2 model, after supporting the disaggregated serving, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9245">PR-9245</a> simply registered the model with the Expert Parallelism Load Balancer (EPLB). This integration allows Wide-EP and EPLB to be enabled, significantly enhancing performance.</p>
</section>
<section id="chat-template-and-tool-parser">
<h3>Chat Template and Tool Parser<a class="headerlink" href="#chat-template-and-tool-parser" title="Link to this heading">#</a></h3>
<p>DeepSeek-V3.2 introduces a new chat template compared to prior versions. This update incorporates support for tool calling and the ‘thinking with tools’ capability. These enhancements, along with the necessary tool parser, were implemented in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9814">PR-9814</a> and <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/10126">PR-10126</a>. To enable this new chat template when deploying with <code class="docutils literal notranslate"><span class="pre">trtllm-serve</span></code> or <code class="docutils literal notranslate"><span class="pre">trtllm-eval</span></code>, please specify the argument <code class="docutils literal notranslate"><span class="pre">--custom_tokenizer</span> <span class="pre">deepseek_v32</span></code>.</p>
</section>
</section>
<section id="key-optimizations">
<h2>Key Optimizations<a class="headerlink" href="#key-optimizations" title="Link to this heading">#</a></h2>
<p>DeepSeek-V3.2 can inherit the MoE optimizations from DeepSeek-R1. Consequently, this section focuses exclusively on the DSA part, covering both kernel and system-level optimizations.</p>
<section id="kernel-optimizations">
<h3>Kernel Optimizations<a class="headerlink" href="#kernel-optimizations" title="Link to this heading">#</a></h3>
<section id="sparse-mla-kernel">
<h4>Sparse MLA Kernel<a class="headerlink" href="#sparse-mla-kernel" title="Link to this heading">#</a></h4>
<p>Sparse MLA serves as the core kernel of DSA, enabling attention computation with fine-grained token sparsity. To efficiently support this sparsity pattern, we leverage the new TMALDG.Gather4 instruction on Blackwell GPUs. This instruction loads four rows from a source 2D tensor and coalesces them into a single destination tensor, making it ideal for fine-grained sparse attention operations.</p>
<p>Similar to the dense MLA kernel, FP8 KV cache optimization is crucial for reducing KV cache size and improving E2E throughput. For DSA, we employ per-tensor FP8 quantization: both Query (Q) and Key-Value (KV) tensors are quantized, and FP8 arithmetic is utilized for the sparse MLA computation. To validate the model accuracy under this configuration, the table below presents the GPQA-Diamond accuracy comparison between BF16 and per-tensor FP8 KV cache for the DeepSeek-V3.2-Exp model. PR-8692 introduced this FP8 sparse MLA support, yielding up to a 47.03% improvement in throughput (TPS/GPU).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>KV Cache Type</p></th>
<th class="head text-left"><p>FP8 checkpoint</p></th>
<th class="head text-left"><p>NVFP4 checkpoint</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>BF16 Sparse MLA and KV cache</p></td>
<td class="text-left"><p>80.30</p></td>
<td class="text-left"><p>79.29</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>FP8 Sparse MLA and KV cache</p></td>
<td class="text-left"><p>78.28</p></td>
<td class="text-left"><p>80.30</p></td>
</tr>
</tbody>
</table>
</div>
<p>Another important optimization is SwapsMmaAb, designed specifically for Tensor Parallelism (TP) scenarios. When TP is enabled for sparse MLA, input tensors are partitioned along the Q head dimension. Consequently, each rank processes a reduced number of Q heads (<span class="math notranslate nohighlight">\(128 / \text{TP}\)</span>), leading to Tensor Core underutilization. SwapsMmaAb addresses this bottleneck by swapping the A and B operands during matrix multiplication to improve hardware utilization.</p>
</section>
<section id="indexer-top-k-kernel">
<h4>Indexer Top-K Kernel<a class="headerlink" href="#indexer-top-k-kernel" title="Link to this heading">#</a></h4>
<p>DSA contains a module called Top-K Selector. It is a fine-grained token selection mechanism that retrieves only the key-value entries corresponding to the Top-K index scores. The index scores are from Lightning Indexer. This part will select the top 2048 tokens for each query.</p>
<section id="deterministic-top-k-vs-non-deterministic-top-k">
<h5>Deterministic Top-K vs Non-deterministic Top-K<a class="headerlink" href="#deterministic-top-k-vs-non-deterministic-top-k" title="Link to this heading">#</a></h5>
<p>The Top‑K problem aims to find the largest (or smallest) K elements from a set of N candidates. Because some of the N candidates may have identical values, there can be more than K elements that are tied with the K‑th element. In such cases, deciding which of the tied elements are included in the final Top‑K set affects whether the output is deterministic. If the tied elements are selected randomly, the results will be non‑deterministic. Conversely, if we always prioritize elements with smaller indices, the results will be deterministic.</p>
<p>Obtaining deterministic results generally requires a more complex algorithm and incurs higher latency than a non‑deterministic version. In DeepSeek V3.2, we first need to determine whether such determinism is actually necessary. We compare the accuracy between the deterministic (DE) and non‑deterministic versions of Top‑K with the GPQA-Diamond dataset. The scores are pretty close:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>GPQA-Diamond</p></th>
<th class="head text-left"><p>DE Top-K</p></th>
<th class="head text-left"><p>Non-DE Top-K</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FP8 model</p></td>
<td class="text-left"><p>79.8</p></td>
<td class="text-left"><p>79.9</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>NVFP4 model</p></td>
<td class="text-left"><p>80.3</p></td>
<td class="text-left"><p>79.4</p></td>
</tr>
</tbody>
</table>
</div>
<p>So we decided to use the non‑DE parallel Top‑K algorithm for DeepSeek V3.2.</p>
</section>
<section id="radix-select-based-top-k-parallel-algorithm">
<h5>Radix-select-based Top-K Parallel Algorithm<a class="headerlink" href="#radix-select-based-top-k-parallel-algorithm" title="Link to this heading">#</a></h5>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog15_radix_select_topk.png" alt="tech_blog15_radix_select_topk" width="1280" height="auto">
</figure>
</div>
<p align="center"><sub><em>Figure 3. Radix-select-based Top-K.</em></sub></p>
<p>In general, there are two kinds of parallel Top‑K algorithms: partition‑based methods and priority‑queue‑based methods. The runtime of existing priority‑queue approaches grows rapidly as K increases, and the K value is as large as 2048 for the indexer Top-K in deepseek v3.2, so we choose partition‑based methods instead. Specifically, we adopt radix‑select as our baseline.
For 32‑bit values with 8‑bit digits, a naïve radix Top‑K algorithm runs 4 iterations, with 4 kernel launches per iteration. In each iteration, it (1) Histogram: counts how many elements fall into each digit bucket based on the current bits; (2) Prefix Sum: builds a prefix sum over these bucket counts; (3) Find target digits: identifies which bucket contains the K‑th element; and (4) Filtering: keeps all elements in smaller buckets as definite Top‑K, discards elements in larger buckets, and passes elements in the target bucket to the next iteration as new candidates.</p>
</section>
<section id="optimizations-for-indexer-top-k">
<h5>Optimizations for Indexer Top-K<a class="headerlink" href="#optimizations-for-indexer-top-k" title="Link to this heading">#</a></h5>
<p><strong>Skip iterations with parallel sorting.</strong> In addition to the basic radix‑select method, we introduce further optimizations to speed up the Top‑K computation. In practice, with either 8‑bit radix select (four iterations) or 11‑bit radix select (three iterations), the number of candidates typically drops sharply after the first one or two iterations on real datasets.
Our key optimization is to bypass the remaining radix‑select iterations and switch to a parallel sort once the candidate set becomes sufficiently small (smaller than 2048 in the current implementation). When the number of candidates is relatively small, we use a low-overhead naive O(N²) comparison-based ranking algorithm. For each element, we compare it against all others to determine its final position, and if this position is smaller than K, we keep it as part of the Top‑K output.  Otherwise, we use the parallel sort from CUB to get the results. The basic implementation and this optimization were added in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8882">PR-8882</a>.</p>
<p><strong>Specialization for different cases.</strong> When running with real datasets, we found that the number of candidates reaching the final sorting stage was larger than expected, which resulted in higher runtime overhead. To address this issue, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9255">PR-9255</a> introduced an additional preliminary bin-distribution step to reduce the number of candidates more efficiently before the final sort. This preprocessing step halves the candidate set and uses the leading 11 bits of each value to compute its bin index.</p>
</section>
<section id="performance-results">
<h5>Performance Results<a class="headerlink" href="#performance-results" title="Link to this heading">#</a></h5>
<p><sub><em>Table1: Compare the performance of torch.topk and our customized Top-K op on B200.</em></sub></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>torch.topk(us)</p></th>
<th class="head"><p>TopKPerRow(us)</p></th>
<th class="head"><p>Speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>topk_inputs_layer0_rank0.npy</p></td>
<td><p>106.877</p></td>
<td><p>14.069</p></td>
<td><p>7.596</p></td>
</tr>
<tr class="row-odd"><td><p>topk_inputs_layer0_rank1.npy</p></td>
<td><p>109.501</p></td>
<td><p>14.217</p></td>
<td><p>7.702</p></td>
</tr>
<tr class="row-even"><td><p>topk_inputs_layer0_rank2.npy</p></td>
<td><p>104.616</p></td>
<td><p>14.079</p></td>
<td><p>7.431</p></td>
</tr>
<tr class="row-odd"><td><p>topk_inputs_layer0_rank3.npy</p></td>
<td><p>105.049</p></td>
<td><p>14.016</p></td>
<td><p>7.495</p></td>
</tr>
<tr class="row-even"><td><p>topk_inputs_layer0_rank4.npy</p></td>
<td><p>105.526</p></td>
<td><p>14.073</p></td>
<td><p>7.498</p></td>
</tr>
<tr class="row-odd"><td><p>topk_inputs_layer0_rank5.npy</p></td>
<td><p>105.034</p></td>
<td><p>13.986</p></td>
<td><p>7.510</p></td>
</tr>
<tr class="row-even"><td><p>topk_inputs_layer0_rank6.npy</p></td>
<td><p>104.516</p></td>
<td><p>14.079</p></td>
<td><p>7.423</p></td>
</tr>
<tr class="row-odd"><td><p>topk_inputs_layer0_rank7.npy</p></td>
<td><p>105.099</p></td>
<td><p>14.189</p></td>
<td><p>7.407</p></td>
</tr>
<tr class="row-even"><td><p>topk_inputs_layer10_rank0.npy</p></td>
<td><p>109.614</p></td>
<td><p>15.281</p></td>
<td><p>7.173</p></td>
</tr>
<tr class="row-odd"><td><p>topk_inputs_layer10_rank1.npy</p></td>
<td><p>104.838</p></td>
<td><p>15.284</p></td>
<td><p>6.859</p></td>
</tr>
<tr class="row-even"><td><p>Average</p></td>
<td><p>106.067</p></td>
<td><p>14.327</p></td>
<td><p>7.410</p></td>
</tr>
</tbody>
</table>
</div>
<p>We use the data that is exported from real datasets across different layers. The input tensor size for each case is [64, 9295]. We select the top 2048 from the valid candidates for each query. As shown in Table 1, compared to the native torch.topk implementation, our implementation achieves an average speedup of 7.41x. This significantly optimizes the duration of the indexer module.</p>
<p>Overall, by replacing the DE-version Top-K from PyTorch with our customized non-DE Top-K kernel, which brings 25%~40% and 14%~24% e2e speedup for the low latency and throughput scenarios.</p>
</section>
</section>
<section id="deepgemm-mqa-kernel">
<h4>DeepGEMM MQA Kernel<a class="headerlink" href="#deepgemm-mqa-kernel" title="Link to this heading">#</a></h4>
<p>The DeepGEMM MQA kernel computes logits for the Top-K selection process. To enhance efficiency on Blackwell GPUs, several optimizations were implemented targeting both performance and ease of use:</p>
<ul class="simple">
<li><p>Larger MMA Tile Size: We increased the MMA tile size for both the prefill and decoding MQA kernels, yielding up to a 10% performance improvement. This optimization was implemented in commit <a class="reference external" href="https://github.com/deepseek-ai/DeepGEMM/commit/2f9d87877ed691a62796c25f2e9496a5e0b7123a">2f9d878</a> and <a class="reference external" href="https://github.com/deepseek-ai/DeepGEMM/commit/fc97232c6f23bf5b4be5bdef52af8ce5dc499460">fc97232</a>.</p></li>
<li><p>Flexible Paged KV Cache Configurations: The decoding MQA kernel now supports a wider range of configurations. While the initial version was restricted to a block size of 64 tokens, commit <a class="reference external" href="https://github.com/deepseek-ai/DeepGEMM/commit/c5d4d7448665ae90a81d9d31d60d445010da50f0">c5d4d74</a> enabled support for any block size <span class="math notranslate nohighlight">\(B\)</span> satisfying the condition <span class="math notranslate nohighlight">\(64 \% B = 0\)</span>.</p></li>
<li><p>MTP-3 Support: Previously, the kernel was limited to MTP-0 or MTP-1 (predicting at most one draft token). Since MTP-3 typically delivers superior performance in low-latency scenarios, optimizations were introduced (see commit <a class="reference external" href="https://github.com/deepseek-ai/DeepGEMM/commit/2be3f367854702e3887ff5b28b274cb16b441af9">2be3f36</a>) to enable native MTP-3 support.</p></li>
</ul>
</section>
<section id="kernel-fusion">
<h4>Kernel Fusion<a class="headerlink" href="#kernel-fusion" title="Link to this heading">#</a></h4>
<p>Kernel fusion is a standard optimization technique for improving performance. For DeepSeek-V3.2, we implemented specific fusion strategies:</p>
<ul class="simple">
<li><p>Custom Kernels for Indexer K Cache Population: The indexer MQA utilizes blockwise FP8 for both Q and K inputs, requiring the indexer K cache to store data in a specific blockwise FP8 format. During the forward pass, the indexer K tensor must be quantized, and both the values and scaling factors are saved to the cache. To optimize this, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8701">PR-8701</a> fused the blockwise FP8 quantization logic into a single kernel. Since the original PyTorch operations were a bottleneck, this resulted in a significant 32.64%–64.20% improvement in inference throughput. Subsequently, <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8960">PR-8960</a> fused indexer K tensor storage operations into a custom kernel, delivering an additional 3.5%–13.4% end-to-end (E2E) performance gain.</p></li>
<li><p>Fusing Small Kernels via torch.compile(): Beyond the major kernels, DSA involves numerous small kernels with low latencies. To reduce kernel launch overhead, we leverage torch.compile() to fuse these smaller operations:</p>
<ul>
<li><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8988">PR-8988</a> consolidated indexer weight scaling for blockwise FP8 quantization.</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9052">PR-9052</a> fused LayerNorm operations, yielding around 1.42% speedup for low-latency scenarios and 1.90% for throughput-oriented scenarios.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="system-optimizations">
<h3>System Optimizations<a class="headerlink" href="#system-optimizations" title="Link to this heading">#</a></h3>
<section id="multi-steams">
<h4>Multi-steams<a class="headerlink" href="#multi-steams" title="Link to this heading">#</a></h4>
<p>Multi-stream execution is leveraged in the following optimizations:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8988">PR-8988</a> employs multi-stream to overlap indexer weight scaling with the indexer K cache update. Combined with torch.compile() optimization for the indexer weight scaling, this yields approximately 2.53% speedup in low-latency scenarios.</p></li>
<li><p>When improving the blockwise FP8 quantization in <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/8701">PR-8701</a>, multi-stream is also used to enable concurrent quantization of the indexer Q and K tensors.</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9243">PR-9243</a> changed the indexer weight projection GEMM to FP32 to improve accuracy. However, this introduced a performance regression compared to the low-precision implementation. To mitigate this, multi-stream is utilized to overlap the FP32 weight projection GEMM with the indexer low-rank Q projection GEMM, LayerNorm, and Q/K RoPE operations.</p></li>
</ul>
</section>
<section id="a-fast-path-for-short-sequences">
<h4>A Fast Path for Short Sequences<a class="headerlink" href="#a-fast-path-for-short-sequences" title="Link to this heading">#</a></h4>
<p>DeepSeek-V3.2 employs K=2048 for the Top-K selector. For sequences with length <span class="math notranslate nohighlight">\(N \le 2048\)</span>, all past KV tokens are inherently selected, rendering the MQA and Top-K operations redundant. <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/pull/9524">PR-9524</a> implements a “fast path” to bypass these unnecessary operations for short sequences.</p>
<p>For the implementation, we can simply generate dense indices during DSA preparation, and directly change to use these dense indices in the indexer forward for prefill requests. However, decoding requests present a challenge due to CUDA Graph integration since the CUDA graph is usually enabled for decoding-only iterations. To ensure compatibility, we capture separate CUDA graphs for short and long sequences. At the start of each iteration, the system checks the sequence lengths: if any request in the batch exceeds the threshold, the long-sequence graph is triggered; otherwise, the short-sequence graph is utilized. This optimization yields approximately 1.03x speedup for 1K/1K scenarios.</p>
</section>
</section>
</section>
<section id="how-to-reproduce">
<h2>How to Reproduce<a class="headerlink" href="#how-to-reproduce" title="Link to this heading">#</a></h2>
<p>This section provides the reproducing steps for NVIDIA Blackwell B200 GPUs, for both model accuracy evaluation and performance benchmark.</p>
<p>The DeepSeek-V3.2 FP4 model is used for evaluation and benchmarking. You can follow <a class="reference external" href="https://github.com/NVIDIA/Model-Optimizer/tree/main/examples/deepseek#experimental-deepseek-v32">the command of the Model-Optimizer</a> to quantize the original DeepSeek-V3.2 model to FP4.</p>
<section id="accuracy-evaluation">
<h3>Accuracy Evaluation<a class="headerlink" href="#accuracy-evaluation" title="Link to this heading">#</a></h3>
<p>Evaluate the model accuracy using trtllm-eval.</p>
<ol class="arabic simple">
<li><p>Prepare an advanced configuration file:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cat</span> <span class="o">&gt;./</span><span class="n">config</span><span class="o">.</span><span class="n">yml</span> <span class="o">&lt;&lt;</span><span class="n">EOF</span>
<span class="n">cuda_graph_config</span><span class="p">:</span>
	<span class="n">enable_padding</span><span class="p">:</span> <span class="n">true</span>
	<span class="n">batch_sizes</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]</span>
<span class="n">enable_attention_dp</span><span class="p">:</span> <span class="n">true</span>
<span class="n">kv_cache_config</span><span class="p">:</span>
	<span class="n">free_gpu_memory_fraction</span><span class="p">:</span> <span class="mf">0.8</span>
	<span class="n">dtype</span><span class="p">:</span> <span class="n">fp8</span>
<span class="n">moe_config</span><span class="p">:</span>
	<span class="n">backend</span><span class="p">:</span> <span class="n">TRTLLM</span>
<span class="n">speculative_config</span><span class="p">:</span>
	<span class="n">decoding_type</span><span class="p">:</span> <span class="n">MTP</span>
	<span class="n">num_nextn_predict_layers</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">EOF</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Evaluate accuracy on the <a class="reference external" href="https://people.eecs.berkeley.edu/~hendrycks/data.tar">MMLU</a> dataset:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model_path=&lt;your model path&gt;
trtllm-eval --model ${model_path} \
	--tp_size 8 \
	--ep_size 8 \
	--kv_cache_free_gpu_memory_fraction 0.8 \
	--config ./config.yml \
	--custom_tokenizer deepseek_v32 \
	mmlu
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Evaluate accuracy on the <a class="reference external" href="https://huggingface.co/datasets/openai/gsm8k">GSM8K</a> dataset:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trtllm-eval --model ${model_path} \
	--tp_size 8 \
	--ep_size 8 \
	--kv_cache_free_gpu_memory_fraction 0.8 \
	--config ./config.yml \
	--custom_tokenizer deepseek_v32 \
	gsm8k
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Evaluate accuracy on the <a class="reference external" href="https://huggingface.co/datasets/Idavidrein/gpqa">GPQA-Diamond</a> dataset:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trtllm-eval --model ${model_path} \
	--tp_size 8 \
	--ep_size 8 \
	--kv_cache_free_gpu_memory_fraction 0.8 \
	--config ./config.yml \
	--custom_tokenizer deepseek_v32 \
	gpqa_diamond \
	--apply_chat_template \
	--chat_template_kwargs &#39;{&quot;thinking&quot;: true}&#39; \
	--max_output_length 120000
</pre></div>
</div>
</section>
<section id="benchmark-on-b200">
<h3>Benchmark on B200<a class="headerlink" href="#benchmark-on-b200" title="Link to this heading">#</a></h3>
<section id="min-latency">
<h4>Min-latency<a class="headerlink" href="#min-latency" title="Link to this heading">#</a></h4>
<p>Our benchmark results are based on Batch = 1, ISL = 8K, OSL = 1K, num_requests = 10 from a synthetic dataset.
To do the benchmark, run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>data_path=&lt;your dataset file following the format&gt;
model_path=&lt;your model path&gt;
 
cat &lt;&lt;EOF &gt; ./config.yml
cuda_graph_config:
	enable_padding: true
	batch_sizes: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,32,64,128]
kv_cache_config:
	free_gpu_memory_fraction: 0.8
	dtype: fp8
moe_config:
	backend: TRTLLM
speculative_config:
	decoding_type: MTP
	num_nextn_predict_layers: 3
EOF
 
trtllm-bench -m deepseek-ai/DeepSeek-V3.2-Exp \
	--model_path ${model_path} throughput \
	--tp 4 \
	--warmup 1 \
	--dataset ${data_path} \
	--backend pytorch \
	--max_batch_size 1 \
	--max_num_tokens 8384 \
	--kv_cache_free_gpu_mem_fraction 0.8 \
	--concurrency 1 \
	--config ./config.yml \
	--num_requests 10 \
	--streaming
</pre></div>
</div>
<p>The expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">PERFORMANCE</span> <span class="n">OVERVIEW</span>
<span class="o">===========================================================</span>
<span class="n">Request</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">req</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>                 	  <span class="mf">0.2678</span>
<span class="n">Total</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>         	  <span class="mf">274.1786</span>
<span class="n">Total</span> <span class="n">Token</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>          	  <span class="mf">2467.6070</span>
<span class="n">Total</span> <span class="n">Latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                           	  <span class="mf">37347.9238</span>
<span class="n">Average</span> <span class="n">request</span> <span class="n">latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                 	  <span class="mf">3734.7334</span>
<span class="n">Per</span> <span class="n">User</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">[</span><span class="n">w</span><span class="o">/</span> <span class="n">ctx</span><span class="p">]</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">user</span><span class="p">):</span>   <span class="mf">276.2231</span>
<span class="n">Per</span> <span class="n">GPU</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">gpu</span><span class="p">):</span>          	  <span class="mf">68.5446</span>
<span class="n">Average</span> <span class="n">time</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">first</span><span class="o">-</span><span class="n">token</span> <span class="p">[</span><span class="n">TTFT</span><span class="p">]</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>      	  <span class="mf">425.9885</span>
<span class="n">Average</span> <span class="n">time</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">output</span><span class="o">-</span><span class="n">token</span> <span class="p">[</span><span class="n">TPOT</span><span class="p">]</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>    	  <span class="mf">3.2344</span>
<span class="n">Per</span> <span class="n">User</span> <span class="n">Output</span> <span class="n">Speed</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">user</span><span class="p">):</span>             	  <span class="mf">312.0708</span>
</pre></div>
</div>
<p><sub><em>* Note that <code class="docutils literal notranslate"><span class="pre">max_num_tokens</span></code> is set to a large value to cover the maximum sequence length. Please refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Best_perf_practice_on_DeepSeek-R1_in_TensorRT-LLM.md#wip-enable-more-features-by-default">Best Performance Practices</a> for more details on <code class="docutils literal notranslate"><span class="pre">max_num_tokens</span></code> configuration.</em></sub></p>
</section>
<section id="max-throughput">
<h4>Max-throughput<a class="headerlink" href="#max-throughput" title="Link to this heading">#</a></h4>
<p>Our benchmark results are based on Batch = 256, ISL = 8K, OSL = 1K, num_requests = 768 from a synthetic dataset.
To do the benchmark, run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>data_path=&lt;your dataset file following the format&gt;
model_path=&lt;your model path&gt;
 
cat &lt;&lt;EOF &gt; ./config.yml
cuda_graph_config:
	enable_padding: true
    batch_sizes: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,32,64,128]
enable_attention_dp: true
kv_cache_config:
	free_gpu_memory_fraction: 0.8
	dtype: fp8
moe_config:
	backend: TRTLLM
speculative_config:
	decoding_type: MTP
	num_nextn_predict_layers: 1
EOF
 
trtllm-bench -m deepseek-ai/DeepSeek-V3.2-Exp \
	--model_path ${model_path} throughput \
	--tp 8 \
	--ep 8 \
	--warmup 1 \
	--dataset ${data_path} \
	--backend pytorch \
	--max_batch_size 256 \
	--max_num_tokens 8576 \
	--kv_cache_free_gpu_mem_fraction 0.8 \
	--concurrency 256 \
	--config ./config.yml \
	--num_requests 768 \
	--streaming
</pre></div>
</div>
<p>The expected results:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">===========================================================</span>
<span class="o">=</span> <span class="n">PERFORMANCE</span> <span class="n">OVERVIEW</span> 
<span class="o">===========================================================</span>
<span class="n">Request</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">req</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>                     <span class="mf">8.4162</span>
<span class="n">Total</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>             <span class="mf">8618.2158</span>
<span class="n">Total</span> <span class="n">Token</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tokens</span><span class="o">/</span><span class="n">sec</span><span class="p">):</span>              <span class="mf">77563.9425</span>
<span class="n">Total</span> <span class="n">Latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                               <span class="mf">365009.1921</span>
<span class="n">Average</span> <span class="n">request</span> <span class="n">latency</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>                     <span class="mf">120325.7013</span>
<span class="n">Per</span> <span class="n">User</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">[</span><span class="n">w</span><span class="o">/</span> <span class="n">ctx</span><span class="p">]</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">user</span><span class="p">):</span>   <span class="mf">9.8876</span>
<span class="n">Per</span> <span class="n">GPU</span> <span class="n">Output</span> <span class="n">Throughput</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">gpu</span><span class="p">):</span>              <span class="mf">1077.2770</span>
<span class="n">Average</span> <span class="n">time</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">first</span><span class="o">-</span><span class="n">token</span> <span class="p">[</span><span class="n">TTFT</span><span class="p">]</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>          <span class="mf">19537.7776</span>
<span class="n">Average</span> <span class="n">time</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">output</span><span class="o">-</span><span class="n">token</span> <span class="p">[</span><span class="n">TPOT</span><span class="p">]</span> <span class="p">(</span><span class="n">ms</span><span class="p">):</span>        <span class="mf">98.5219</span>
<span class="n">Per</span> <span class="n">User</span> <span class="n">Output</span> <span class="n">Speed</span> <span class="p">(</span><span class="n">tps</span><span class="o">/</span><span class="n">user</span><span class="p">):</span>                 <span class="mf">11.2591</span>
</pre></div>
</div>
</section>
</section>
<section id="benchmark-with-wide-ep-on-gb200">
<h3>Benchmark with Wide-EP on GB200<a class="headerlink" href="#benchmark-with-wide-ep-on-gb200" title="Link to this heading">#</a></h3>
<p>To validate the efficacy of Wide-EP on DeepSeek-V3.2, we evaluated performance using the NVFP4 model on a GB200 NVL72 system. We compared EP16 and EP32 configurations against EP4 and EP8 baselines, with benchmarks conducted at ISL=8K and OSL=1K using the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog5_Disaggregated_Serving_in_TensorRT-LLM.md#measurement-methodology">Rate Matching</a> methodology.</p>
<div align="center">
<figure>
  <img src="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/media/tech_blog15_ds32_wide_ep.png" alt="tech_blog15_ds32_wide_ep" width="700" height="auto">
</figure>
</div>
<p align="center"><sub><em>Figure 4. DeepSeek-V3.2 throughput on ISL/OSL 8k/1k. Note that the numbers were collected on November 20th, and more optimizations are still on-going.</em></sub></p>
<p>As illustrated in Figure 4, Wide-EP yields up to a 2.28x improvement in per-GPU output throughput. To reproduce these results, please refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/wide_ep/slurm_scripts">examples/wide_ep/slurm_scripts</a> directory. These scripts demonstrate how to launch disaggregated serving with large-scale EP and associated features on a SLURM cluster.</p>
</section>
</section>
<section id="future-works">
<h2>Future Works<a class="headerlink" href="#future-works" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Optimize performance for long-sequence scenarios (e.g., ISL=32K, OSL=4K).</p></li>
<li><p>Optimize performance for large Expert Parallelism (EP) configurations.</p></li>
<li><p>Evaluate dense MHA versus MQA modes for context sparse MLA to determine the optimal configuration for processing short sequences.</p></li>
<li><p>Explore more aggressive quantization strategies for DSA.</p></li>
<li><p>Optimize the implementation of the indexer Top-K kernel.</p></li>
<li><p>Investigate KV cache offloading mechanisms for DSA.</p></li>
</ul>
</section>
<section id="acknowledgement">
<h2>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Link to this heading">#</a></h2>
<p>Achieving these remarkable performance gains since the release of DeepSeek-V3.2-Exp was truly a collaborative triumph. We extend our deepest gratitude to everyone who contributed to the functional implementation and performance optimization of the DeepSeek-V3.2 model.</p>
<p>This work serves as a testament to TensorRT LLM’s flexibility and effectiveness in supporting architectural innovations and novel sparse attention mechanisms. We hope this work paves the way for further advancements in sparse attention support.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="blog14_Scaling_Expert_Parallelism_in_TensorRT-LLM_part3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Scaling Expert Parallelism in TensorRT LLM (Part 3: Pushing the Performance Boundary)</p>
      </div>
    </a>
    <a class="right-next"
       href="blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deepseek-sparse-attention-dsa">DeepSeek Sparse Attention (DSA)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#precision-strategy">Precision Strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-strategy">Parallel Strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mtp">MTP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disaggregated-serving">Disaggregated Serving</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chunked-prefill-and-kv-cache-reuse">Chunked Prefill and KV Cache Reuse</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wide-expert-parallelism-wide-ep">Wide Expert Parallelism (Wide-EP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-template-and-tool-parser">Chat Template and Tool Parser</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-optimizations">Key Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-optimizations">Kernel Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-mla-kernel">Sparse MLA Kernel</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#indexer-top-k-kernel">Indexer Top-K Kernel</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#deterministic-top-k-vs-non-deterministic-top-k">Deterministic Top-K vs Non-deterministic Top-K</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#radix-select-based-top-k-parallel-algorithm">Radix-select-based Top-K Parallel Algorithm</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizations-for-indexer-top-k">Optimizations for Indexer Top-K</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-results">Performance Results</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deepgemm-mqa-kernel">DeepGEMM MQA Kernel</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-fusion">Kernel Fusion</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#system-optimizations">System Optimizations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-steams">Multi-steams</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-fast-path-for-short-sequences">A Fast Path for Short Sequences</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-reproduce">How to Reproduce</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-evaluation">Accuracy Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-on-b200">Benchmark on B200</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#min-latency">Min-latency</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#max-throughput">Max-throughput</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-with-wide-ep-on-gb200">Benchmark with Wide-EP on GB200</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-works">Future Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgement">Acknowledgement</a></li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Your Privacy Choices</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2025, NVidia.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
<div class="extra_footer">
  
  <p>Last updated on January 30, 2026.</p>
  
  <p>This page is generated by TensorRT-LLM commit <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/f42a6cb">f42a6cb</a>.</p>
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>