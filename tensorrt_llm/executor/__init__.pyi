from collections.abc import Callable, Mapping, Sequence
import datetime
import enum
import os
from typing import overload

import torch

from . import kv_cache as kv_cache
import bindings


class ModelType(enum.Enum):
    DECODER_ONLY = 0

    ENCODER_ONLY = 1

    ENCODER_DECODER = 2

class DecodingMode:
    def Auto() -> : ...

    def TopK() -> : ...

    def TopP() -> : ...

    def TopKTopP() -> : ...

    def BeamSearch() -> : ...

    def Medusa() -> : ...

    def Lookahead() -> : ...

    def ExplicitDraftTokens() -> : ...

    def Eagle() -> : ...

    def isAuto(self) -> bool: ...

    def isTopK(self) -> bool: ...

    def isTopP(self) -> bool: ...

    def isTopKorTopP(self) -> bool: ...

    def isTopKandTopP(self) -> bool: ...

    def isBeamSearch(self) -> bool: ...

    def isMedusa(self) -> bool: ...

    def isLookahead(self) -> bool: ...

    def isExplicitDraftTokens(self) -> bool: ...

    def isEagle(self) -> bool: ...

    def useVariableBeamWidthSearch(self, arg: bool, /) -> DecodingMode: ...

    @property
    def name(self) -> str: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class CapacitySchedulerPolicy(enum.Enum):
    MAX_UTILIZATION = 0

    GUARANTEED_NO_EVICT = 1

    STATIC_BATCH = 2

class ContextChunkingPolicy(enum.Enum):
    EQUAL_PROGRESS = 1

    FIRST_COME_FIRST_SERVED = 0

class CommunicationType(enum.Enum):
    MPI = 0

class CommunicationMode(enum.Enum):
    LEADER = 0

    ORCHESTRATOR = 1

class KvCacheStats:
    def __init__(self) -> None: ...

    @property
    def max_num_blocks(self) -> int: ...

    @max_num_blocks.setter
    def max_num_blocks(self, arg: int, /) -> None: ...

    @property
    def free_num_blocks(self) -> int: ...

    @free_num_blocks.setter
    def free_num_blocks(self, arg: int, /) -> None: ...

    @property
    def used_num_blocks(self) -> int: ...

    @used_num_blocks.setter
    def used_num_blocks(self, arg: int, /) -> None: ...

    @property
    def tokens_per_block(self) -> int: ...

    @tokens_per_block.setter
    def tokens_per_block(self, arg: int, /) -> None: ...

    @property
    def alloc_total_blocks(self) -> int: ...

    @alloc_total_blocks.setter
    def alloc_total_blocks(self, arg: int, /) -> None: ...

    @property
    def alloc_new_blocks(self) -> int: ...

    @alloc_new_blocks.setter
    def alloc_new_blocks(self, arg: int, /) -> None: ...

    @property
    def reused_blocks(self) -> int: ...

    @reused_blocks.setter
    def reused_blocks(self, arg: int, /) -> None: ...

    @property
    def missed_blocks(self) -> int: ...

    @missed_blocks.setter
    def missed_blocks(self, arg: int, /) -> None: ...

    @property
    def cache_hit_rate(self) -> float: ...

    @cache_hit_rate.setter
    def cache_hit_rate(self, arg: float, /) -> None: ...

class StaticBatchingStats:
    def __init__(self) -> None: ...

    @property
    def num_scheduled_requests(self) -> int: ...

    @num_scheduled_requests.setter
    def num_scheduled_requests(self, arg: int, /) -> None: ...

    @property
    def num_context_requests(self) -> int: ...

    @num_context_requests.setter
    def num_context_requests(self, arg: int, /) -> None: ...

    @property
    def num_ctx_tokens(self) -> int: ...

    @num_ctx_tokens.setter
    def num_ctx_tokens(self, arg: int, /) -> None: ...

    @property
    def num_gen_tokens(self) -> int: ...

    @num_gen_tokens.setter
    def num_gen_tokens(self, arg: int, /) -> None: ...

    @property
    def empty_gen_slots(self) -> int: ...

    @empty_gen_slots.setter
    def empty_gen_slots(self, arg: int, /) -> None: ...

class InflightBatchingStats:
    def __init__(self) -> None: ...

    @property
    def num_scheduled_requests(self) -> int: ...

    @num_scheduled_requests.setter
    def num_scheduled_requests(self, arg: int, /) -> None: ...

    @property
    def num_context_requests(self) -> int: ...

    @num_context_requests.setter
    def num_context_requests(self, arg: int, /) -> None: ...

    @property
    def num_gen_requests(self) -> int: ...

    @num_gen_requests.setter
    def num_gen_requests(self, arg: int, /) -> None: ...

    @property
    def num_paused_requests(self) -> int: ...

    @num_paused_requests.setter
    def num_paused_requests(self, arg: int, /) -> None: ...

    @property
    def num_ctx_tokens(self) -> int: ...

    @num_ctx_tokens.setter
    def num_ctx_tokens(self, arg: int, /) -> None: ...

    @property
    def micro_batch_id(self) -> int: ...

    @micro_batch_id.setter
    def micro_batch_id(self, arg: int, /) -> None: ...

    @property
    def avg_num_decoded_tokens_per_iter(self) -> float: ...

    @avg_num_decoded_tokens_per_iter.setter
    def avg_num_decoded_tokens_per_iter(self, arg: float, /) -> None: ...

class SpecDecodingStats:
    def __init__(self) -> None: ...

    @property
    def num_draft_tokens(self) -> int: ...

    @num_draft_tokens.setter
    def num_draft_tokens(self, arg: int, /) -> None: ...

    @property
    def num_accepted_tokens(self) -> int: ...

    @num_accepted_tokens.setter
    def num_accepted_tokens(self, arg: int, /) -> None: ...

    @property
    def num_requests_with_draft_tokens(self) -> int: ...

    @num_requests_with_draft_tokens.setter
    def num_requests_with_draft_tokens(self, arg: int, /) -> None: ...

    @property
    def acceptance_length(self) -> float: ...

    @acceptance_length.setter
    def acceptance_length(self, arg: float, /) -> None: ...

    @property
    def iter_latency_ms(self) -> float: ...

    @iter_latency_ms.setter
    def iter_latency_ms(self, arg: float, /) -> None: ...

    @property
    def draft_overhead(self) -> float: ...

    @draft_overhead.setter
    def draft_overhead(self, arg: float, /) -> None: ...

class IterationStats:
    def __init__(self) -> None: ...

    @property
    def timestamp(self) -> str: ...

    @timestamp.setter
    def timestamp(self, arg: str, /) -> None: ...

    @property
    def iter(self) -> int: ...

    @iter.setter
    def iter(self, arg: int, /) -> None: ...

    @property
    def iter_latency_ms(self) -> float: ...

    @iter_latency_ms.setter
    def iter_latency_ms(self, arg: float, /) -> None: ...

    @property
    def new_active_requests_queue_latency_ms(self) -> float: ...

    @new_active_requests_queue_latency_ms.setter
    def new_active_requests_queue_latency_ms(self, arg: float, /) -> None: ...

    @property
    def num_new_active_requests(self) -> int: ...

    @num_new_active_requests.setter
    def num_new_active_requests(self, arg: int, /) -> None: ...

    @property
    def num_active_requests(self) -> int: ...

    @num_active_requests.setter
    def num_active_requests(self, arg: int, /) -> None: ...

    @property
    def num_queued_requests(self) -> int: ...

    @num_queued_requests.setter
    def num_queued_requests(self, arg: int, /) -> None: ...

    @property
    def num_completed_requests(self) -> int: ...

    @num_completed_requests.setter
    def num_completed_requests(self, arg: int, /) -> None: ...

    @property
    def max_num_active_requests(self) -> int: ...

    @max_num_active_requests.setter
    def max_num_active_requests(self, arg: int, /) -> None: ...

    @property
    def gpu_mem_usage(self) -> int: ...

    @gpu_mem_usage.setter
    def gpu_mem_usage(self, arg: int, /) -> None: ...

    @property
    def cpu_mem_usage(self) -> int: ...

    @cpu_mem_usage.setter
    def cpu_mem_usage(self, arg: int, /) -> None: ...

    @property
    def pinned_mem_usage(self) -> int: ...

    @pinned_mem_usage.setter
    def pinned_mem_usage(self, arg: int, /) -> None: ...

    @property
    def kv_cache_stats(self) -> KvCacheStats | None: ...

    @kv_cache_stats.setter
    def kv_cache_stats(self, arg: KvCacheStats, /) -> None: ...

    @property
    def cross_kv_cache_stats(self) -> KvCacheStats | None: ...

    @cross_kv_cache_stats.setter
    def cross_kv_cache_stats(self, arg: KvCacheStats, /) -> None: ...

    @property
    def static_batching_stats(self) -> StaticBatchingStats | None: ...

    @static_batching_stats.setter
    def static_batching_stats(self, arg: StaticBatchingStats, /) -> None: ...

    @property
    def inflight_batching_stats(self) -> InflightBatchingStats | None: ...

    @inflight_batching_stats.setter
    def inflight_batching_stats(self, arg: InflightBatchingStats, /) -> None: ...

    @property
    def specdec_stats(self) -> SpecDecodingStats | None: ...

    @specdec_stats.setter
    def specdec_stats(self, arg: SpecDecodingStats, /) -> None: ...

    def to_json_str(self) -> str: ...

class DebugTensorsPerIteration:
    def __init__(self) -> None: ...

    @property
    def iter(self) -> int: ...

    @iter.setter
    def iter(self, arg: int, /) -> None: ...

    @property
    def debug_tensors(self) -> "std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorrt_llm::executor::Tensor, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tensorrt_llm::executor::Tensor> > >": ...

    @debug_tensors.setter
    def debug_tensors(self, arg: "std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorrt_llm::executor::Tensor, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, tensorrt_llm::executor::Tensor> > >", /) -> None: ...

class RequestStage(enum.Enum):
    QUEUED = 0

    ENCODER_IN_PROGRESS = 1

    CONTEXT_IN_PROGRESS = 2

    GENERATION_IN_PROGRESS = 3

    GENERATION_COMPLETE = 4

class DisServingRequestStats:
    def __init__(self) -> None: ...

    @property
    def kv_cache_transfer_ms(self) -> float: ...

    @kv_cache_transfer_ms.setter
    def kv_cache_transfer_ms(self, arg: float, /) -> None: ...

    @property
    def kv_cache_size(self) -> int: ...

    @kv_cache_size.setter
    def kv_cache_size(self, arg: int, /) -> None: ...

class RequestStats:
    def __init__(self) -> None: ...

    @property
    def id(self) -> int: ...

    @id.setter
    def id(self, arg: int, /) -> None: ...

    @property
    def stage(self) -> RequestStage: ...

    @stage.setter
    def stage(self, arg: RequestStage, /) -> None: ...

    @property
    def context_prefill_position(self) -> int: ...

    @context_prefill_position.setter
    def context_prefill_position(self, arg: int, /) -> None: ...

    @property
    def num_generated_tokens(self) -> int: ...

    @num_generated_tokens.setter
    def num_generated_tokens(self, arg: int, /) -> None: ...

    @property
    def avg_num_decoded_tokens_per_iter(self) -> float: ...

    @avg_num_decoded_tokens_per_iter.setter
    def avg_num_decoded_tokens_per_iter(self, arg: float, /) -> None: ...

    @property
    def scheduled(self) -> bool: ...

    @scheduled.setter
    def scheduled(self, arg: bool, /) -> None: ...

    @property
    def paused(self) -> bool: ...

    @paused.setter
    def paused(self, arg: bool, /) -> None: ...

    @property
    def dis_serving_stats(self) -> DisServingRequestStats | None: ...

    @dis_serving_stats.setter
    def dis_serving_stats(self, arg: DisServingRequestStats, /) -> None: ...

    @property
    def alloc_total_blocks_per_request(self) -> int: ...

    @alloc_total_blocks_per_request.setter
    def alloc_total_blocks_per_request(self, arg: int, /) -> None: ...

    @property
    def alloc_new_blocks_per_request(self) -> int: ...

    @alloc_new_blocks_per_request.setter
    def alloc_new_blocks_per_request(self, arg: int, /) -> None: ...

    @property
    def reused_blocks_per_request(self) -> int: ...

    @reused_blocks_per_request.setter
    def reused_blocks_per_request(self, arg: int, /) -> None: ...

    @property
    def missed_blocks_per_request(self) -> int: ...

    @missed_blocks_per_request.setter
    def missed_blocks_per_request(self, arg: int, /) -> None: ...

    @property
    def kv_cache_hit_rate_per_request(self) -> float: ...

    @kv_cache_hit_rate_per_request.setter
    def kv_cache_hit_rate_per_request(self, arg: float, /) -> None: ...

    def to_json_str(self) -> str: ...

class RequestStatsPerIteration:
    def __init__(self) -> None: ...

    @property
    def iter(self) -> int: ...

    @iter.setter
    def iter(self, arg: int, /) -> None: ...

    @property
    def request_stats(self) -> list[RequestStats]: ...

    @request_stats.setter
    def request_stats(self, arg: Sequence[RequestStats], /) -> None: ...

    def to_json_str(self) -> str: ...

class RequestType(enum.Enum):
    REQUEST_TYPE_CONTEXT_AND_GENERATION = 0

    REQUEST_TYPE_CONTEXT_ONLY = 1

    REQUEST_TYPE_GENERATION_ONLY = 2

class FinishReason(enum.Enum):
    NOT_FINISHED = 0

    END_ID = 1

    STOP_WORDS = 2

    LENGTH = 3

    TIMED_OUT = 4

    CANCELLED = 5

class KvCacheTransferMode(enum.Enum):
    DRAM = 0

    GDS = 1

    POSIX_DEBUG_FALLBACK = 2

class SamplingConfig:
    def __init__(self, beam_width: int = 1, *, top_k: int | None = None, top_p: float | None = None, top_p_min: float | None = None, top_p_reset_ids: int | None = None, top_p_decay: float | None = None, seed: int | None = None, temperature: float | None = None, min_tokens: int | None = None, beam_search_diversity_rate: float | None = None, repetition_penalty: float | None = None, presence_penalty: float | None = None, frequency_penalty: float | None = None, length_penalty: float | None = None, early_stopping: int | None = None, no_repeat_ngram_size: int | None = None, num_return_sequences: int | None = None, min_p: float | None = None, beam_width_array: Sequence[int] | None = None) -> None: ...

    @property
    def beam_width(self) -> int: ...

    @beam_width.setter
    def beam_width(self, arg: int, /) -> None: ...

    @property
    def top_k(self) -> int | None: ...

    @top_k.setter
    def top_k(self, arg: int, /) -> None: ...

    @property
    def top_p(self) -> float | None: ...

    @top_p.setter
    def top_p(self, arg: float, /) -> None: ...

    @property
    def top_p_min(self) -> float | None: ...

    @top_p_min.setter
    def top_p_min(self, arg: float, /) -> None: ...

    @property
    def top_p_reset_ids(self) -> int | None: ...

    @top_p_reset_ids.setter
    def top_p_reset_ids(self, arg: int, /) -> None: ...

    @property
    def top_p_decay(self) -> float | None: ...

    @top_p_decay.setter
    def top_p_decay(self, arg: float, /) -> None: ...

    @property
    def seed(self) -> int | None: ...

    @seed.setter
    def seed(self, arg: int, /) -> None: ...

    @property
    def temperature(self) -> float | None: ...

    @temperature.setter
    def temperature(self, arg: float, /) -> None: ...

    @property
    def min_tokens(self) -> int | None: ...

    @min_tokens.setter
    def min_tokens(self, arg: int, /) -> None: ...

    @property
    def beam_search_diversity_rate(self) -> float | None: ...

    @beam_search_diversity_rate.setter
    def beam_search_diversity_rate(self, arg: float, /) -> None: ...

    @property
    def repetition_penalty(self) -> float | None: ...

    @repetition_penalty.setter
    def repetition_penalty(self, arg: float, /) -> None: ...

    @property
    def presence_penalty(self) -> float | None: ...

    @presence_penalty.setter
    def presence_penalty(self, arg: float, /) -> None: ...

    @property
    def frequency_penalty(self) -> float | None: ...

    @frequency_penalty.setter
    def frequency_penalty(self, arg: float, /) -> None: ...

    @property
    def length_penalty(self) -> float | None: ...

    @length_penalty.setter
    def length_penalty(self, arg: float, /) -> None: ...

    @property
    def early_stopping(self) -> int | None: ...

    @early_stopping.setter
    def early_stopping(self, arg: int, /) -> None: ...

    @property
    def no_repeat_ngram_size(self) -> int | None: ...

    @no_repeat_ngram_size.setter
    def no_repeat_ngram_size(self, arg: int, /) -> None: ...

    @property
    def num_return_sequences(self) -> int | None: ...

    @num_return_sequences.setter
    def num_return_sequences(self, arg: int, /) -> None: ...

    @property
    def min_p(self) -> float | None: ...

    @min_p.setter
    def min_p(self, arg: float, /) -> None: ...

    @property
    def beam_width_array(self) -> list[int] | None: ...

    @beam_width_array.setter
    def beam_width_array(self, arg: Sequence[int], /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class AdditionalModelOutput:
    def __init__(self, name: str, gather_context: bool = False) -> None: ...

    @property
    def name(self) -> str: ...

    @name.setter
    def name(self, arg: str, /) -> None: ...

    @property
    def gather_context(self) -> bool: ...

    @gather_context.setter
    def gather_context(self, arg: bool, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class OutputConfig:
    def __init__(self, return_log_probs: bool | None = None, return_context_logits: bool | None = None, return_generation_logits: bool | None = None, exclude_input_from_output: bool | None = None, return_encoder_output: bool | None = None, return_perf_metrics: bool | None = None, additional_model_outputs: Sequence[AdditionalModelOutput] | None = None) -> None: ...

    @property
    def return_log_probs(self) -> bool: ...

    @return_log_probs.setter
    def return_log_probs(self, arg: bool, /) -> None: ...

    @property
    def return_context_logits(self) -> bool: ...

    @return_context_logits.setter
    def return_context_logits(self, arg: bool, /) -> None: ...

    @property
    def return_generation_logits(self) -> bool: ...

    @return_generation_logits.setter
    def return_generation_logits(self, arg: bool, /) -> None: ...

    @property
    def exclude_input_from_output(self) -> bool: ...

    @exclude_input_from_output.setter
    def exclude_input_from_output(self, arg: bool, /) -> None: ...

    @property
    def return_encoder_output(self) -> bool: ...

    @return_encoder_output.setter
    def return_encoder_output(self, arg: bool, /) -> None: ...

    @property
    def return_perf_metrics(self) -> bool: ...

    @return_perf_metrics.setter
    def return_perf_metrics(self, arg: bool, /) -> None: ...

    @property
    def additional_model_outputs(self) -> list[AdditionalModelOutput] | None: ...

    @additional_model_outputs.setter
    def additional_model_outputs(self, arg: Sequence[AdditionalModelOutput], /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class ExternalDraftTokensConfig:
    def __init__(self, tokens: Sequence[int], logits: torch.Tensor | None = None, acceptance_threshold: float | None = None, fast_logits: bool | None = None) -> None: ...

    @property
    def tokens(self) -> list[int]: ...

    @property
    def logits(self) -> torch.Tensor | None: ...

    @property
    def acceptance_threshold(self) -> float | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

    @property
    def fast_logits(self) -> bool | None: ...

class PromptTuningConfig:
    def __init__(self, embedding_table: torch.Tensor, input_token_extra_ids: Sequence[int] | None = None) -> None: ...

    @property
    def embedding_table(self) -> torch.Tensor: ...

    @property
    def input_token_extra_ids(self) -> list[int] | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class LoraConfig:
    def __init__(self, task_id: int, weights: torch.Tensor | None = None, config: torch.Tensor | None = None) -> None: ...

    @property
    def task_id(self) -> int: ...

    @property
    def weights(self) -> torch.Tensor | None: ...

    @property
    def config(self) -> torch.Tensor | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class MultimodalInput:
    def __init__(self, multimodal_hashes: Sequence[Sequence[int]], multimodal_positions: Sequence[int], multimodal_lengths: Sequence[int]) -> None: ...

    @property
    def multimodal_hashes(self) -> list[list[int]]: ...

    @property
    def multimodal_positions(self) -> list[int]: ...

    @property
    def multimodal_lengths(self) -> list[int]: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class MropeConfig:
    def __init__(self, mrope_rotary_cos_sin: torch.Tensor, mrope_position_deltas: int) -> None: ...

    @property
    def mrope_rotary_cos_sin(self) -> torch.Tensor: ...

    @property
    def mrope_position_deltas(self) -> int: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class LookaheadDecodingConfig:
    def __init__(self, max_window_size: int, max_ngram_size: int, max_verification_set_size: int) -> None: ...

    @property
    def max_window_size(self) -> int: ...

    @property
    def max_ngram_size(self) -> int: ...

    @property
    def max_verification_set_size(self) -> int: ...

    def calculate_speculative_resource(self) -> tuple[int, int, int, int]: ...

    @staticmethod
    def calculate_speculative_resource_tuple(arg0: int, arg1: int, arg2: int, /) -> tuple[int, int, int, int]: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

    @staticmethod
    def get_default_lookahead_decoding_window() -> int: ...

    @staticmethod
    def get_default_lookahead_decoding_ngram() -> int: ...

    @staticmethod
    def get_default_lookahead_decoding_verification_set() -> int: ...

class KvCacheRetentionConfig:
    def __init__(self, token_range_retention_configs: Sequence[KvCacheRetentionConfig.TokenRangeRetentionConfig], decode_retention_priority: int = 35, decode_duration_ms: datetime.timedelta | float | None = None, transfer_mode: KvCacheTransferMode = KvCacheTransferMode.DRAM, directory: str | None = None) -> None: ...

    class TokenRangeRetentionConfig:
        def __init__(self, token_start: int, token_end: int, priority: int, duration_ms: datetime.timedelta | float | None = None) -> None: ...

        @property
        def token_start(self) -> int: ...

        @token_start.setter
        def token_start(self, arg: int, /) -> None: ...

        @property
        def token_end(self) -> int | None: ...

        @token_end.setter
        def token_end(self, arg: int, /) -> None: ...

        @property
        def priority(self) -> int: ...

        @priority.setter
        def priority(self, arg: int, /) -> None: ...

        @property
        def duration_ms(self) -> datetime.timedelta | None: ...

        @duration_ms.setter
        def duration_ms(self, arg: datetime.timedelta | float, /) -> None: ...

        def __getstate__(self) -> tuple: ...

        def __setstate__(self, arg: tuple, /) -> None: ...

        def __eq__(self, arg: KvCacheRetentionConfig.TokenRangeRetentionConfig, /) -> bool: ...

    @property
    def token_range_retention_configs(self) -> list[KvCacheRetentionConfig.TokenRangeRetentionConfig]: ...

    @property
    def decode_retention_priority(self) -> int: ...

    @property
    def decode_duration_ms(self) -> datetime.timedelta | None: ...

    @property
    def transfer_mode(self) -> KvCacheTransferMode: ...

    @property
    def directory(self) -> str: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

    def __eq__(self, arg: KvCacheRetentionConfig, /) -> bool: ...

class ContextPhaseParams:
    def __init__(self, first_gen_tokens: Sequence[int], req_id: int, opaque_state: bytes | None, draft_tokens: Sequence[int] | None) -> None: ...

    @property
    def first_gen_tokens(self) -> list[int]: ...

    @property
    def draft_tokens(self) -> list[int] | None: ...

    @property
    def req_id(self) -> int: ...

    @property
    def opaque_state(self) -> bytes | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class EagleConfig:
    def __init__(self, eagle_choices: Sequence[Sequence[int]] | None = None, greedy_sampling: bool = True, posterior_threshold: float | None = None, use_dynamic_tree: bool = False, dynamic_tree_max_topK: int | None = None) -> None: ...

    @property
    def eagle_choices(self) -> list[list[int]] | None: ...

    @property
    def greedy_sampling(self) -> bool: ...

    @property
    def posterior_threshold(self) -> float | None: ...

    @property
    def use_dynamic_tree(self) -> bool: ...

    @property
    def dynamic_tree_max_topK(self) -> int | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class GuidedDecodingParams:
    def __init__(self, guide_type: GuidedDecodingParams.GuideType, guide: str | None = None) -> None: ...

    class GuideType(enum.Enum):
        JSON = 0

        JSON_SCHEMA = 1

        REGEX = 2

        EBNF_GRAMMAR = 3

        STRUCTURAL_TAG = 4

    @property
    def guide_type(self) -> GuidedDecodingParams.GuideType: ...

    @property
    def guide(self) -> str | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class Request:
    def __init__(self, input_token_ids: Sequence[int], max_tokens: int, *, streaming: bool = False, sampling_config: SamplingConfig = ..., output_config: OutputConfig = ..., end_id: int | None = None, pad_id: int | None = None, position_ids: Sequence[int] | None = None, bad_words: Sequence[Sequence[int]] | None = None, stop_words: Sequence[Sequence[int]] | None = None, embedding_bias: torch.Tensor | None = None, external_draft_tokens_config: ExternalDraftTokensConfig | None = None, prompt_tuning_config: PromptTuningConfig | None = None, multimodal_input: MultimodalInput | None = None, multimodal_embedding: torch.Tensor | None = None, mrope_config: MropeConfig | None = None, lora_config: LoraConfig | None = None, lookahead_config: LookaheadDecodingConfig | None = None, kv_cache_retention_config: KvCacheRetentionConfig | None = None, logits_post_processor_name: str | None = None, logits_post_processor: Callable[[int, torch.Tensor, Sequence[Sequence[int]], int, int | None], None] | None = None, encoder_input_token_ids: Sequence[int] | None = None, client_id: int | None = None, return_all_generated_tokens: bool = False, priority: float = 0.5, type: RequestType = RequestType.REQUEST_TYPE_CONTEXT_AND_GENERATION, context_phase_params: ContextPhaseParams | None = None, encoder_input_features: torch.Tensor | None = None, encoder_output_length: int | None = None, cross_attention_mask: torch.Tensor | None = None, num_return_sequences: int = 1, eagle_config: EagleConfig | None = None, skip_cross_attn_blocks: torch.Tensor | None = None, guided_decoding_params: GuidedDecodingParams | None = None, language_adapter_uid: int | None = None, allotted_time_ms: datetime.timedelta | float | None = None, cache_salt_id: int | None = None) -> None: ...

    @property
    def input_token_ids(self) -> list[int]: ...

    @property
    def max_tokens(self) -> int: ...

    @property
    def streaming(self) -> bool: ...

    @streaming.setter
    def streaming(self, arg: bool, /) -> None: ...

    @property
    def sampling_config(self) -> SamplingConfig: ...

    @sampling_config.setter
    def sampling_config(self, arg: SamplingConfig, /) -> None: ...

    @property
    def output_config(self) -> OutputConfig: ...

    @output_config.setter
    def output_config(self, arg: OutputConfig, /) -> None: ...

    @property
    def end_id(self) -> int | None: ...

    @end_id.setter
    def end_id(self, arg: int, /) -> None: ...

    @property
    def pad_id(self) -> int | None: ...

    @pad_id.setter
    def pad_id(self, arg: int, /) -> None: ...

    @property
    def position_ids(self) -> list[int] | None: ...

    @position_ids.setter
    def position_ids(self, arg: Sequence[int], /) -> None: ...

    @property
    def bad_words(self) -> list[list[int]] | None: ...

    @bad_words.setter
    def bad_words(self, arg: Sequence[Sequence[int]], /) -> None: ...

    @property
    def stop_words(self) -> list[list[int]] | None: ...

    @stop_words.setter
    def stop_words(self, arg: Sequence[Sequence[int]], /) -> None: ...

    @property
    def embedding_bias(self) -> torch.Tensor | None: ...

    @embedding_bias.setter
    def embedding_bias(self, arg: torch.Tensor, /) -> None: ...

    @property
    def external_draft_tokens_config(self) -> ExternalDraftTokensConfig | None: ...

    @external_draft_tokens_config.setter
    def external_draft_tokens_config(self, arg: ExternalDraftTokensConfig, /) -> None: ...

    @property
    def prompt_tuning_config(self) -> PromptTuningConfig | None: ...

    @prompt_tuning_config.setter
    def prompt_tuning_config(self, arg: PromptTuningConfig, /) -> None: ...

    @property
    def multimodal_input(self) -> MultimodalInput | None: ...

    @multimodal_input.setter
    def multimodal_input(self, arg: MultimodalInput, /) -> None: ...

    @property
    def multimodal_embedding(self) -> torch.Tensor | None: ...

    @multimodal_embedding.setter
    def multimodal_embedding(self, arg: torch.Tensor, /) -> None: ...

    @property
    def mrope_config(self) -> MropeConfig | None: ...

    @mrope_config.setter
    def mrope_config(self, arg: MropeConfig, /) -> None: ...

    @property
    def lora_config(self) -> LoraConfig | None: ...

    @lora_config.setter
    def lora_config(self, arg: LoraConfig, /) -> None: ...

    @property
    def lookahead_config(self) -> LookaheadDecodingConfig | None: ...

    @lookahead_config.setter
    def lookahead_config(self, arg: LookaheadDecodingConfig, /) -> None: ...

    @property
    def kv_cache_retention_config(self) -> KvCacheRetentionConfig | None: ...

    @kv_cache_retention_config.setter
    def kv_cache_retention_config(self, arg: KvCacheRetentionConfig, /) -> None: ...

    @property
    def logits_post_processor_name(self) -> str | None: ...

    @logits_post_processor_name.setter
    def logits_post_processor_name(self, arg: str, /) -> None: ...

    @property
    def logits_post_processor(self) -> Callable[[int, torch.Tensor, list[list[int]], int, int | None], None] | None: ...

    @logits_post_processor.setter
    def logits_post_processor(self, arg: Callable[[int, torch.Tensor, Sequence[Sequence[int]], int, int | None], None], /) -> None: ...

    @property
    def encoder_input_token_ids(self) -> list[int] | None: ...

    @encoder_input_token_ids.setter
    def encoder_input_token_ids(self, arg: Sequence[int], /) -> None: ...

    @property
    def client_id(self) -> int | None: ...

    @client_id.setter
    def client_id(self, arg: int, /) -> None: ...

    @property
    def return_all_generated_tokens(self) -> bool: ...

    @return_all_generated_tokens.setter
    def return_all_generated_tokens(self, arg: bool, /) -> None: ...

    @property
    def request_type(self) -> RequestType: ...

    @request_type.setter
    def request_type(self, arg: RequestType, /) -> None: ...

    @property
    def encoder_input_features(self) -> torch.Tensor | None: ...

    @encoder_input_features.setter
    def encoder_input_features(self, arg: torch.Tensor, /) -> None: ...

    @property
    def cross_attention_mask(self) -> torch.Tensor | None: ...

    @cross_attention_mask.setter
    def cross_attention_mask(self, arg: torch.Tensor, /) -> None: ...

    @property
    def eagle_config(self) -> EagleConfig | None: ...

    @eagle_config.setter
    def eagle_config(self, arg: EagleConfig, /) -> None: ...

    @property
    def skip_cross_attn_blocks(self) -> torch.Tensor | None: ...

    @skip_cross_attn_blocks.setter
    def skip_cross_attn_blocks(self, arg: torch.Tensor, /) -> None: ...

    @property
    def guided_decoding_params(self) -> GuidedDecodingParams | None: ...

    @guided_decoding_params.setter
    def guided_decoding_params(self, arg: GuidedDecodingParams, /) -> None: ...

    @property
    def allotted_time_ms(self) -> datetime.timedelta | None: ...

    @allotted_time_ms.setter
    def allotted_time_ms(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def cache_salt_id(self) -> int | None: ...

    @cache_salt_id.setter
    def cache_salt_id(self, arg: int, /) -> None: ...

    @property
    def context_phase_params(self) -> ContextPhaseParams | None: ...

    @context_phase_params.setter
    def context_phase_params(self, arg: ContextPhaseParams, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

    BATCHED_POST_PROCESSOR_NAME: str = 'batched'

class SpeculativeDecodingFastLogitsInfo:
    def __init__(self) -> None: ...

    @property
    def draft_request_id(self) -> int: ...

    @draft_request_id.setter
    def draft_request_id(self, arg: int, /) -> None: ...

    @property
    def draft_participant_id(self) -> int: ...

    @draft_participant_id.setter
    def draft_participant_id(self, arg: int, /) -> None: ...

    def to_tensor(self) -> torch.Tensor: ...

class RequestPerfMetrics:
    def __init__(self) -> None: ...

    @property
    def timing_metrics(self) -> TimingMetrics: ...

    @timing_metrics.setter
    def timing_metrics(self, arg: TimingMetrics, /) -> None: ...

    @property
    def kv_cache_metrics(self) -> KvCacheMetrics: ...

    @kv_cache_metrics.setter
    def kv_cache_metrics(self, arg: KvCacheMetrics, /) -> None: ...

    @property
    def speculative_decoding(self) -> SpeculativeDecodingMetrics: ...

    @speculative_decoding.setter
    def speculative_decoding(self, arg: SpeculativeDecodingMetrics, /) -> None: ...

    @property
    def first_iter(self) -> int | None: ...

    @first_iter.setter
    def first_iter(self, arg: int, /) -> None: ...

    @property
    def last_iter(self) -> int | None: ...

    @last_iter.setter
    def last_iter(self, arg: int, /) -> None: ...

    @property
    def iter(self) -> int | None: ...

    @iter.setter
    def iter(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class TimingMetrics:
    def __init__(self) -> None: ...

    @property
    def arrival_time(self) -> datetime.timedelta: ...

    @arrival_time.setter
    def arrival_time(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def first_scheduled_time(self) -> datetime.timedelta: ...

    @first_scheduled_time.setter
    def first_scheduled_time(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def first_token_time(self) -> datetime.timedelta: ...

    @first_token_time.setter
    def first_token_time(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def last_token_time(self) -> datetime.timedelta: ...

    @last_token_time.setter
    def last_token_time(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def kv_cache_transfer_start(self) -> datetime.timedelta: ...

    @kv_cache_transfer_start.setter
    def kv_cache_transfer_start(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def kv_cache_transfer_end(self) -> datetime.timedelta: ...

    @kv_cache_transfer_end.setter
    def kv_cache_transfer_end(self, arg: datetime.timedelta | float, /) -> None: ...

    @property
    def kv_cache_size(self) -> int: ...

    @kv_cache_size.setter
    def kv_cache_size(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class KvCacheMetrics:
    def __init__(self) -> None: ...

    @property
    def num_total_allocated_blocks(self) -> int: ...

    @num_total_allocated_blocks.setter
    def num_total_allocated_blocks(self, arg: int, /) -> None: ...

    @property
    def num_new_allocated_blocks(self) -> int: ...

    @num_new_allocated_blocks.setter
    def num_new_allocated_blocks(self, arg: int, /) -> None: ...

    @property
    def num_reused_blocks(self) -> int: ...

    @num_reused_blocks.setter
    def num_reused_blocks(self, arg: int, /) -> None: ...

    @property
    def num_missed_blocks(self) -> int: ...

    @num_missed_blocks.setter
    def num_missed_blocks(self, arg: int, /) -> None: ...

    @property
    def kv_cache_hit_rate(self) -> float: ...

    @kv_cache_hit_rate.setter
    def kv_cache_hit_rate(self, arg: float, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class SpeculativeDecodingMetrics:
    def __init__(self) -> None: ...

    @property
    def acceptance_rate(self) -> float: ...

    @acceptance_rate.setter
    def acceptance_rate(self, arg: float, /) -> None: ...

    @property
    def total_accepted_draft_tokens(self) -> int: ...

    @total_accepted_draft_tokens.setter
    def total_accepted_draft_tokens(self, arg: int, /) -> None: ...

    @property
    def total_draft_tokens(self) -> int: ...

    @total_draft_tokens.setter
    def total_draft_tokens(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class AdditionalOutput:
    def __init__(self, name: str, output: torch.Tensor) -> None: ...

    @property
    def name(self) -> str: ...

    @name.setter
    def name(self, arg: str, /) -> None: ...

    @property
    def output(self) -> torch.Tensor: ...

    @output.setter
    def output(self, arg: torch.Tensor, /) -> None: ...

class Result:
    def __init__(self) -> None: ...

    @property
    def is_final(self) -> bool: ...

    @is_final.setter
    def is_final(self, arg: bool, /) -> None: ...

    @property
    def output_token_ids(self) -> list[list[int]]: ...

    @output_token_ids.setter
    def output_token_ids(self, arg: Sequence[Sequence[int]], /) -> None: ...

    @property
    def cum_log_probs(self) -> list[float] | None: ...

    @cum_log_probs.setter
    def cum_log_probs(self, cum_log_probs: Sequence[float] | None) -> None: ...

    @property
    def log_probs(self) -> list[list[float]] | None: ...

    @log_probs.setter
    def log_probs(self, log_probs: Sequence[Sequence[float]] | None) -> None: ...

    @property
    def context_logits(self) -> torch.Tensor | None: ...

    @context_logits.setter
    def context_logits(self, context_logits: torch.Tensor | None) -> None: ...

    @property
    def generation_logits(self) -> torch.Tensor | None: ...

    @generation_logits.setter
    def generation_logits(self, generation_logits: torch.Tensor | None) -> None: ...

    @property
    def spec_dec_fast_logits_info(self) -> SpeculativeDecodingFastLogitsInfo | None: ...

    @spec_dec_fast_logits_info.setter
    def spec_dec_fast_logits_info(self, spec_dec_fast_logits_info: SpeculativeDecodingFastLogitsInfo | None) -> None: ...

    @property
    def encoder_output(self) -> torch.Tensor | None: ...

    @encoder_output.setter
    def encoder_output(self, encoder_output: torch.Tensor | None) -> None: ...

    @property
    def finish_reasons(self) -> list[FinishReason]: ...

    @finish_reasons.setter
    def finish_reasons(self, arg: Sequence[FinishReason], /) -> None: ...

    @property
    def sequence_index(self) -> int: ...

    @sequence_index.setter
    def sequence_index(self, arg: int, /) -> None: ...

    @property
    def is_sequence_final(self) -> bool: ...

    @is_sequence_final.setter
    def is_sequence_final(self, arg: bool, /) -> None: ...

    @property
    def decoding_iter(self) -> int: ...

    @decoding_iter.setter
    def decoding_iter(self, arg: int, /) -> None: ...

    @property
    def avg_decoded_tokens_per_iter(self) -> float: ...

    @avg_decoded_tokens_per_iter.setter
    def avg_decoded_tokens_per_iter(self, arg: float, /) -> None: ...

    @property
    def context_phase_params(self) -> ContextPhaseParams | None: ...

    @context_phase_params.setter
    def context_phase_params(self, context_phase_params: ContextPhaseParams | None) -> None: ...

    @property
    def request_perf_metrics(self) -> RequestPerfMetrics | None: ...

    @request_perf_metrics.setter
    def request_perf_metrics(self, request_perf_metrics: RequestPerfMetrics | None) -> None: ...

    @property
    def additional_outputs(self) -> list[AdditionalOutput]: ...

    @additional_outputs.setter
    def additional_outputs(self, arg: Sequence[AdditionalOutput], /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

def deserialize_result(arg: bytes, /) -> Result: ...

class Response:
    @overload
    def __init__(self, request_id: int, error_msg: str, client_id: int | None = None) -> None: ...

    @overload
    def __init__(self, request_id: int, result: Result, client_id: int | None = None) -> None: ...

    @property
    def request_id(self) -> int: ...

    @property
    def client_id(self) -> int | None: ...

    def has_error(self) -> bool: ...

    @property
    def error_msg(self) -> str: ...

    @property
    def result(self) -> Result: ...

    def clear_context_logits(self) -> None: ...

    def clear_generation_logits(self) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class BatchingType(enum.Enum):
    STATIC = 0

    INFLIGHT = 1

class DynamicBatchConfig:
    def __init__(self, enable_batch_size_tuning: bool, enable_max_num_tokens_tuning: bool, dynamic_batch_moving_average_window: int) -> None: ...

    @property
    def enable_batch_size_tuning(self) -> bool: ...

    @property
    def enable_max_num_tokens_tuning(self) -> bool: ...

    @property
    def dynamic_batch_moving_average_window(self) -> int: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class SchedulerConfig:
    def __init__(self, capacity_scheduler_policy: CapacitySchedulerPolicy = CapacitySchedulerPolicy.GUARANTEED_NO_EVICT, context_chunking_policy: ContextChunkingPolicy | None = None, dynamic_batch_config: DynamicBatchConfig | None = None) -> None: ...

    @property
    def capacity_scheduler_policy(self) -> CapacitySchedulerPolicy: ...

    @property
    def context_chunking_policy(self) -> ContextChunkingPolicy | None: ...

    @property
    def dynamic_batch_config(self) -> DynamicBatchConfig | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class RuntimeDefaults:
    def __init__(self, max_attention_window: Sequence[int] | None = None, sink_token_length: int | None = None) -> None: ...

    @property
    def max_attention_window(self) -> list[int] | None: ...

    @property
    def sink_token_length(self) -> int | None: ...

class KvCacheConfig:
    def __init__(self, enable_block_reuse: bool = True, max_tokens: int | None = None, max_attention_window: Sequence[int] | None = None, sink_token_length: int | None = None, free_gpu_memory_fraction: float | None = None, host_cache_size: int | None = None, onboard_blocks: bool = True, cross_kv_cache_fraction: float | None = None, secondary_offload_min_priority: int | None = None, event_buffer_max_size: int = 0, *, enable_partial_reuse: bool = True, copy_on_partial_reuse: bool = True, use_uvm: bool = False, attention_dp_events_gather_period_ms: int = 5, runtime_defaults: RuntimeDefaults | None = None, max_gpu_total_bytes: int = 0) -> None: ...

    @property
    def enable_block_reuse(self) -> bool: ...

    @enable_block_reuse.setter
    def enable_block_reuse(self, arg: bool, /) -> None: ...

    @property
    def max_tokens(self) -> int | None: ...

    @max_tokens.setter
    def max_tokens(self, arg: int, /) -> None: ...

    @property
    def max_attention_window(self) -> list[int] | None: ...

    @max_attention_window.setter
    def max_attention_window(self, arg: Sequence[int], /) -> None: ...

    @property
    def sink_token_length(self) -> int | None: ...

    @sink_token_length.setter
    def sink_token_length(self, arg: int, /) -> None: ...

    @property
    def free_gpu_memory_fraction(self) -> float | None: ...

    @free_gpu_memory_fraction.setter
    def free_gpu_memory_fraction(self, arg: float, /) -> None: ...

    @property
    def host_cache_size(self) -> int | None: ...

    @host_cache_size.setter
    def host_cache_size(self, arg: int, /) -> None: ...

    @property
    def onboard_blocks(self) -> bool: ...

    @onboard_blocks.setter
    def onboard_blocks(self, arg: bool, /) -> None: ...

    @property
    def cross_kv_cache_fraction(self) -> float | None: ...

    @cross_kv_cache_fraction.setter
    def cross_kv_cache_fraction(self, arg: float, /) -> None: ...

    @property
    def secondary_offload_min_priority(self) -> int | None: ...

    @secondary_offload_min_priority.setter
    def secondary_offload_min_priority(self, arg: int, /) -> None: ...

    @property
    def event_buffer_max_size(self) -> int: ...

    @event_buffer_max_size.setter
    def event_buffer_max_size(self, arg: int, /) -> None: ...

    @property
    def enable_partial_reuse(self) -> bool: ...

    @enable_partial_reuse.setter
    def enable_partial_reuse(self, arg: bool, /) -> None: ...

    @property
    def copy_on_partial_reuse(self) -> bool: ...

    @copy_on_partial_reuse.setter
    def copy_on_partial_reuse(self, arg: bool, /) -> None: ...

    @property
    def use_uvm(self) -> bool: ...

    @use_uvm.setter
    def use_uvm(self, arg: bool, /) -> None: ...

    @property
    def attention_dp_events_gather_period_ms(self) -> int: ...

    @attention_dp_events_gather_period_ms.setter
    def attention_dp_events_gather_period_ms(self, arg: int, /) -> None: ...

    @property
    def max_gpu_total_bytes(self) -> int: ...

    @max_gpu_total_bytes.setter
    def max_gpu_total_bytes(self, arg: int, /) -> None: ...

    def fill_empty_fields_from_runtime_defaults(self, arg: RuntimeDefaults, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class OrchestratorConfig:
    def __init__(self, is_orchestrator: bool = True, worker_executable_path: str = '', orch_leader_comm: bindings.MpiComm | None = None, spawn_processes: bool = True) -> None: ...

    @property
    def is_orchestrator(self) -> bool: ...

    @is_orchestrator.setter
    def is_orchestrator(self, arg: bool, /) -> None: ...

    @property
    def worker_executable_path(self) -> str: ...

    @worker_executable_path.setter
    def worker_executable_path(self, arg: str, /) -> None: ...

    @property
    def orch_leader_comm(self) -> bindings.MpiComm: ...

    @orch_leader_comm.setter
    def orch_leader_comm(self, arg: bindings.MpiComm, /) -> None: ...

    @property
    def spawn_processes(self) -> bool: ...

    @spawn_processes.setter
    def spawn_processes(self, arg: bool, /) -> None: ...

class ParallelConfig:
    def __init__(self, communication_type: CommunicationType = CommunicationType.MPI, communication_mode: CommunicationMode = CommunicationMode.LEADER, device_ids: Sequence[int] | None = None, participant_ids: Sequence[int] | None = None, orchestrator_config: OrchestratorConfig | None = None, num_nodes: int | None = None) -> None: ...

    @property
    def communication_type(self) -> CommunicationType: ...

    @communication_type.setter
    def communication_type(self, arg: CommunicationType, /) -> None: ...

    @property
    def communication_mode(self) -> CommunicationMode: ...

    @communication_mode.setter
    def communication_mode(self, arg: CommunicationMode, /) -> None: ...

    @property
    def device_ids(self) -> list[int] | None: ...

    @device_ids.setter
    def device_ids(self, arg: Sequence[int], /) -> None: ...

    @property
    def participant_ids(self) -> list[int] | None: ...

    @participant_ids.setter
    def participant_ids(self, arg: Sequence[int], /) -> None: ...

    @property
    def orchestrator_config(self) -> OrchestratorConfig | None: ...

    @orchestrator_config.setter
    def orchestrator_config(self, arg: OrchestratorConfig, /) -> None: ...

    @property
    def num_nodes(self) -> int | None: ...

    @num_nodes.setter
    def num_nodes(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class PeftCacheConfig:
    def __init__(self, num_host_module_layer: int = 0, num_device_module_layer: int = 0, optimal_adapter_size: int = 8, max_adapter_size: int = 64, num_put_workers: int = 1, num_ensure_workers: int = 1, num_copy_streams: int = 1, max_pages_per_block_host: int = 24, max_pages_per_block_device: int = 8, device_cache_percent: float | None = None, host_cache_size: int | None = None, lora_prefetch_dir: str | None = None) -> None: ...

    @property
    def num_host_module_layer(self) -> int: ...

    @property
    def num_device_module_layer(self) -> int: ...

    @property
    def optimal_adapter_size(self) -> int: ...

    @property
    def max_adapter_size(self) -> int: ...

    @property
    def num_put_workers(self) -> int: ...

    @property
    def num_ensure_workers(self) -> int: ...

    @property
    def num_copy_streams(self) -> int: ...

    @property
    def max_pages_per_block_host(self) -> int: ...

    @property
    def max_pages_per_block_device(self) -> int: ...

    @property
    def device_cache_percent(self) -> float | None: ...

    @property
    def host_cache_size(self) -> int | None: ...

    @property
    def lora_prefetch_dir(self) -> str | None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class DecodingConfig:
    def __init__(self, decoding_mode: DecodingMode | None = None, lookahead_decoding_config: LookaheadDecodingConfig | None = None, medusa_choices: Sequence[Sequence[int]] | None = None, eagle_config: EagleConfig | None = None) -> None: ...

    @property
    def decoding_mode(self) -> DecodingMode | None: ...

    @decoding_mode.setter
    def decoding_mode(self, arg: DecodingMode, /) -> None: ...

    @property
    def lookahead_decoding_config(self) -> LookaheadDecodingConfig | None: ...

    @lookahead_decoding_config.setter
    def lookahead_decoding_config(self, arg: LookaheadDecodingConfig, /) -> None: ...

    @property
    def medusa_choices(self) -> list[list[int]] | None: ...

    @medusa_choices.setter
    def medusa_choices(self, arg: Sequence[Sequence[int]], /) -> None: ...

    @property
    def eagle_config(self) -> EagleConfig | None: ...

    @eagle_config.setter
    def eagle_config(self, arg: EagleConfig, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class DebugConfig:
    def __init__(self, debug_input_tensors: bool = False, debug_output_tensors: bool = False, debug_tensor_names: Sequence[str] | None = None, debug_tensors_max_iterations: int = False) -> None: ...

    @property
    def debug_input_tensors(self) -> bool: ...

    @debug_input_tensors.setter
    def debug_input_tensors(self, arg: bool, /) -> None: ...

    @property
    def debug_output_tensors(self) -> bool: ...

    @debug_output_tensors.setter
    def debug_output_tensors(self, arg: bool, /) -> None: ...

    @property
    def debug_tensor_names(self) -> list[str]: ...

    @debug_tensor_names.setter
    def debug_tensor_names(self, arg: Sequence[str], /) -> None: ...

    @property
    def debug_tensors_max_iterations(self) -> int: ...

    @debug_tensors_max_iterations.setter
    def debug_tensors_max_iterations(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class LogitsPostProcessorConfig:
    def __init__(self, processor_map: Mapping[str, Callable[[int, torch.Tensor, Sequence[Sequence[int]], int, int | None], None]] | None = None, processor_batched: Callable[[Sequence[int], Sequence[torch.Tensor], List[Sequence[Sequence[int]]], int, Sequence[int | None]], None] | None = None, replicate: bool = True) -> None: ...

    @property
    def processor_map(self) -> dict[str, Callable[[int, torch.Tensor, list[list[int]], int, int | None], None]] | None: ...

    @processor_map.setter
    def processor_map(self, arg: Mapping[str, Callable[[int, torch.Tensor, Sequence[Sequence[int]], int, int | None], None]], /) -> None: ...

    @property
    def processor_batched(self) -> Callable[[list[int], list[torch.Tensor], List[list[list[int]]], int, list[int | None]], None] | None: ...

    @processor_batched.setter
    def processor_batched(self, arg: Callable[[Sequence[int], Sequence[torch.Tensor], List[Sequence[Sequence[int]]], int, Sequence[int | None]], None], /) -> None: ...

    @property
    def replicate(self) -> bool: ...

    @replicate.setter
    def replicate(self, arg: bool, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class ExtendedRuntimePerfKnobConfig:
    def __init__(self, multi_block_mode: bool = True, enable_context_fmha_fp32_acc: bool = False) -> None: ...

    @property
    def multi_block_mode(self) -> bool: ...

    @multi_block_mode.setter
    def multi_block_mode(self, arg: bool, /) -> None: ...

    @property
    def enable_context_fmha_fp32_acc(self) -> bool: ...

    @enable_context_fmha_fp32_acc.setter
    def enable_context_fmha_fp32_acc(self, arg: bool, /) -> None: ...

    @property
    def cuda_graph_mode(self) -> bool: ...

    @cuda_graph_mode.setter
    def cuda_graph_mode(self, arg: bool, /) -> None: ...

    @property
    def cuda_graph_cache_size(self) -> int: ...

    @cuda_graph_cache_size.setter
    def cuda_graph_cache_size(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class SpeculativeDecodingConfig:
    def __init__(self, fast_logits: bool = False) -> None: ...

    @property
    def fast_logits(self) -> bool: ...

    @fast_logits.setter
    def fast_logits(self, arg: bool, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class GuidedDecodingConfig:
    def __init__(self, backend: GuidedDecodingConfig.GuidedDecodingBackend, encoded_vocab: Sequence[str] | None = None, tokenizer_str: str | None = None, stop_token_ids: Sequence[int] | None = None) -> None: ...

    class GuidedDecodingBackend(enum.Enum):
        XGRAMMAR = 0

        LLGUIDANCE = 1

    @property
    def backend(self) -> GuidedDecodingConfig.GuidedDecodingBackend: ...

    @backend.setter
    def backend(self, arg: GuidedDecodingConfig.GuidedDecodingBackend, /) -> None: ...

    @property
    def encoded_vocab(self) -> list[str] | None: ...

    @encoded_vocab.setter
    def encoded_vocab(self, arg: Sequence[str], /) -> None: ...

    @property
    def tokenizer_str(self) -> str | None: ...

    @tokenizer_str.setter
    def tokenizer_str(self, arg: str, /) -> None: ...

    @property
    def stop_token_ids(self) -> list[int] | None: ...

    @stop_token_ids.setter
    def stop_token_ids(self, arg: Sequence[int], /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class CacheTransceiverBackendType(enum.Enum):
    DEFAULT = 0

    MPI = 1

    UCX = 2

    NIXL = 3

    def from_string(self) -> CacheTransceiverBackendType: ...

class CacheTransceiverConfig:
    def __init__(self, backend: CacheTransceiverBackendType | None = None, max_tokens_in_buffer: int | None = None, kv_transfer_timeout_ms: int | None = None) -> None: ...

    @property
    def backend(self) -> CacheTransceiverBackendType | None: ...

    @backend.setter
    def backend(self, arg: CacheTransceiverBackendType, /) -> None: ...

    @property
    def max_tokens_in_buffer(self) -> int | None: ...

    @max_tokens_in_buffer.setter
    def max_tokens_in_buffer(self, arg: int, /) -> None: ...

    @property
    def kv_transfer_timeout_ms(self) -> int | None: ...

    @kv_transfer_timeout_ms.setter
    def kv_transfer_timeout_ms(self, arg: int, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class ExecutorConfig:
    def __init__(self, max_beam_width: int = 1, scheduler_config: SchedulerConfig = ..., kv_cache_config: KvCacheConfig = ..., enable_chunked_context: bool = False, normalize_log_probs: bool = True, iter_stats_max_iterations: int = 1000, request_stats_max_iterations: int = 0, batching_type: BatchingType = BatchingType.INFLIGHT, max_batch_size: int | None = None, max_num_tokens: int | None = None, parallel_config: ParallelConfig | None = None, peft_cache_config: PeftCacheConfig = ..., logits_post_processor_config: LogitsPostProcessorConfig | None = None, decoding_config: DecodingConfig | None = None, use_gpu_direct_storage: bool = False, gpu_weights_percent: float = 1.0, max_queue_size: int | None = None, extended_runtime_perf_knob_config: ExtendedRuntimePerfKnobConfig = ..., debug_config: DebugConfig | None = None, recv_poll_period_ms: int = 0, max_seq_idle_microseconds: int = 180000000, spec_dec_config: SpeculativeDecodingConfig | None = None, guided_decoding_config: GuidedDecodingConfig | None = None, additional_model_outputs: Sequence[AdditionalModelOutput] | None = None, cache_transceiver_config: CacheTransceiverConfig | None = None, gather_generation_logits: bool = False, mm_embedding_offloading: bool = False, enable_trt_overlap: bool = False, fail_fast_on_attention_window_too_large: bool = False) -> None: ...

    @property
    def max_beam_width(self) -> int: ...

    @max_beam_width.setter
    def max_beam_width(self, arg: int, /) -> None: ...

    @property
    def max_batch_size(self) -> int | None: ...

    @max_batch_size.setter
    def max_batch_size(self, arg: int, /) -> None: ...

    @property
    def max_num_tokens(self) -> int | None: ...

    @max_num_tokens.setter
    def max_num_tokens(self, arg: int, /) -> None: ...

    @property
    def scheduler_config(self) -> SchedulerConfig: ...

    @scheduler_config.setter
    def scheduler_config(self, arg: SchedulerConfig, /) -> None: ...

    @property
    def kv_cache_config(self) -> KvCacheConfig: ...

    @kv_cache_config.setter
    def kv_cache_config(self, arg: KvCacheConfig, /) -> None: ...

    @property
    def enable_chunked_context(self) -> bool: ...

    @enable_chunked_context.setter
    def enable_chunked_context(self, arg: bool, /) -> None: ...

    @property
    def normalize_log_probs(self) -> bool: ...

    @normalize_log_probs.setter
    def normalize_log_probs(self, arg: bool, /) -> None: ...

    @property
    def iter_stats_max_iterations(self) -> int: ...

    @iter_stats_max_iterations.setter
    def iter_stats_max_iterations(self, arg: int, /) -> None: ...

    @property
    def request_stats_max_iterations(self) -> int: ...

    @request_stats_max_iterations.setter
    def request_stats_max_iterations(self, arg: int, /) -> None: ...

    @property
    def batching_type(self) -> BatchingType: ...

    @batching_type.setter
    def batching_type(self, arg: BatchingType, /) -> None: ...

    @property
    def parallel_config(self) -> ParallelConfig | None: ...

    @parallel_config.setter
    def parallel_config(self, arg: ParallelConfig, /) -> None: ...

    @property
    def peft_cache_config(self) -> PeftCacheConfig | None: ...

    @peft_cache_config.setter
    def peft_cache_config(self, arg: PeftCacheConfig, /) -> None: ...

    @property
    def logits_post_processor_config(self) -> LogitsPostProcessorConfig | None: ...

    @logits_post_processor_config.setter
    def logits_post_processor_config(self, arg: LogitsPostProcessorConfig, /) -> None: ...

    @property
    def decoding_config(self) -> DecodingConfig | None: ...

    @decoding_config.setter
    def decoding_config(self, arg: DecodingConfig, /) -> None: ...

    @property
    def use_gpu_direct_storage(self) -> bool: ...

    @use_gpu_direct_storage.setter
    def use_gpu_direct_storage(self, arg: bool, /) -> None: ...

    @property
    def gpu_weights_percent(self) -> float: ...

    @gpu_weights_percent.setter
    def gpu_weights_percent(self, arg: float, /) -> None: ...

    @property
    def max_queue_size(self) -> int | None: ...

    @max_queue_size.setter
    def max_queue_size(self, arg: int, /) -> None: ...

    @property
    def extended_runtime_perf_knob_config(self) -> ExtendedRuntimePerfKnobConfig: ...

    @extended_runtime_perf_knob_config.setter
    def extended_runtime_perf_knob_config(self, arg: ExtendedRuntimePerfKnobConfig, /) -> None: ...

    @property
    def debug_config(self) -> DebugConfig | None: ...

    @debug_config.setter
    def debug_config(self, arg: DebugConfig, /) -> None: ...

    @property
    def recv_poll_period_ms(self) -> int: ...

    @recv_poll_period_ms.setter
    def recv_poll_period_ms(self, arg: int, /) -> None: ...

    @property
    def max_seq_idle_microseconds(self) -> int: ...

    @max_seq_idle_microseconds.setter
    def max_seq_idle_microseconds(self, arg: int, /) -> None: ...

    @property
    def spec_dec_config(self) -> SpeculativeDecodingConfig | None: ...

    @spec_dec_config.setter
    def spec_dec_config(self, arg: SpeculativeDecodingConfig, /) -> None: ...

    @property
    def guided_decoding_config(self) -> GuidedDecodingConfig | None: ...

    @guided_decoding_config.setter
    def guided_decoding_config(self, arg: GuidedDecodingConfig, /) -> None: ...

    @property
    def additional_model_outputs(self) -> list[AdditionalModelOutput] | None: ...

    @additional_model_outputs.setter
    def additional_model_outputs(self, arg: Sequence[AdditionalModelOutput], /) -> None: ...

    @property
    def cache_transceiver_config(self) -> CacheTransceiverConfig | None: ...

    @cache_transceiver_config.setter
    def cache_transceiver_config(self, arg: CacheTransceiverConfig, /) -> None: ...

    @property
    def gather_generation_logits(self) -> bool: ...

    @gather_generation_logits.setter
    def gather_generation_logits(self, arg: bool, /) -> None: ...

    @property
    def mm_embedding_offloading(self) -> bool: ...

    @mm_embedding_offloading.setter
    def mm_embedding_offloading(self, arg: bool, /) -> None: ...

    @property
    def enable_trt_overlap(self) -> bool: ...

    @enable_trt_overlap.setter
    def enable_trt_overlap(self, arg: bool, /) -> None: ...

    @property
    def fail_fast_on_attention_window_too_large(self) -> bool: ...

    @fail_fast_on_attention_window_too_large.setter
    def fail_fast_on_attention_window_too_large(self, arg: bool, /) -> None: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class Executor:
    @overload
    def __init__(self, model_path: str | os.PathLike, model_type: ModelType, executor_config: ExecutorConfig) -> None: ...

    @overload
    def __init__(self, encoder_model_path: str | os.PathLike, decoder_model_path: str | os.PathLike, model_type: ModelType, executor_config: ExecutorConfig) -> None: ...

    @overload
    def __init__(self, engine_buffer: bytes, json_config_str: str, model_type: ModelType, executor_config: ExecutorConfig, managed_weights: dict = {}) -> None: ...

    @overload
    def __init__(self, encoder_engine_buffer: str, encoder_json_config_str: str, decoder_engine_buffer: str, decoder_json_config_str: str, model_type: ModelType, executor_config: ExecutorConfig) -> None: ...

    def shutdown(self) -> None: ...

    def __enter__(self) -> object: ...

    def __exit__(self, type: object | None, value: object | None, traceback: object | None) -> None: ...

    def enqueue_request(self, request: Request) -> int: ...

    def enqueue_requests(self, requests: Sequence[Request]) -> list[int]: ...

    @overload
    def await_responses(self, timeout: datetime.timedelta | float | None = None) -> list[Response]: ...

    @overload
    def await_responses(self, id: int, timeout: datetime.timedelta | float | None = None) -> list[Response]: ...

    @overload
    def await_responses(self, ids: Sequence[int], timeout: datetime.timedelta | float | None = None) -> list[list[Response]]: ...

    def get_num_responses_ready(self, id: int | None = None) -> int: ...

    def cancel_request(self, id: int | None = None) -> None: ...

    def get_latest_iteration_stats(self) -> List: ...

    def get_latest_request_stats(self) -> List: ...

    def get_latest_debug_tensors(self) -> List: ...

    def can_enqueue_requests(self) -> bool: ...

    def get_kv_cache_event_manager(self) -> kv_cache.KVCacheEventManager | None: ...
