from collections.abc import Iterable, Iterator, Mapping, Sequence
import datetime
import enum
from typing import overload

import torch

import bindings
import bindings.executor
import bindings.executor.kv_cache
import bindings.internal.runtime


class PeftTaskNotCachedException(Exception):
    pass

class LoraCacheFullException(Exception):
    pass

class ReqIdsSet:
    def __init__(self) -> None: ...

    def clear(self) -> None: ...

    def size(self) -> int: ...

    def insert(self, arg: int, /) -> None: ...

    def erase(self, arg: int, /) -> int: ...

    def __len__(self) -> int: ...

    def __contains__(self, arg: int, /) -> bool: ...

    def __iter__(self) -> Iterator[int]: ...

    def __eq__(self, arg: ReqIdsSet, /) -> bool: ...

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class LlmRequestType(enum.Enum):
    LLMREQUEST_TYPE_CONTEXT_AND_GENERATION = 0

    LLMREQUEST_TYPE_CONTEXT_ONLY = 1

    LLMREQUEST_TYPE_GENERATION_ONLY = 2

LLMREQUEST_TYPE_CONTEXT_AND_GENERATION: LlmRequestType = LlmRequestType.LLMREQUEST_TYPE_CONTEXT_AND_GENERATION

LLMREQUEST_TYPE_CONTEXT_ONLY: LlmRequestType = LlmRequestType.LLMREQUEST_TYPE_CONTEXT_ONLY

LLMREQUEST_TYPE_GENERATION_ONLY: LlmRequestType = LlmRequestType.LLMREQUEST_TYPE_GENERATION_ONLY

class ContextChunkingConfig:
    def __init__(self, chunking_policy: bindings.executor.ContextChunkingPolicy, chunk_unit_size: int) -> None: ...

    @property
    def chunking_policy(self) -> bindings.executor.ContextChunkingPolicy: ...

    @chunking_policy.setter
    def chunking_policy(self, arg: bindings.executor.ContextChunkingPolicy, /) -> None: ...

    @property
    def chunk_unit_size(self) -> int: ...

    @chunk_unit_size.setter
    def chunk_unit_size(self, arg: int, /) -> None: ...

class GenericLlmRequest:
    def set_exclude_input_from_output(self, exclude: bool) -> None: ...

    def get_num_tokens(self, beam: int) -> int: ...

    @property
    def max_beam_num_tokens(self) -> int: ...

    def get_token(self, beam: int, pos: int) -> int: ...

    @overload
    def get_tokens(self, beam: int) -> list[int]: ...

    @overload
    def get_tokens(self) -> list[list[int]]: ...

    @overload
    def get_last_tokens(self, beam: int) -> int: ...

    @overload
    def get_last_tokens(self) -> list[int]: ...

    def get_beam_width_by_iter(self, for_next_iteration: bool = False) -> int: ...

    @property
    def max_num_generated_tokens(self) -> int: ...

    def add_new_token(self, token: int, beam: int) -> int: ...

    def add_new_tokens(self, beam_tokens: Sequence[int]) -> None: ...

    @property
    def num_draft_tokens(self) -> int: ...

    def set_generated_tokens(self, generated_beam_tokens: Sequence[Sequence[int]]) -> None: ...

    def pause(self, max_input_len: int) -> None: ...

    @property
    def max_sent_token_len(self) -> int: ...

    @max_sent_token_len.setter
    def max_sent_token_len(self, arg: int, /) -> None: ...

    @property
    def prompt_embedding_table(self) -> torch.Tensor | None: ...

    @property
    def multimodal_embedding(self) -> torch.Tensor | None: ...

    @property
    def mrope_rotary_cos_sin(self) -> torch.Tensor | None: ...

    @property
    def bad_words_list(self) -> torch.Tensor | None: ...

    @property
    def draft_logits(self) -> torch.Tensor | None: ...

    @draft_logits.setter
    def draft_logits(self, arg: torch.Tensor, /) -> None: ...

    @property
    def embedding_bias(self) -> torch.Tensor | None: ...

    @property
    def lora_config(self) -> torch.Tensor | None: ...

    @lora_config.setter
    def lora_config(self, arg: torch.Tensor, /) -> None: ...

    @property
    def lora_weights(self) -> torch.Tensor | None: ...

    @lora_weights.setter
    def lora_weights(self, arg: torch.Tensor, /) -> None: ...

    @property
    def stop_words_list(self) -> torch.Tensor | None: ...

    @property
    def context_logits(self) -> torch.Tensor: ...

    @property
    def generation_logits(self) -> torch.Tensor: ...

    @property
    def prompt_vocab_size(self) -> int | None: ...

    @property
    def mrope_position_deltas(self) -> int | None: ...

    @property
    def lora_task_id(self) -> int | None: ...

    @property
    def lookahead_config(self) -> bindings.executor.LookaheadDecodingConfig | None: ...

    @property
    def context_chunk_size(self) -> int: ...

    @context_chunk_size.setter
    def context_chunk_size(self, arg: int, /) -> None: ...

    @property
    def decoding_iter(self) -> int: ...

    @decoding_iter.setter
    def decoding_iter(self, arg: int, /) -> None: ...

    @property
    def request_id(self) -> int: ...

    @request_id.setter
    def request_id(self, arg: int, /) -> None: ...

    @property
    def prompt_len(self) -> int: ...

    @prompt_len.setter
    def prompt_len(self, arg: int, /) -> None: ...

    @property
    def max_new_tokens(self) -> int: ...

    @max_new_tokens.setter
    def max_new_tokens(self, arg: int, /) -> None: ...

    @property
    def sampling_config(self) -> bindings.SamplingConfig: ...

    @sampling_config.setter
    def sampling_config(self, arg: bindings.SamplingConfig, /) -> None: ...

    @property
    def state(self) -> bindings.LlmRequestState: ...

    @state.setter
    def state(self, arg: bindings.LlmRequestState, /) -> None: ...

    @property
    def streaming(self) -> bool: ...

    @streaming.setter
    def streaming(self, arg: bool, /) -> None: ...

    @property
    def end_id(self) -> int | None: ...

    @end_id.setter
    def end_id(self, arg: int, /) -> None: ...

    @property
    def pad_id(self) -> int | None: ...

    @pad_id.setter
    def pad_id(self, arg: int, /) -> None: ...

    @property
    def seq_slot(self) -> int | None: ...

    @seq_slot.setter
    def seq_slot(self, arg: int, /) -> None: ...

    @property
    def return_log_probs(self) -> bool: ...

    @property
    def return_context_logits(self) -> bool: ...

    @property
    def return_generation_logits(self) -> bool: ...

    @property
    def log_probs(self) -> list[list[float]]: ...

    def get_log_probs(self, arg: int, /) -> list[float]: ...

    def set_log_probs(self, log_probs: Sequence[float], beam: int) -> None: ...

    def set_return_encoder_output(self, return_encoder_output: bool) -> None: ...

    def get_return_encoder_output(self) -> bool: ...

    def priority(self) -> float: ...

    def set_priority(self, arg: float, /) -> None: ...

    @property
    def cum_log_probs(self) -> list[float]: ...

    def set_cum_log_prob(self, cum_log_prob: float, beam: int) -> None: ...

    def update_num_tokens_per_iteration(self, num_tokens_per_iteration: int, model_config: bindings.ModelConfig) -> None: ...

    @property
    def orig_prompt_len(self) -> int: ...

    def has_draft_tokens(self) -> bool: ...

    def move_to_next_context_chunk(self) -> None: ...

    @property
    def is_last_context_chunk(self) -> bool: ...

    @property
    def is_first_context_chunk(self) -> bool: ...

    @property
    def context_remaining_length(self) -> int: ...

    def set_finished_reason(self, finish_reason: bindings.executor.FinishReason, beam: int) -> None: ...

    @property
    def is_finished(self) -> bool: ...

    @property
    def is_finished_due_to_length(self) -> bool: ...

    @property
    def context_current_position(self) -> int: ...

    @context_current_position.setter
    def context_current_position(self, arg: int, /) -> None: ...

    @property
    def prepopulated_prompt_len(self) -> int: ...

    @property
    def guided_decoding_params(self) -> bindings.executor.GuidedDecodingParams | None: ...

    @guided_decoding_params.setter
    def guided_decoding_params(self, arg: bindings.executor.GuidedDecodingParams, /) -> None: ...

    @property
    def context_phase_params(self) -> bindings.executor.ContextPhaseParams | None: ...

    @property
    def is_context_only_request(self) -> bool: ...

    @property
    def is_generation_only_request(self) -> bool: ...

    @property
    def is_generation_complete_state(self) -> bool: ...

    @property
    def is_context_finished(self) -> bool: ...

    @property
    def is_disagg_generation_init_state(self) -> bool: ...

    @property
    def is_disagg_generation_transmission_complete(self) -> bool: ...

    @property
    def is_disagg_generation_transmission_in_progress(self) -> bool: ...

    @property
    def is_context_init_state(self) -> bool: ...

    @property
    def is_generation_in_progress_state(self) -> bool: ...

    @property
    def is_disagg_context_transmission_state(self) -> bool: ...

    @property
    def is_disagg_context_complete_state(self) -> bool: ...

    @property
    def stage(self) -> bindings.executor.RequestStage: ...

    @property
    def kv_cache_transfer_time_ms(self) -> float: ...

    @property
    def kv_cache_size(self) -> int: ...

    @property
    def avg_decoded_tokens_per_iter(self) -> float: ...

    @property
    def alloc_total_blocks(self) -> int: ...

    @property
    def alloc_new_blocks(self) -> int: ...

    def alloc_context_logits(self, vocab_size: int, logit_dtype: bindings.DataType) -> None: ...

    @property
    def reused_blocks(self) -> int: ...

    @property
    def missed_blocks(self) -> int: ...

    @property
    def kv_cache_hit_rate(self) -> float: ...

    @property
    def llm_request_type(self) -> LlmRequestType: ...

    @property
    def parent_request_id(self) -> int: ...

    @property
    def is_child(self) -> bool: ...

    @property
    def cache_salt_id(self) -> int | None: ...

    @property
    def multimodal_hashes(self) -> list[list[int]] | None: ...

    @property
    def multimodal_positions(self) -> list[int] | None: ...

    @property
    def multimodal_lengths(self) -> list[int] | None: ...

    @property
    def position_ids(self) -> list[int] | None: ...

    @property
    def draft_tokens(self) -> list[int] | None: ...

    @draft_tokens.setter
    def draft_tokens(self, arg: Sequence[int], /) -> None: ...

    @property
    def is_dummy_request(self) -> bool: ...

    @is_dummy_request.setter
    def is_dummy_request(self, arg: bool, /) -> None: ...

    @property
    def return_perf_metrics(self) -> bool: ...

    @property
    def use_draft_model(self) -> bool: ...

    @use_draft_model.setter
    def use_draft_model(self, arg: bool, /) -> None: ...

class LlmRequest(GenericLlmRequest):
    @overload
    def __init__(self, request_id: int, max_new_tokens: int, input_tokens: Sequence[int], sampling_config: bindings.SamplingConfig, is_streaming: bool, end_id: int | None = None, pad_id: int | None = None, embedding_bias: torch.Tensor | None = None, bad_words_list: torch.Tensor | None = None, stop_words_list: torch.Tensor | None = None, position_ids: Sequence[int] | None = None, prompt_embedding_table: torch.Tensor | None = None, prompt_vocab_size: int | None = None, multimodal_hashes: Sequence[Sequence[int]] | None = None, multimodal_positions: Sequence[int] | None = None, multimodal_lengths: Sequence[int] | None = None, multimodal_embedding: torch.Tensor | None = None, mrope_rotary_cos_sin: torch.Tensor | None = None, mrope_position_deltas: int | None = None, lora_task_id: int | None = None, lora_weights: torch.Tensor | None = None, lora_config: torch.Tensor | None = None, lookahead_config: bindings.executor.LookaheadDecodingConfig | None = None, kv_cache_retention_config: bindings.executor.KvCacheRetentionConfig | None = None, return_log_probs: bool = False, return_context_logits: bool = False, return_generation_logits: bool = False, draft_tokens: Sequence[int] | None = None, draft_logits: torch.Tensor | None = None, exclude_input_from_output: bool = False, logits_post_processor: "std::function<void (unsigned long, std::shared_ptr<tensorrt_llm::runtime::ITensor>&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::shared_ptr<tensorrt_llm::runtime::CudaStream> const&, std::optional<unsigned long>)>" | None = None, apply_logits_post_processor_batched: bool = False, encoder_input_tokens: Sequence[int] | None = None, return_encoder_output: bool = False, client_id: int | None = None, priority: float = 0.5, encoder_input_features: torch.Tensor | None = None, encoder_output_len: int | None = None, cross_attention_mask: torch.Tensor | None = None, llm_request_type: LlmRequestType = LlmRequestType.LLMREQUEST_TYPE_CONTEXT_AND_GENERATION, input_token_extra_ids: Sequence[int] | None = None, num_return_sequences: int = 1, eagle_config: bindings.executor.EagleConfig | None = None, skip_cross_attn_blocks: torch.Tensor | None = None, return_perf_metrics: bool = False, guided_decoding_params: bindings.executor.GuidedDecodingParams | None = None, language_adapter_uid: int | None = None, allotted_time_ms: datetime.timedelta | float | None = None, context_phase_params: bindings.executor.ContextPhaseParams | None = None, cache_salt_id: int | None = None, arrival_time: datetime.timedelta | float | None = None) -> None: ...

    @overload
    def __init__(self, arg: LlmRequest) -> None: ...

    def check_token_id_range(self, vocab_size: int) -> bool: ...

    def validate(self, max_input_len: int, max_seq_len: int, max_draft_len: int, vocab_size_padded: int, max_endocer_input_len: int | None = None, enable_kv_cache_reuse: bool = False) -> None: ...

    def create_response(self, use_fast_logits: bool = False, mpi_world_rank: int = 0) -> bindings.executor.Response | None: ...

    def create_child_request(self, child_id: int) -> LlmRequest: ...

    def create_result(self, use_fast_logits: bool = False, mpi_world_rank: int = 0) -> bindings.executor.Result | None: ...

    def create_serialized_result(self, arg0: bool, arg1: int, /) -> tuple[bytes, bool]: ...

    def move_prompt_embedding_table_to_gpu(self, manager: bindings.internal.runtime.BufferManager) -> None: ...

    def move_lora_weights_to_gpu(self, manager: bindings.internal.runtime.BufferManager) -> None: ...

    def finish_by_reason(self, finish_reason: bindings.executor.FinishReason) -> None: ...

    def set_first_scheduled_time(self) -> None: ...

    def update_perf_metrics(self, iter_counter: int) -> None: ...

    def remove_lora_tensors(self) -> None: ...

    global_steady_clock_offset: datetime.timedelta | None = ...
    """(arg: object, /) -> datetime.timedelta | None"""

class SequenceSlotManager:
    def __init__(self, max_num_slots: int, max_sequence_idle_microseconds: int) -> None: ...

    def get_sequence_slot(self, start_flag: bool, sequence_id: int) -> int | None: ...

    def free_sequence_slot(self, sequence_id: int) -> None: ...

    def free_idle_sequence_slots(self) -> None: ...

class RnnStateManager:
    def __init__(self, max_num_sequences: int, model_config: bindings.ModelConfig, world_config: bindings.WorldConfig, buffer_manager: bindings.internal.runtime.BufferManager) -> None: ...

class DecoderInputBuffers:
    def __init__(self, max_batch_size: int, max_tokens_per_engine_step: int, manager: bindings.internal.runtime.BufferManager) -> None: ...

    @property
    def setup_batch_slots(self) -> torch.Tensor: ...

    @setup_batch_slots.setter
    def setup_batch_slots(self, arg: torch.Tensor, /) -> None: ...

    @property
    def setup_batch_slots_device(self) -> torch.Tensor: ...

    @setup_batch_slots_device.setter
    def setup_batch_slots_device(self, arg: torch.Tensor, /) -> None: ...

    @property
    def fill_values(self) -> torch.Tensor: ...

    @fill_values.setter
    def fill_values(self, arg: torch.Tensor, /) -> None: ...

    @property
    def fill_values_device(self) -> torch.Tensor: ...

    @fill_values_device.setter
    def fill_values_device(self, arg: torch.Tensor, /) -> None: ...

    @property
    def inputs_ids(self) -> torch.Tensor: ...

    @inputs_ids.setter
    def inputs_ids(self, arg: torch.Tensor, /) -> None: ...

    @property
    def forward_batch_slots(self) -> list[torch.Tensor]: ...

    @forward_batch_slots.setter
    def forward_batch_slots(self, arg: Sequence[torch.Tensor], /) -> None: ...

    @property
    def logits(self) -> list[torch.Tensor]: ...

    @logits.setter
    def logits(self, arg: Sequence[torch.Tensor], /) -> None: ...

    @property
    def decoder_requests(self) -> list[LlmRequest]: ...

    @decoder_requests.setter
    def decoder_requests(self, arg: Sequence[LlmRequest], /) -> None: ...

class DecoderOutputBuffers:
    @property
    def sequence_lengths_host(self) -> torch.Tensor: ...

    @sequence_lengths_host.setter
    def sequence_lengths_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def finished_sum_host(self) -> torch.Tensor: ...

    @finished_sum_host.setter
    def finished_sum_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def new_output_tokens_host(self) -> torch.Tensor: ...

    @property
    def cum_log_probs_host(self) -> torch.Tensor: ...

    @cum_log_probs_host.setter
    def cum_log_probs_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def log_probs_host(self) -> torch.Tensor: ...

    @log_probs_host.setter
    def log_probs_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def finish_reasons_host(self) -> torch.Tensor: ...

    @finish_reasons_host.setter
    def finish_reasons_host(self, arg: torch.Tensor, /) -> None: ...

class SlotDecoderBuffers:
    def __init__(self, max_beam_width: int, max_seq_len: int, buffer_manager: bindings.internal.runtime.BufferManager) -> None: ...

    @property
    def output_ids(self) -> torch.Tensor: ...

    @output_ids.setter
    def output_ids(self, arg: torch.Tensor, /) -> None: ...

    @property
    def output_ids_host(self) -> torch.Tensor: ...

    @output_ids_host.setter
    def output_ids_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def sequence_lengths_host(self) -> torch.Tensor: ...

    @sequence_lengths_host.setter
    def sequence_lengths_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def cum_log_probs(self) -> torch.Tensor: ...

    @cum_log_probs.setter
    def cum_log_probs(self, arg: torch.Tensor, /) -> None: ...

    @property
    def cum_log_probs_host(self) -> torch.Tensor: ...

    @cum_log_probs_host.setter
    def cum_log_probs_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def log_probs(self) -> torch.Tensor: ...

    @log_probs.setter
    def log_probs(self, arg: torch.Tensor, /) -> None: ...

    @property
    def log_probs_host(self) -> torch.Tensor: ...

    @log_probs_host.setter
    def log_probs_host(self, arg: torch.Tensor, /) -> None: ...

    @property
    def finish_reasons_host(self) -> torch.Tensor: ...

    @finish_reasons_host.setter
    def finish_reasons_host(self, arg: torch.Tensor, /) -> None: ...

def add_new_tokens_to_requests(requests: Sequence[LlmRequest], tokens: Sequence[int], beam_idx: int) -> None:
    """
    Add new tokens to multiple LLM requests. The tokens vector should contain tokens for beam beam_idx of all requests in order.
    """

def make_decoding_batch_input(context_requests: Sequence[LlmRequest], generation_requests: Sequence[LlmRequest], logits: torch.Tensor, beam_width: int, num_context_logits_prefix_sum: Sequence[int], decoder_input_buffers: DecoderInputBuffers, decoder_state: bindings.internal.runtime.DecoderState, buffer_manager: bindings.internal.runtime.BufferManager) -> bindings.internal.runtime.DecoderBatchInput:
    """Make decoding batch input."""

class KvCacheConnectorManager:
    def __init__(self) -> None: ...

    def get_num_new_matched_tokens(self, request: LlmRequest, num_computed_tokens: int) -> int: ...

class KvCacheStats:
    def __init__(self) -> None: ...

    @property
    def max_num_blocks(self) -> int: ...

    @max_num_blocks.setter
    def max_num_blocks(self, arg: int, /) -> None: ...

    @property
    def free_num_blocks(self) -> int: ...

    @free_num_blocks.setter
    def free_num_blocks(self, arg: int, /) -> None: ...

    @property
    def used_num_blocks(self) -> int: ...

    @used_num_blocks.setter
    def used_num_blocks(self, arg: int, /) -> None: ...

    @property
    def tokens_per_block(self) -> int: ...

    @tokens_per_block.setter
    def tokens_per_block(self, arg: int, /) -> None: ...

    @property
    def alloc_total_blocks(self) -> int: ...

    @alloc_total_blocks.setter
    def alloc_total_blocks(self, arg: int, /) -> None: ...

    @property
    def alloc_new_blocks(self) -> int: ...

    @alloc_new_blocks.setter
    def alloc_new_blocks(self, arg: int, /) -> None: ...

    @property
    def reused_blocks(self) -> int: ...

    @reused_blocks.setter
    def reused_blocks(self, arg: int, /) -> None: ...

    @property
    def missed_blocks(self) -> int: ...

    @missed_blocks.setter
    def missed_blocks(self, arg: int, /) -> None: ...

    @property
    def cache_hit_rate(self) -> float: ...

    @cache_hit_rate.setter
    def cache_hit_rate(self, arg: float, /) -> None: ...

    @property
    def num_free_blocks_per_window_size(self) -> dict[int, int]: ...

    @num_free_blocks_per_window_size.setter
    def num_free_blocks_per_window_size(self, arg: Mapping[int, int], /) -> None: ...

    @property
    def allocated_bytes(self) -> int: ...

class TempAttentionWindowInputs:
    def __init__(self) -> None: ...

    @property
    def paged_context_fmha(self) -> bool: ...

    @paged_context_fmha.setter
    def paged_context_fmha(self, arg: bool, /) -> None: ...

    @property
    def max_input_len(self) -> int: ...

    @max_input_len.setter
    def max_input_len(self, arg: int, /) -> None: ...

    @property
    def max_num_tokens(self) -> int: ...

    @max_num_tokens.setter
    def max_num_tokens(self, arg: int, /) -> None: ...

class BlockKey:
    @overload
    def __init__(self) -> None: ...

    @overload
    def __init__(self, tokens: Sequence[int], lora_task_id: int | None = None) -> None: ...

    @overload
    def __init__(self, uses_extra_ids: bool, lora_task_id: int, unique_tokens: Sequence[bindings.executor.kv_cache.UniqueToken]) -> None: ...

    @property
    def uses_extra_ids(self) -> bool: ...

    @property
    def lora_task_id(self) -> int | None: ...

    @property
    def unique_tokens(self) -> list[bindings.executor.kv_cache.UniqueToken]: ...

class BlockKeyHasher:
    @staticmethod
    def hash(block_key: BlockKey, parent_hash: int = 0) -> int: ...

class KVCacheEventManager:
    def __init__(self, max_kv_event_entries: int, attention_dp_rank: int | None = None, attention_dp_size: int | None = None, attention_dp_events_gather_period_ms: int = 5) -> None: ...

class BaseKVCacheManager:
    @staticmethod
    def calculate_max_num_blocks(config: bindings.executor.KvCacheConfig, is_cross_attention: bool, dtype: bindings.DataType, model_config: bindings.ModelConfig, world_config: bindings.WorldConfig, window_size_to_layers: Mapping[int, Sequence[int]], allotted_primary_mem_bytes: int, allotted_secondary_mem_bytes: int, extra_cost_memory: int, kv_factor: int) -> dict[int, tuple[int, int]]: ...

    def allocate_pools(self, arg: bool, /) -> None: ...

    def release_pools(self) -> None: ...

    def start_scheduling(self) -> None: ...

    @property
    def tokens_per_block(self) -> int: ...

    @property
    def max_num_blocks(self) -> int: ...

    @property
    def num_pools(self) -> int: ...

    def get_kv_cache_stats(self) -> KvCacheStats: ...

    @property
    def max_blocks_per_seq(self) -> int: ...

    def get_needed_blocks_one_step(self, arg0: LlmRequest, arg1: bool, arg2: int, /) -> int: ...

    def get_remaining_blocks_to_completion(self, arg0: LlmRequest, arg1: int, /) -> int: ...

    def add_token(self, arg: int, /) -> None: ...

    def add_sequence(self, arg0: int, arg1: int, arg2: int, arg3: LlmRequest, /) -> None: ...

    def remove_sequence(self, arg0: int, arg1: LlmRequest, arg2: bool, /) -> int | None: ...

    def pin_blocks(self, arg: int, /) -> None: ...

    def scheduling_remove_sequence(self, arg: int, /) -> None: ...

    def get_block_pool_pointers(self) -> torch.Tensor | None: ...

    def get_block_scale_pool_pointers(self) -> torch.Tensor | None: ...

    def get_layer_to_pool_mapping(self) -> torch.Tensor | None: ...

    def get_primary_pool_data(self, arg: int, /) -> torch.Tensor: ...

    def get_unique_primary_pool(self) -> torch.Tensor: ...

    def get_block_offsets_of_batch(self, arg0: torch.Tensor, arg1: int, arg2: int, arg3: int, /) -> None: ...

    def copy_block_offsets(self, arg0: torch.Tensor, arg1: int, arg2: int, /) -> int: ...

    def copy_batch_block_offsets(self, arg0: torch.Tensor, arg1: Sequence[int], arg2: int, arg3: int, /) -> None: ...

    def get_latest_events(self, timeout_ms: float | None = None) -> List: ...

    @property
    def enable_block_reuse(self) -> bool: ...

    def rewind_kv_cache(self, arg0: int, arg1: int, /) -> None: ...

    @property
    def cross_kv(self) -> bool: ...

    def store_context_blocks(self, arg: LlmRequest, /) -> None: ...

    def store_blocks_for_reuse(self, arg0: int, arg1: LlmRequest, arg2: bool, /) -> int | None: ...

    def get_cache_block_ids(self, arg0: int, arg1: int, /) -> CacheBlockIds: ...

    def get_batch_cache_block_ids(self, arg0: Sequence[int], arg1: int, /) -> list[CacheBlockIds]: ...

    def flush_iteration_events(self) -> None: ...

    def get_last_block_id(self, arg: int, /) -> int | None: ...

    def unpin_blocks_by_id(self, arg: int, /) -> None: ...

class CacheBlockIds:
    @overload
    def __init__(self) -> None:
        """Default constructor"""

    @overload
    def __init__(self, arg: CacheBlockIds) -> None:
        """Copy constructor"""

    @overload
    def __init__(self, arg: Iterable[Sequence[int]], /) -> None:
        """Construct from an iterable object"""

    def __len__(self) -> int: ...

    def __bool__(self) -> bool:
        """Check whether the vector is nonempty"""

    def __repr__(self) -> str: ...

    def __iter__(self) -> Iterator[list[int]]: ...

    @overload
    def __getitem__(self, arg: int, /) -> list[int]: ...

    @overload
    def __getitem__(self, arg: slice, /) -> CacheBlockIds: ...

    def clear(self) -> None:
        """Remove all items from list."""

    def append(self, arg: Sequence[int], /) -> None:
        """Append `arg` to the end of the list."""

    def insert(self, arg0: int, arg1: Sequence[int], /) -> None:
        """Insert object `arg1` before index `arg0`."""

    def pop(self, index: int = -1) -> list[int]:
        """Remove and return item at `index` (default last)."""

    def extend(self, arg: CacheBlockIds, /) -> None:
        """Extend `self` by appending elements from `arg`."""

    @overload
    def __setitem__(self, arg0: int, arg1: Sequence[int], /) -> None: ...

    @overload
    def __setitem__(self, arg0: slice, arg1: CacheBlockIds, /) -> None: ...

    @overload
    def __delitem__(self, arg: int, /) -> None: ...

    @overload
    def __delitem__(self, arg: slice, /) -> None: ...

    def __eq__(self, arg: object, /) -> bool: ...

    def __ne__(self, arg: object, /) -> bool: ...

    @overload
    def __contains__(self, arg: Sequence[int], /) -> bool: ...

    @overload
    def __contains__(self, arg: object, /) -> bool: ...

    def count(self, arg: Sequence[int], /) -> int:
        """Return number of occurrences of `arg`."""

    def remove(self, arg: Sequence[int], /) -> None:
        """Remove first occurrence of `arg`."""

    def __getstate__(self) -> tuple: ...

    def __setstate__(self, arg: tuple, /) -> None: ...

class CacheType(enum.Enum):
    SELF = 0

    CROSS = 1

    SELFKONLY = 2

class KVCacheManager(BaseKVCacheManager):
    def __init__(self, num_kv_heads_per_layer: Sequence[int], size_per_head: int, tokens_per_block: int, blocks_per_window: Mapping[int, tuple[int, int]], max_num_sequences: int, max_beam_width: int, max_attention_window_vec: Sequence[int], temp_attention_window_inputs: TempAttentionWindowInputs | None, dtype: bindings.DataType, sink_token_length: int, stream: int, max_sequence_length: int | None, enable_block_reuse: bool = False, onboard_blocks: bool = True, cache_type: CacheType = CacheType.SELF, secondary_offload_min_priority: int | None = None, event_manager: KVCacheEventManager | None = None, enable_partial_reuse: bool = True, copy_on_partial_reuse: bool = True, kv_connector_manager: KvCacheConnectorManager | None = None) -> None: ...

class BasePeftCacheManager:
    def add_request_peft(self, request: LlmRequest, try_gpu_cache: bool = True) -> None: ...

    def ensure_batch(self, context_requests: Sequence[LlmRequest], generation_requests: Sequence[LlmRequest], reset_gpu_cache: bool = False) -> dict[int, list[bindings.internal.runtime.TaskLayerModuleConfig]]: ...

    def reset_device_cache(self) -> None: ...

    def mark_request_done(self, request: LlmRequest, pause: bool = False) -> None: ...

    @property
    def max_device_pages(self) -> int: ...

    @property
    def max_host_pages(self) -> int: ...

    def determine_num_pages(self, request: LlmRequest) -> int: ...

    @property
    def enabled(self) -> bool: ...

class PeftCacheManager(BasePeftCacheManager):
    def __init__(self, config: bindings.PeftCacheManagerConfig, model_config: bindings.ModelConfig, world_config: bindings.WorldConfig, buffer_manager: bindings.internal.runtime.BufferManager) -> None: ...

    def is_task_cached(self, taskId: int) -> bool: ...

class NoOpPeftCacheManager(BasePeftCacheManager):
    def __init__(self) -> None: ...

class BaseCacheTransceiver:
    def respond_and_send_async(self, arg: LlmRequest, /) -> None: ...

    def request_and_receive_sync(self, arg: LlmRequest, /) -> None: ...

    def request_and_receive_async(self, arg: LlmRequest, /) -> None: ...

    def check_context_transfer_status(self, arg: int, /) -> None: ...

    def check_gen_transfer_status(self, arg: int, /) -> None: ...

    def check_gen_transfer_complete(self) -> bool: ...

    def cancel_request(self, arg: LlmRequest, /) -> bool: ...

class AttentionType(enum.Enum):
    DEFAULT = 0

    MLA = 1

class CacheTransceiver(BaseCacheTransceiver):
    def __init__(self, cache_manager: BaseKVCacheManager, num_kv_heads_per_layer: Sequence[int], size_per_head: int, tokens_per_block: int, world_config: bindings.WorldConfig, attention_layer_num_per_pp: Sequence[int], dtype: bindings.DataType, attention_type: AttentionType, cache_transceiver_config: bindings.executor.CacheTransceiverConfig | None = None) -> None: ...

class CacheTransceiverComm:
    def __init__(self, process_group: object, pybind11_abi: str) -> None: ...

    def get_rank(self) -> int: ...

    def get_size(self) -> int: ...

    def split(self, color: int, key: int) -> CacheTransceiverComm: ...

    @overload
    def allgather(self, input: int) -> tuple: ...

    @overload
    def allgather(self, input: float) -> tuple: ...

    @overload
    def allgather(self, input: str) -> tuple: ...

    @overload
    def allgatherv(self, input: Sequence[int], sizes: Sequence[int]) -> tuple: ...

    @overload
    def allgatherv(self, input: Sequence[float], sizes: Sequence[int]) -> tuple: ...

    @overload
    def allgatherv(self, input: Sequence[str], sizes: Sequence[int]) -> tuple: ...

class CacheTransBufferManager:
    def __init__(self, cache_manager: BaseKVCacheManager, max_num_tokens: int | None = None) -> None: ...

    @staticmethod
    def pre_alloc_buffer_size(tokens_per_block: int, cache_size_bytes_per_token_per_window: "std::map<int, int, std::less<int>, std::allocator<std::pair<int const, int> > >", cache_transceiver_config: bindings.executor.CacheTransceiverConfig | None = None) -> int: ...
