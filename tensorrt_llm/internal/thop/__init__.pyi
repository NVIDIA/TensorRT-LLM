from collections.abc import Sequence

import torch


def attention(q: torch.Tensor, k: torch.Tensor | None = None, v: torch.Tensor | None = None, output: torch.Tensor, output_sf: torch.Tensor | None = None, out_dtype: torch.dtype | None = None, workspace_: torch.Tensor | None = None, sequence_length: torch.Tensor, host_past_key_value_lengths: torch.Tensor, host_total_kv_lens: torch.Tensor, context_lengths: torch.Tensor, host_context_lengths: torch.Tensor, host_request_types: torch.Tensor, kv_cache_block_offsets: torch.Tensor | None = None, host_kv_cache_block_offsets: torch.Tensor | None = None, host_kv_cache_pool_pointers: torch.Tensor | None = None, host_kv_cache_pool_mapping: torch.Tensor | None = None, cache_indirection: torch.Tensor | None = None, kv_scale_orig_quant: torch.Tensor | None = None, kv_scale_quant_orig: torch.Tensor | None = None, out_scale: torch.Tensor | None = None, rotary_inv_freq: torch.Tensor | None = None, rotary_cos_sin: torch.Tensor | None = None, latent_cache: torch.Tensor | None = None, q_pe: torch.Tensor | None = None, block_ids_per_seq: torch.Tensor | None = None, attention_sinks: torch.Tensor | None = None, is_fused_qkv: bool, update_kv_cache: bool, predicted_tokens_per_seq: int, layer_idx: int, num_heads: int, num_kv_heads: int, head_size: int, tokens_per_block: int | None = None, max_num_requests: int, max_context_length: int, attention_window_size: int, sink_token_length: int, beam_width: int, mask_type: int, quant_mode: int, q_scaling: float, position_embedding_type: int, rotary_embedding_dim: int, rotary_embedding_base: float, rotary_embedding_scale_type: int, rotary_embedding_scales: Sequence[float], rotary_embedding_max_position_info: Sequence[int], use_paged_context_fmha: bool, attention_input_type: int | None = None, is_mla_enable: bool, chunked_prefill_buffer_batch_size: int | None = None, q_lora_rank: int | None = None, kv_lora_rank: int | None = None, qk_nope_head_dim: int | None = None, qk_rope_head_dim: int | None = None, v_head_dim: int | None = None, mrope_rotary_cos_sin: torch.Tensor | None = None, mrope_position_deltas: torch.Tensor | None = None, mla_tensor_params: Sequence[torch.Tensor | None], attention_chunk_size: int | None = None, softmax_stats_tensor: torch.Tensor | None = None, spec_decoding_bool_params: Sequence[bool], spec_decoding_tensor_params: Sequence[torch.Tensor | None], sparse_attention_params: Sequence[torch.Tensor | None]) -> None:
    """Multi-head attention operation"""
