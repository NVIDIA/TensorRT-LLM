from collections.abc import Sequence

import torch

import bindings
import bindings.executor
import bindings.internal.batch_manager
import bindings.internal.runtime


class CapacityScheduler:
    def __init__(self, max_num_requests: int, capacity_scheduler_policy: bindings.executor.CapacitySchedulerPolicy, has_kv_cache_manager: bool, two_step_lookahead: bool = False, no_schedule_until_state: bindings.LlmRequestState = bindings.LlmRequestState.CONTEXT_INIT, no_schedule_after_state: bindings.LlmRequestState = bindings.LlmRequestState.GENERATION_COMPLETE) -> None: ...

    def __call__(self, active_requests: Sequence[bindings.internal.batch_manager.LlmRequest], kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager | None = None, peft_cache_manager: bindings.internal.batch_manager.BasePeftCacheManager | None = None, cross_kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager | None = None) -> tuple[list[bindings.internal.batch_manager.LlmRequest], list[bindings.internal.batch_manager.LlmRequest], list[bindings.internal.batch_manager.LlmRequest]]: ...

    def name(self) -> str: ...

class MicroBatchScheduler:
    def __init__(self, ctx_chunk_config: bindings.internal.batch_manager.ContextChunkingConfig | None = None, max_context_length: int | None = None, no_schedule_until_state: bindings.LlmRequestState = bindings.LlmRequestState.CONTEXT_INIT, no_schedule_after_state: bindings.LlmRequestState = bindings.LlmRequestState.GENERATION_COMPLETE) -> None: ...

    def __call__(self, active_requests: Sequence[bindings.internal.batch_manager.LlmRequest], inflight_req_ids: bindings.internal.batch_manager.ReqIdsSet, max_batch_size_runtime: int, max_num_tokens_runtime: int) -> tuple[list[bindings.internal.batch_manager.LlmRequest], list[bindings.internal.batch_manager.LlmRequest]]: ...

    def name(self) -> str: ...

class PauseRequests:
    def __init__(self, max_input_len: int) -> None: ...

    def __call__(self, requests_to_pause: Sequence[bindings.internal.batch_manager.LlmRequest], inflight_req_ids: bindings.internal.batch_manager.ReqIdsSet, req_ids_to_pause: bindings.internal.batch_manager.ReqIdsSet, pause_flagged: bool, seq_slot_manager: bindings.internal.batch_manager.SequenceSlotManager, kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager | None = None, cross_kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager | None = None, peft_cache_manager: bindings.internal.batch_manager.BasePeftCacheManager | None = None) -> None: ...

    def name(self) -> str: ...

class AssignReqSeqSlots:
    def __init__(self) -> None: ...

    def __call__(self, seq_slot_manager: bindings.internal.batch_manager.SequenceSlotManager, context_requests: Sequence[bindings.internal.batch_manager.LlmRequest], generation_requests: Sequence[bindings.internal.batch_manager.LlmRequest]) -> None: ...

    def name(self) -> str: ...

class AllocateKvCache:
    def __init__(self) -> None: ...

    def __call__(self, kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager, context_requests: Sequence[bindings.internal.batch_manager.LlmRequest], generation_requests: Sequence[bindings.internal.batch_manager.LlmRequest], model_config: bindings.ModelConfig, cross_kv_cache_manager: bindings.internal.batch_manager.BaseKVCacheManager | None = None) -> None: ...

    def name(self) -> str: ...

class LogitsPostProcessor:
    def __init__(self) -> None: ...

    def __call__(self, decoder_input_buffers: bindings.internal.batch_manager.DecoderInputBuffers, replicate_logits_post_processor: bool, world_config: bindings.WorldConfig, stream: int, logits_post_processor_batched: "std::function<void (std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<std::shared_ptr<tensorrt_llm::runtime::ITensor>, std::allocator<std::shared_ptr<tensorrt_llm::runtime::ITensor> > >&, std::vector<std::reference_wrapper<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const>, std::allocator<std::reference_wrapper<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const> > > const&, std::shared_ptr<tensorrt_llm::runtime::CudaStream> const&, std::vector<std::optional<unsigned long>, std::allocator<std::optional<unsigned long> > > const&)>" | None = None) -> bool: ...

    def name(self) -> str: ...

class CreateNewDecoderRequests:
    def __init__(self, speculative_decoding_fast_logits: bool, is_leader_in_orch_mode: bool, is_normalize_log_probs: bool) -> None: ...

    def __call__(self, model_config: bindings.ModelConfig, world_config: bindings.WorldConfig, decoding_config: bindings.executor.DecodingConfig, context_requests: Sequence[bindings.internal.batch_manager.LlmRequest], logits_type: bindings.DataType, decoder_input_buffers: bindings.internal.batch_manager.DecoderInputBuffers, decoder_state: bindings.internal.runtime.DecoderState, runtime_stream: bindings.CudaStream, decoder_stream: bindings.CudaStream, max_sequence_length: int, beam_width: int) -> tuple[torch.Tensor, bindings.SamplingConfigVector, list[torch.Tensor], list[bindings.executor.LookaheadDecodingConfig]]: ...

    def name(self) -> str: ...
