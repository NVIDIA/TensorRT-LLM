# Additional default args for AutoDeployConfig/LlmArgs in _torch/auto_deploy/llm_args.py

transforms:
  ############################################################################################
  # BUILD MODEL, LOAD WEIGHTS, AND WRAP IT INTO FAKE GRAPH MODULE
  ############################################################################################
  build_and_load_factory_model:
    stage: factory
  ############################################################################################
  # MOVE ARGUMENTS TO DEVICE
  ############################################################################################
  move_cm_to_device:
    stage: weight_load
  ############################################################################################
  # SWITCH TO CACHED+FLATTENED ATTENTION + INITIALIZE CACHES
  ############################################################################################
  transformers_replace_cached_attn2:
    stage: cache_init
    attn_backend: flashinfer
  initialize_cache:
    stage: cache_init
  resize_kv_cache:
    stage: cache_init
