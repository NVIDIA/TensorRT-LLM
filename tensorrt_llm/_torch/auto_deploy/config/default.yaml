# This is the set of transforms running in "graph" mode. In this mode, we capture the full graph
# of the model and optimize it for inference.
transforms:
  ############################################################################################
  # BUILD MODEL, EXPORT TO GRAPH MODULE, AND CLEAN UP
  ############################################################################################
  build_model:
    stage: factory
    run_per_gm: false
    device: meta
    requires_clean_graph: false
  export_to_gm:
    stage: export
    clone_state_dict: false
    strict: false
    run_per_gm: false
    requires_clean_graph: false
  cleanup_noop_slice:
    stage: post_export
  cleanup_noop_add:
    stage: post_export
  cleanup_input_constraints:
    stage: post_export
  ############################################################################################
  # RUN PATTERN MATCHER TRANSFORMATIONS TO STANDARDIZE GRAPH REPRESENTATION
  ############################################################################################
  match_moe_pattern:
    stage: pattern_matcher
  match_dense_moe_pattern:
    stage: pattern_matcher
  match_bmm_moe_pattern:
    stage: pattern_matcher
  match_repeat_kv:
    stage: pattern_matcher
    run_shape_prop: true
  match_eager_attention:
    stage: pattern_matcher
    requires_shape_prop: true
  match_sdpa_to_torch_attention:
    stage: pattern_matcher
  match_grouped_attention:
    stage: pattern_matcher
  match_attention_layout:
    stage: pattern_matcher
    attn_layout: bsnd
  match_rope_pattern:
    stage: pattern_matcher
  match_rope_layout:
    stage: pattern_matcher
    expected_layout: bsnd
  match_rmsnorm_pattern:
    stage: pattern_matcher
  match_l2norm_pattern:
    stage: pattern_matcher
  ############################################################################################
  # RUN TRANSFORMATIONS ON STANDARDIZED GRAPH REPRESENTATION
  ############################################################################################
  eliminate_redundant_transposes:
    stage: pattern_matcher
  # TODO (lucaslie): let's move this to perf optimization once TP sharding is improved
  # see https://github.com/NVIDIA/TensorRT-LLM/pull/3668#discussion_r2052714528
  optimize_rope:
    stage: pattern_matcher
  quantize_int4_linear_from_config:
    stage: pattern_matcher
  quantize_int4_gptq_linear_from_config:
    stage: pattern_matcher
  quantize_fp8_linear_from_config:
    stage: pattern_matcher
  quantize_nvfp4_linear_from_config:
    stage: pattern_matcher
  quantize_fp8_bmm_from_config:
    stage: pattern_matcher
  quantize_fp8_from_graph:
    stage: pattern_matcher
  quantize_nvfp4_from_graph:
    stage: pattern_matcher
  quantize_fp8_moe:
    stage: pattern_matcher
  quantize_nvfp4_moe:
    stage: pattern_matcher
  quantize_mxfp4_moe:
    stage: pattern_matcher
  detect_hidden_states_for_capture:
    stage: pattern_matcher
  detect_sharding:
    stage: sharding
    simple_shard_only: false
    sharding_source: ['manual', 'factory', 'heuristic']
    support_partial_config: true
    sharding_dims: ['tp', 'ep', 'bmm']
    shard_all_unprocessed: true
    allreduce_strategy: 'NCCL'
    dist_backend: auto
    requires_shape_prop: true
  sharding_transform_executor:
    stage: sharding
    run_shape_prop: true
  ############################################################################################
  # MOVE MODEL AND LOAD WEIGHTS
  ############################################################################################
  load_weights:
    stage: weight_load
    run_per_gm: false
    checkpoint_device: null
    expect_mem_change: true
  move_inputs_to_device:
    stage: weight_load
    run_per_gm: false
    expect_mem_change: true
  ############################################################################################
  # RUN POST-LOAD FUSION AND OPTIMIZATIONS
  ############################################################################################
  fuse_gemms:
    stage: post_load_fusion
    enabled: false # TODO: https://github.com/NVIDIA/TensorRT-LLM/issues/4674 this is causing OOMs
  fuse_fp4_gemms:
    stage: post_load_fusion
    enabled: false # TODO: https://github.com/NVIDIA/TensorRT-LLM/issues/4674 this is causing OOMs
  fuse_fp8_gemms:
    stage: post_load_fusion
    enabled: false # TODO: https://github.com/NVIDIA/TensorRT-LLM/issues/4674 this is causing OOMs
  fuse_fp8_linear:
    stage: post_load_fusion
    backend: trtllm
  fuse_nvfp4_linear:
    stage: post_load_fusion
    backend: trtllm
  fuse_moe:
    stage: post_load_fusion
    expect_mem_change: true
    backend: trtllm
  fuse_fp8_moe:
    stage: post_load_fusion
    expect_mem_change: true
    backend: trtllm
  fuse_nvfp4_moe:
    stage: post_load_fusion
    expect_mem_change: true
  fuse_allreduce_residual_rmsnorm:
    stage: post_load_fusion
  fuse_rmsnorm:
    stage: post_load_fusion
    rmsnorm_backend: flashinfer
    gated_rmsnorm_backend: triton
    requires_shape_prop: true
  fuse_l2norm:
    stage: post_load_fusion
    backend: fla
  fuse_add_rms_norm:
    stage: post_load_fusion
    enabled: true
  gather_logits_before_lm_head:
    stage: post_load_fusion
    # TODO: fix https://github.com/NVIDIA/TensorRT-LLM/issues/9878 to enable by default
    enabled: false
  ############################################################################################
  # VISUALIZE GRAPH
  ############################################################################################
  visualize_namespace:
    stage: visualize
    enabled: false
  ############################################################################################
  # SWITCH TO CACHED+FLATTENED ATTENTION + INITIALIZE CACHES
  ############################################################################################
  insert_cached_attention:
    stage: cache_init
    backend: flashinfer
  insert_cached_mla_attention:
    stage: cache_init
    backend: MultiHeadLatentAttention
  insert_cached_ssm_attention:
    stage: cache_init
    backend: triton_ssm
  insert_cached_causal_conv:
    stage: cache_init
    backend: cuda_causal_conv
  insert_cached_delta_rule:
    stage: cache_init
    backend: fla_delta
  insert_cached_residual_add:
    stage: cache_init
    backend: cached_residual_add
  initialize_cache:
    stage: cache_init
    expect_mem_change: true
    run_per_gm: false
  resize_kv_cache:
    stage: cache_init
    expect_mem_change: true
    run_per_gm: false
  ############################################################################################
  # COMPILE MODEL
  ############################################################################################
  fuse_causal_conv_activation:
    stage: compile
  multi_stream_moe:
    stage: compile
    enabled: false
  compile_model:
    stage: compile
    expect_mem_change: true
    run_per_gm: false
    cuda_graph_batch_sizes: null
    backend: torch-compile
