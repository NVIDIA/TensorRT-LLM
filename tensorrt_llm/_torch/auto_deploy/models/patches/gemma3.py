"""Export patch for Gemma 3 VLM.

This patch replaces the mask generation logic in Gemma3TextModel with a custom op
that can be properly traced and optimized by the AutoDeploy backend.

Assumption: Export is done for prefill configuration where attention_mask input is None.
The custom op generates the appropriate masks inside the graph.
"""

import torch

from transformers.models.gemma3.modeling_gemma3 import Gemma3Model, Gemma3TextModel

# Import to ensure the custom op is registered
from ...custom_ops import custom_attn_mask_gen  # noqa: F401
from ...export.interface import BaseExportPatch, ExportPatchRegistry


def _gemma3_model_forward(
    self: Gemma3Model,
    input_ids: torch.LongTensor = None,
    pixel_values: torch.FloatTensor = None,
    attention_mask: torch.Tensor = None,
    position_ids: torch.LongTensor = None,
    past_key_values=None,
    token_type_ids: torch.LongTensor = None,
    cache_position: torch.LongTensor = None,
    inputs_embeds: torch.FloatTensor = None,
    use_cache: bool = None,
    **lm_kwargs,
):
    """Patched forward for Gemma3Model that passes attention_mask=None to language_model.

    This ensures the Gemma3TextModel (language_model) uses the custom op for mask generation
    instead of receiving a pre-computed mask dict.
    """
    from transformers.models.gemma3.modeling_gemma3 import Gemma3ModelOutputWithPast

    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

    # Replace image id with PAD if the image token is OOV, to avoid index-errors
    if input_ids is not None and self.config.image_token_id >= self.vocab_size:
        special_image_mask = input_ids == self.config.image_token_id
        llm_input_ids = input_ids.clone()
        llm_input_ids[special_image_mask] = 0
    else:
        llm_input_ids = input_ids

    if inputs_embeds is None:
        inputs_embeds = self.get_input_embeddings()(llm_input_ids)

    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(
            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
        )

    # Merge text and images
    image_features = None
    if pixel_values is not None:
        image_features = self.get_image_features(pixel_values)
        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
        special_image_mask = self.get_placeholder_mask(
            input_ids, inputs_embeds=inputs_embeds, image_features=image_features
        )
        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)

    # PATCH: Skip mask creation here - pass attention_mask=None and token_type_ids
    # The Gemma3TextModel patch will use the custom op to generate masks
    # Don't pass **lm_kwargs - those are HF training/inference args we don't need
    outputs = self.language_model(
        attention_mask=None,  # Let TextModel generate via custom op
        position_ids=position_ids,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        use_cache=use_cache,
        cache_position=cache_position,
        token_type_ids=token_type_ids,  # Pass through for custom op
    )

    return Gemma3ModelOutputWithPast(
        last_hidden_state=outputs.last_hidden_state,
        past_key_values=outputs.past_key_values,
        hidden_states=outputs.hidden_states,
        attentions=outputs.attentions,
        image_hidden_states=image_features if pixel_values is not None else None,
    )


def _gemma3_text_model_forward(
    self: Gemma3TextModel,
    input_ids: torch.LongTensor = None,
    attention_mask: torch.Tensor = None,  # Expected to be None during export
    position_ids: torch.LongTensor = None,
    past_key_values=None,
    inputs_embeds: torch.FloatTensor = None,
    use_cache: bool = None,
    cache_position: torch.LongTensor = None,
    token_type_ids: torch.Tensor = None,
    **kwargs,
):
    """Patched forward method for Gemma3TextModel using custom mask op.

    This patched forward:
    1. Ignores the attention_mask input (expected to be None for prefill export)
    2. Uses a custom op for mask generation (traceable)
    3. Directly passes the correct mask/position_embeddings to each layer
    """
    # Handle inputs_embeds
    if inputs_embeds is None and input_ids is not None:
        inputs_embeds = self.embed_tokens(input_ids)

    # Ensure position_ids are set
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    assert attention_mask is None, (
        "AD Module export should not be given attention_mask as input. "
        "It will be generated by the custom op."
    )

    # Get batch size from inputs_embeds
    batch_size = inputs_embeds.shape[0]

    # Generate masks using custom op
    full_mask = torch.ops.autodeploy.custom_attn_mask_gen_op(
        token_type_ids,
        cache_position,
        "full_attention",
        self.config.sliding_window,
        inputs_embeds.dtype,
        batch_size,
    )
    sliding_mask = torch.ops.autodeploy.custom_attn_mask_gen_op(
        token_type_ids,
        cache_position,
        "sliding_attention",
        self.config.sliding_window,
        inputs_embeds.dtype,
        batch_size,
    )

    # Compute position embeddings
    hidden_states = inputs_embeds

    # Check if rotary_emb returns dict or tuple (version dependent)
    rotary_output = self.rotary_emb(hidden_states, position_ids)

    # Handle different rotary_emb return types
    if isinstance(rotary_output, dict):
        # Newer versions return dict with 'global' and 'local' keys
        pos_emb_global = rotary_output.get("global", rotary_output.get("full_attention"))
        pos_emb_local = rotary_output.get("local", rotary_output.get("sliding_attention"))
    else:
        # Older versions return tuple (cos, sin) - use same for both
        pos_emb_global = rotary_output
        pos_emb_local = rotary_output

    # Forward through decoder layers
    for decoder_layer in self.layers[: self.config.num_hidden_layers]:
        # Select mask based on layer's attention type
        layer_type = getattr(decoder_layer, "attention_type", "full_attention")
        layer_mask = full_mask if layer_type == "full_attention" else sliding_mask

        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=layer_mask,
            position_embeddings_global=pos_emb_global,
            position_embeddings_local=pos_emb_local,
            position_ids=position_ids,
            past_key_values=past_key_values,
            cache_position=cache_position,
            **kwargs,
        )
        # Decoder layer returns tuple (hidden_states, ...) - extract first element
        hidden_states = layer_outputs[0] if isinstance(layer_outputs, tuple) else layer_outputs

    hidden_states = self.norm(hidden_states)

    # Return in the expected format
    from transformers.modeling_outputs import BaseModelOutputWithPast

    return BaseModelOutputWithPast(last_hidden_state=hidden_states)


@ExportPatchRegistry.register("hf_gemma3")
class Gemma3Patch(BaseExportPatch):
    """Patch for Gemma3Model and Gemma3TextModel for export."""

    def _apply_patch(self):
        """Apply the Gemma3 patches."""
        # Patch Gemma3Model to pass attention_mask=None to language_model
        self.original_values["Gemma3Model.forward"] = Gemma3Model.forward
        Gemma3Model.forward = _gemma3_model_forward

        # Patch Gemma3TextModel to use custom mask op
        self.original_values["Gemma3TextModel.forward"] = Gemma3TextModel.forward
        Gemma3TextModel.forward = _gemma3_text_model_forward

    def _revert_patch(self):
        """Revert the Gemma3 patches."""
        Gemma3Model.forward = self.original_values["Gemma3Model.forward"]
        Gemma3TextModel.forward = self.original_values["Gemma3TextModel.forward"]
