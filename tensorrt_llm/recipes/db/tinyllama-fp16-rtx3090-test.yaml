# TinyLlama 1.1B FP16 Recipe for RTX 3090 (Test Configuration)
#
# This recipe provides test settings for running TinyLlama-1.1B
# on RTX 3090 GPUs (24GB VRAM, sm89) for development and testing.
#
# TinyLlama is a small 1.1B parameter model ideal for testing on consumer GPUs.

scenario:
  model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  gpu: RTX_3090
  num_gpus: 1
  target_isl: 1024
  target_osl: 256
  target_concurrency: 32
  # Note: No specific profile needed for TinyLlama FP16
  # Using generic configuration

env: {}

config:
  # Conservative batch size for 24GB VRAM
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 64

  # KV cache configuration for RTX 3090
  kv_cache_config:
    dtype: float16
    enable_block_reuse: false
    free_gpu_memory_fraction: 0.7

  # Single GPU configuration
  tensor_parallel_size: 1
  pipeline_parallel_size: 1

  # Logging and monitoring
  print_iter_log: true

  # Backend selection (pytorch for compatibility)
  backend: pytorch

# Optional overrides section for testing variations
# Uncomment and modify as needed
overrides:
  # kv_cache_config:
  #   free_gpu_memory_fraction: 0.8
  # cuda_graph_config:
  #   max_batch_size: 32
  #   enable_padding: false
