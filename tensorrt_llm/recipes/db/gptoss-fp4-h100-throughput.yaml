# GPT-OSS 120B FP4 Recipe for H100 GPUs (High Throughput)
#
# This recipe provides optimized settings for running GPT-OSS models
# on H100_SXM GPUs targeting high-throughput scenarios.
#
# Based on: InferenceMAX/benchmarks/gptoss_fp4_b200_trt_slurm.sh

scenario:
  model: openai/gpt-oss-120b
  gpu: H100_SXM
  num_gpus: 8
  target_isl: 8000
  target_osl: 1000
  target_concurrency: 256
  profile: gptoss-fp4

env:
  TRTLLM_ENABLE_PDL: 1
  NCCL_GRAPH_REGISTER: 0

llm_api_config:
  cuda_graph_config:
    enable_padding: true
    max_batch_size: 256
  enable_attention_dp: true
  kv_cache_config:
    dtype: fp8
    enable_block_reuse: false
    free_gpu_memory_fraction: 0.85
  print_iter_log: true
  stream_interval: 20
  num_postprocess_workers: 4
  moe_config:
    backend: TRTLLM
  attention_dp_config:
    enable_balance: true

# Optional overrides section for power users
# Uncomment and modify as needed
overrides:
  # kv_cache_config:
  #   free_gpu_memory_fraction: 0.9
  # cuda_graph_config:
  #   max_batch_size: 512
